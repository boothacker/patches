From b0475f9413f51dfbce9eb1972e05417b5629cf81 Mon Sep 17 00:00:00 2001
From: Maksim Lebid <33498219+boothacker@users.noreply.github.com>
Date: Sat, 20 Jan 2018 23:04:55 +0200
Subject: [PATCH] for mtk...

---
 CleanSpec.mk                                       |  16 +-
 core/Makefile                                      | 159 ++---
 core/android_manifest.mk                           |  22 +-
 core/apicheck_msg_current.txt                      |  70 +--
 core/base_rules.mk                                 |   2 +-
 core/binary.mk                                     |  21 +-
 core/clang/HOST_x86_common.mk                      |   5 +-
 core/clang/config.mk                               |   5 -
 core/cleanbuild.mk                                 |  15 +-
 core/clear_vars.mk                                 |   7 -
 core/combo/HOST_darwin-x86.mk                      |  13 +-
 core/combo/HOST_darwin-x86_64.mk                   |  11 +-
 core/combo/HOST_windows-x86.mk                     |   2 +-
 core/combo/TARGET_linux-x86.mk                     |   2 +-
 core/combo/TARGET_linux-x86_64.mk                  |   2 +-
 core/combo/arch/arm/armv7-a-neon.mk                |  26 +-
 core/combo/arch/arm64/armv8-a.mk                   |  12 -
 core/combo/mac_version.mk                          |  21 +-
 core/combo/select.mk                               |   6 -
 core/config.mk                                     |  22 +-
 core/definitions.mk                                |  18 +-
 core/dex_preopt_libart.mk                          |  18 +-
 core/dex_preopt_libart_boot.mk                     |  14 +-
 core/dex_preopt_odex_install.mk                    |  37 +-
 core/dumpvar.mk                                    |   4 +-
 core/dynamic_binary.mk                             |  12 +-
 core/generate_extra_images.mk                      |  62 +-
 core/install_jni_libs_internal.mk                  |   6 -
 core/java.mk                                       |   3 -
 core/main.mk                                       |  12 +-
 core/package_internal.mk                           |  46 +-
 core/pathmap.mk                                    |   1 +
 core/prebuilt.mk                                   |   1 -
 core/prebuilt_internal.mk                          |  37 +-
 core/product.mk                                    |  83 +--
 core/product_config.mk                             |  36 +-
 core/qcom_target.mk                                |  54 +-
 core/qcom_utils.mk                                 |   3 -
 core/setup_one_odex.mk                             |  17 -
 core/tasks/apicheck.mk                             |  31 +-
 core/tasks/factory_bundle.mk                       |   3 +-
 core/tasks/factory_ramdisk.mk                      |   2 +-
 core/tasks/kernel.mk                               | 223 ++-----
 core/tasks/oem_image.mk                            |  11 +-
 envsetup.sh                                        | 331 +----------
 target/board/generic_arm64/BoardConfig.mk          |   2 +-
 target/board/generic_mips/BoardConfig.mk           |   2 +-
 target/board/generic_x86_64/BoardConfig.mk         |   2 +-
 target/product/base.mk                             |  10 -
 target/product/core.mk                             |   6 +-
 target/product/core_base.mk                        |   1 -
 target/product/core_minimal.mk                     |   2 -
 target/product/core_tiny.mk                        |   1 -
 target/product/full_base.mk                        |  11 +-
 target/product/full_base_telephony.mk              |   6 +
 target/product/generic_no_telephony.mk             |   1 -
 target/product/languages_full.mk                   |   5 +-
 target/product/sdk_base.mk                         |   6 +-
 target/product/security/bacon.x509.pem             |  23 +
 target/product/security/verity_key                 | Bin 524 -> 524 bytes
 target/product/security/verity_private_dev_key     |  28 +
 target/product/verity.mk                           |   6 +-
 tools/buildinfo.sh                                 |   3 -
 tools/device/cm.mk.template                        |   3 +
 .../droiddoc/templates-sdk/assets/css/default.css  | 100 +---
 tools/droiddoc/templates-sdk/assets/js/docs.js     | 191 +-----
 .../droiddoc/templates-sdk/components/masthead.cs  |  77 +--
 tools/droiddoc/templates-sdk/customizations.cs     |  20 +-
 tools/droiddoc/templates-sdk/docpage.cs            |   8 +-
 tools/droiddoc/templates-sdk/head_tag.cs           |  15 +-
 tools/droiddoc/templates-sdk/macros_override.cs    |   2 +-
 tools/droiddoc/templates-sdk/sdkpage.cs            | 214 +++----
 tools/post_process_props.py                        |   6 +-
 tools/releasetools/add_img_to_target_files.py      | 180 +-----
 tools/releasetools/blockimgdiff.py                 | 423 +------------
 tools/releasetools/build_image.py                  |  26 +-
 tools/releasetools/common.py                       | 160 +----
 tools/releasetools/edify_generator.py              |  60 +-
 tools/releasetools/img_from_target_files.py        |  30 -
 tools/releasetools/ota_from_target_files           | 224 ++-----
 tools/releasetools/rangelib.py                     |  86 +--
 tools/releasetools/sign_target_files_apks          |   5 +-
 tools/repopick.py                                  | 656 +++++++++++----------
 tools/roomservice.py                               |  32 +-
 84 files changed, 1088 insertions(+), 3049 deletions(-)
 create mode 100644 target/product/security/bacon.x509.pem
 create mode 100644 target/product/security/verity_private_dev_key

diff --git a/CleanSpec.mk b/CleanSpec.mk
index e9d4455..a1dec2e 100644
--- a/CleanSpec.mk
+++ b/CleanSpec.mk
@@ -299,22 +299,10 @@ $(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/build.prop)
 $(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/app/*)
 $(call add-clean-step, rm -rf $(PRODUCT_OUT)/obj/APPS/*)
 
-# API 22!
+# 5.0.1
 $(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/build.prop)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/app/*)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/obj/APPS/*)
-
-# 5.1!
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/build.prop)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/app/*)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/obj/APPS/*)
-
-# 5.1.1!
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/build.prop)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/app/*)
-$(call add-clean-step, rm -rf $(PRODUCT_OUT)/obj/APPS/*)
 
-# Added new build props
+# 5.0.2
 $(call add-clean-step, rm -rf $(PRODUCT_OUT)/system/build.prop)
 
 # ************************************************
diff --git a/core/Makefile b/core/Makefile
index 9ab196a..8ee10b2 100644
--- a/core/Makefile
+++ b/core/Makefile
@@ -225,7 +225,6 @@ ifneq ($(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_OEM_PROPERTIES),)
 		echo "import /oem/oem.prop $(prop)" >> $@;)
 endif
 	$(hide) TARGET_BUILD_TYPE="$(TARGET_BUILD_VARIANT)" \
-			TARGET_BUILD_FLAVOR="$(TARGET_PRODUCT)-$(TARGET_BUILD_VARIANT)" \
 			TARGET_DEVICE="$(TARGET_VENDOR_DEVICE_NAME)" \
 			CM_DEVICE="$(TARGET_DEVICE)" \
 			PRODUCT_NAME="$(TARGET_VENDOR_PRODUCT_NAME)" \
@@ -240,8 +239,6 @@ endif
 			BUILD_DISPLAY_ID="$(BUILD_DISPLAY_ID)" \
 			BUILD_NUMBER="$(BUILD_NUMBER)" \
 			PLATFORM_VERSION="$(PLATFORM_VERSION)" \
-			PLATFORM_SECURITY_PATCH="$(PLATFORM_SECURITY_PATCH)" \
-			PLATFORM_BASE_OS="$(PLATFORM_BASE_OS)" \
 			PLATFORM_SDK_VERSION="$(PLATFORM_SDK_VERSION)" \
 			PLATFORM_VERSION_CODENAME="$(PLATFORM_VERSION_CODENAME)" \
 			PLATFORM_VERSION_ALL_CODENAMES="$(PLATFORM_VERSION_ALL_CODENAMES)" \
@@ -487,6 +484,12 @@ INSTALLED_RAMDISK_TARGET := $(BUILT_RAMDISK_TARGET)
 $(INSTALLED_RAMDISK_TARGET): $(MKBOOTFS) $(INTERNAL_RAMDISK_FILES) | $(MINIGZIP)
 	$(call pretty,"Target ram disk: $@")
 	$(hide) $(MKBOOTFS) $(TARGET_ROOT_OUT) | $(MINIGZIP) > $@
+ifneq ($(TARGET_MKIMAGE),)
+		$(call pretty,"Appending MTK header to ramdisk")
+		$(hide) $(TARGET_MKIMAGE) $@ ROOTFS > $(PRODUCT_OUT)/ramdisk_tmp.img
+		$(hide) mv $@ $(PRODUCT_OUT)/ramdisk_without_header.img
+		$(hide) mv $(PRODUCT_OUT)/ramdisk_tmp.img $@
+endif
 
 .PHONY: ramdisk-nodeps
 ramdisk-nodeps: $(MKBOOTFS) | $(MINIGZIP)
@@ -532,11 +535,6 @@ ifeq ($(TARGET_BOOTIMAGE_USE_EXT2),true)
 tmp_dir_for_image := $(call intermediates-dir-for,EXECUTABLES,boot_img)/bootimg
 INTERNAL_BOOTIMAGE_ARGS += --tmpdir $(tmp_dir_for_image)
 INTERNAL_BOOTIMAGE_ARGS += --genext2fs $(MKEXT2IMG)
-
-ifeq ($(TARGET_BOOTIMAGE_USE_EXTLINUX),true)
-INTERNAL_BOOTIMAGE_ARGS += --extlinuxconf $(TARGET_BOOTIMAGE_EXTLINUX_CONFIG)
-endif
-
 $(INSTALLED_BOOTIMAGE_TARGET): $(MKEXT2IMG) $(INTERNAL_BOOTIMAGE_FILES)
 	$(call pretty,"Target boot image: $@")
 	$(hide) $(MKEXT2BOOTIMG) $(INTERNAL_BOOTIMAGE_ARGS) --output $@
@@ -554,14 +552,14 @@ else ifndef BOARD_CUSTOM_BOOTIMG_MK
 $(INSTALLED_BOOTIMAGE_TARGET): $(MKBOOTIMG) $(INTERNAL_BOOTIMAGE_FILES) $(BOOT_SIGNER) $(BOOTIMAGE_EXTRA_DEPS)
 	$(call pretty,"Target boot image: $@")
 	$(hide) $(MKBOOTIMG) $(INTERNAL_BOOTIMAGE_ARGS) $(BOARD_MKBOOTIMG_ARGS) --output $@
-	$(BOOT_SIGNER) /boot $@ $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).pk8 $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).x509.pem $@
+	$(BOOT_SIGNER) /boot $@ $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY) $@
 	$(hide) $(call assert-max-image-size,$@,$(BOARD_BOOTIMAGE_PARTITION_SIZE))
 
 .PHONY: bootimage-nodeps
 bootimage-nodeps: $(MKBOOTIMG) $(BOOT_SIGNER)
 	@echo "make $@: ignoring dependencies"
 	$(hide) $(MKBOOTIMG) $(INTERNAL_BOOTIMAGE_ARGS) $(BOARD_MKBOOTIMG_ARGS) --output $(INSTALLED_BOOTIMAGE_TARGET)
-	$(BOOT_SIGNER) /boot $(INSTALLED_BOOTIMAGE_TARGET) $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).pk8 $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).x509.pem $(INSTALLED_BOOTIMAGE_TARGET)
+	$(BOOT_SIGNER) /boot $(INSTALLED_BOOTIMAGE_TARGET) $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY) $(INSTALLED_BOOTIMAGE_TARGET)
 	$(hide) $(call assert-max-image-size,$(INSTALLED_BOOTIMAGE_TARGET),$(BOARD_BOOTIMAGE_PARTITION_SIZE))
 
   else # PRODUCT_SUPPORTS_VERITY != true
@@ -725,12 +723,6 @@ INTERNAL_USERIMAGES_EXT_VARIANT := ext4
 endif
 endif
 endif
-ifeq ($(TARGET_USERIMAGES_USE_F2FS),true)
-INTERNAL_USERIMAGES_USE_F2FS := true
-ifeq ($(INTERNAL_USERIMAGES_EXT_VARIANT),)
-INTERNAL_USERIMAGES_EXT_VARIANT := f2fs
-endif
-endif
 
 # These options tell the recovery updater/installer how to mount the partitions writebale.
 # <fstype>=<fstype_opts>[|<fstype_opts>]...
@@ -743,21 +735,15 @@ ifneq (true,$(TARGET_USERIMAGES_SPARSE_EXT_DISABLED))
   INTERNAL_USERIMAGES_SPARSE_EXT_FLAG := -s
 endif
 
-ifneq ($(TARGET_TRANSPARENT_COMPRESSION_METHOD),)
-  INTERNAL_TRANSPARENT_COMPRESSION_METHOD := $(TARGET_TRANSPARENT_COMPRESSION_METHOD)
-endif
-
-INTERNAL_USERIMAGES_DEPS :=
 ifeq ($(INTERNAL_USERIMAGES_USE_EXT),true)
+INTERNAL_USERIMAGES_DEPS := $(SIMG2IMG)
 INTERNAL_USERIMAGES_DEPS += $(MKEXTUSERIMG) $(MAKE_EXT4FS) $(E2FSCK)
-endif
-ifeq ($(INTERNAL_USERIMAGES_USE_F2FS),true)
+ifeq ($(TARGET_USERIMAGES_USE_F2FS),true)
 INTERNAL_USERIMAGES_DEPS += $(MKF2FSUSERIMG) $(MAKE_F2FS)
 endif
-ifeq ($(INTERNAL_USERIMAGES_DEPS),)
+else
 INTERNAL_USERIMAGES_DEPS := $(MKYAFFS2)
 endif
-INTERNAL_USERIMAGES_DEPS += $(SIMG2IMG)
 
 INTERNAL_USERIMAGES_BINARY_PATHS := $(sort $(dir $(INTERNAL_USERIMAGES_DEPS)))
 
@@ -772,22 +758,15 @@ INTERNAL_USERIMAGES_DEPS += $(SELINUX_FC)
 # $(2): additional "key=value" pairs to append to the dictionary file.
 define generate-userimage-prop-dictionary
 $(if $(INTERNAL_USERIMAGES_EXT_VARIANT),$(hide) echo "fs_type=$(INTERNAL_USERIMAGES_EXT_VARIANT)" >> $(1))
-$(if $(BOARD_SYSTEMIMAGE_FILE_SYSTEM_TYPE),$(hide) echo "system_fs_type=$(BOARD_SYSTEMIMAGE_FILE_SYSTEM_TYPE)" >> $(1))
 $(if $(BOARD_SYSTEMIMAGE_PARTITION_SIZE),$(hide) echo "system_size=$(BOARD_SYSTEMIMAGE_PARTITION_SIZE)" >> $(1))
-$(if $(BOARD_SYSTEMIMAGE_JOURNAL_SIZE),$(hide) echo "system_journal_size=$(BOARD_SYSTEMIMAGE_JOURNAL_SIZE)" >> $(1))
 $(if $(BOARD_USERDATAIMAGE_FILE_SYSTEM_TYPE),$(hide) echo "userdata_fs_type=$(BOARD_USERDATAIMAGE_FILE_SYSTEM_TYPE)" >> $(1))
 $(if $(BOARD_USERDATAIMAGE_PARTITION_SIZE),$(hide) echo "userdata_size=$(BOARD_USERDATAIMAGE_PARTITION_SIZE)" >> $(1))
-$(if $(BOARD_USERDATAEXTRAIMAGE_PARTITION_SIZE),$(hide) echo "userdataextra_size=$(BOARD_USERDATAEXTRAIMAGE_PARTITION_SIZE)" >> $(1))
-$(if $(BOARD_USERDATAEXTRAIMAGE_PARTITION_NAME),$(hide) echo "userdataextra_name=$(BOARD_USERDATAEXTRAIMAGE_PARTITION_NAME)" >> $(1))
 $(if $(BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE),$(hide) echo "cache_fs_type=$(BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE)" >> $(1))
 $(if $(BOARD_CACHEIMAGE_PARTITION_SIZE),$(hide) echo "cache_size=$(BOARD_CACHEIMAGE_PARTITION_SIZE)" >> $(1))
 $(if $(BOARD_VENDORIMAGE_FILE_SYSTEM_TYPE),$(hide) echo "vendor_fs_type=$(BOARD_VENDORIMAGE_FILE_SYSTEM_TYPE)" >> $(1))
 $(if $(BOARD_VENDORIMAGE_PARTITION_SIZE),$(hide) echo "vendor_size=$(BOARD_VENDORIMAGE_PARTITION_SIZE)" >> $(1))
-$(if $(BOARD_VENDORIMAGE_JOURNAL_SIZE),$(hide) echo "vendor_journal_size=$(BOARD_VENDORIMAGE_JOURNAL_SIZE)" >> $(1))
 $(if $(BOARD_OEMIMAGE_PARTITION_SIZE),$(hide) echo "oem_size=$(BOARD_OEMIMAGE_PARTITION_SIZE)" >> $(1))
-$(if $(BOARD_OEMIMAGE_JOURNAL_SIZE),$(hide) echo "oem_journal_size=$(BOARD_OEMIMAGE_JOURNAL_SIZE)" >> $(1))
 $(if $(INTERNAL_USERIMAGES_SPARSE_EXT_FLAG),$(hide) echo "extfs_sparse_flag=$(INTERNAL_USERIMAGES_SPARSE_EXT_FLAG)" >> $(1))
-$(if $(INTERNAL_TRANSPARENT_COMPRESSION_METHOD),$(hide) echo "transparent_compression_method=$(INTERNAL_TRANSPARENT_COMPRESSION_METHOD)" >> $(1))
 $(if $(mkyaffs2_extra_flags),$(hide) echo "mkyaffs2_extra_flags=$(mkyaffs2_extra_flags)" >> $(1))
 $(hide) echo "selinux_fc=$(SELINUX_FC)" >> $(1)
 $(if $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_SUPPORTS_VERITY),$(hide) echo "verity=$(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_SUPPORTS_VERITY)" >> $(1))
@@ -817,19 +796,15 @@ recovery_resources_common := $(call project-path-for,recovery)/res
 
 # Set recovery_density to the density bucket of the device.
 recovery_density := unknown
-ifneq (,$(TARGET_RECOVERY_DENSITY))
-recovery_density := $(filter %dpi,$(TARGET_RECOVERY_DENSITY))
-else
 ifneq (,$(PRODUCT_AAPT_PREF_CONFIG))
-ifeq ($(PRODUCT_AAPT_PREF_CONFIG),560dpi)
-recovery_density := xxxhdpi
-else
 # If PRODUCT_AAPT_PREF_CONFIG includes a dpi bucket, then use that value.
 recovery_density := $(filter %dpi,$(PRODUCT_AAPT_PREF_CONFIG))
-endif
 else
-# Otherwise, use the default medium density.
-recovery_densities := mdpi
+# Otherwise, use the highest density that appears in PRODUCT_AAPT_CONFIG.
+# Order is important here; we'll take the first one that's found.
+recovery_densities := $(filter $(PRODUCT_AAPT_CONFIG_SP),xxxhdpi xxhdpi xhdpi hdpi tvdpi mdpi ldpi)
+ifneq (,$(recovery_densities))
+recovery_density := $(word 1,$(recovery_densities))
 endif
 endif
 
@@ -916,7 +891,8 @@ ifneq ($(OTA_PACKAGE_SIGNING_KEY),)
 else
     PRODUCT_EXTRA_RECOVERY_KEYS += \
         build/target/product/security/cm \
-        build/target/product/security/cm-devkey
+        build/target/product/security/cm-devkey \
+        build/target/product/security/bacon
 endif
 
 # Generate a file containing the keys that will be read by the
@@ -945,7 +921,7 @@ $(recovery_ramdisk): $(MKBOOTFS) $(MINIGZIP) $(RECOVERYIMAGE_EXTRA_DEPS) \
 	$(hide) mkdir -p $(TARGET_RECOVERY_OUT)
 	$(hide) mkdir -p $(TARGET_RECOVERY_ROOT_OUT)/etc $(TARGET_RECOVERY_ROOT_OUT)/tmp
 	@echo -e ${CL_CYN}"Copying baseline ramdisk..."${CL_RST}
-	$(hide) (cd $(PRODUCT_OUT) && tar -cf - $(TARGET_COPY_OUT_ROOT) | (cd $(TARGET_RECOVERY_OUT) && tar -xf -))
+	$(hide) cp -R $(TARGET_ROOT_OUT) $(TARGET_RECOVERY_OUT)
 	@echo -e ${CL_CYN}"Modifying ramdisk contents..."${CL_RST}
 	$(hide) rm -f $(TARGET_RECOVERY_ROOT_OUT)/init*.rc
 	$(hide) cp -f $(recovery_initrc) $(TARGET_RECOVERY_ROOT_OUT)/
@@ -958,11 +934,11 @@ $(recovery_ramdisk): $(MKBOOTFS) $(MINIGZIP) $(RECOVERYIMAGE_EXTRA_DEPS) \
 	$(hide) cp -rf $(recovery_resources_common)/* $(TARGET_RECOVERY_ROOT_OUT)/res
 	$(hide) cp -f $(recovery_font) $(TARGET_RECOVERY_ROOT_OUT)/res/images/font.png
 	$(hide) $(foreach item,$(recovery_root_private), \
-	  cp -rf $(item) $(TARGET_RECOVERY_OUT)/;)
+	  cp -rf $(item) $(TARGET_RECOVERY_OUT)/)
 	$(hide) $(foreach item,$(recovery_resources_private), \
-	  cp -rf $(item) $(TARGET_RECOVERY_ROOT_OUT)/;)
+	  cp -rf $(item) $(TARGET_RECOVERY_ROOT_OUT)/)
 	$(hide) $(foreach item,$(recovery_fstab), \
-	  cp -f $(item) $(TARGET_RECOVERY_ROOT_OUT)/etc/recovery.fstab;)
+	  cp -f $(item) $(TARGET_RECOVERY_ROOT_OUT)/etc/recovery.fstab)
 	$(hide) cp $(RECOVERY_INSTALL_OTA_KEYS) $(TARGET_RECOVERY_ROOT_OUT)/res/keys
 	$(hide) cat $(INSTALLED_DEFAULT_PROP_TARGET) $(recovery_build_prop) \
 	        > $(TARGET_RECOVERY_ROOT_OUT)/default.prop
@@ -971,13 +947,18 @@ $(recovery_ramdisk): $(MKBOOTFS) $(MINIGZIP) $(RECOVERYIMAGE_EXTRA_DEPS) \
 $(recovery_uncompressed_ramdisk): $(MINIGZIP) $(recovery_ramdisk)
 	@echo -e ${CL_CYN}"----- Making uncompressed recovery ramdisk ------"${CL_RST}
 	$(MKBOOTFS) $(TARGET_RECOVERY_ROOT_OUT) > $@
+ifneq ($(TARGET_MKIMAGE),)
+		$(call pretty,"Appending MTK header to ramdisk")
+		$(hide) mv $@ $(recovery_ramdisk).tmp
+		$(hide) $(TARGET_MKIMAGE) $(recovery_ramdisk).tmp RECOVERY > $@
+endif
 
 ifndef BOARD_CUSTOM_BOOTIMG_MK
 $(INSTALLED_RECOVERYIMAGE_TARGET): $(MKBOOTIMG) $(recovery_ramdisk) \
 		$(recovery_kernel)
 	$(hide) $(MKBOOTIMG) $(INTERNAL_RECOVERYIMAGE_ARGS) $(BOARD_MKBOOTIMG_ARGS) --output $@
 ifeq (true,$(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_SUPPORTS_VERITY))
-	$(BOOT_SIGNER) /recovery $@ $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).pk8 $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY).x509.pem $@
+	$(BOOT_SIGNER) /recovery $@ $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_VERITY_SIGNING_KEY) $@
 endif
 	$(hide) $(call assert-max-image-size,$@,$(BOARD_RECOVERYIMAGE_PARTITION_SIZE))
 	@echo -e ${CL_CYN}"Made recovery image: $@"${CL_RST}
@@ -1106,23 +1087,15 @@ SYSTEMIMAGE_SOURCE_DIR := $(TARGET_OUT)
 # image size check calculation.
 ifneq ($(INSTALLED_RECOVERYIMAGE_TARGET),)
 intermediates := $(call intermediates-dir-for,PACKAGING,recovery_patch)
-ifndef BOARD_CANT_BUILD_RECOVERY_FROM_BOOT_PATCH
 RECOVERY_FROM_BOOT_PATCH := $(intermediates)/recovery_from_boot.p
-else
-RECOVERY_FROM_BOOT_PATCH :=
-endif
 $(RECOVERY_FROM_BOOT_PATCH): $(INSTALLED_RECOVERYIMAGE_TARGET) \
                              $(INSTALLED_BOOTIMAGE_TARGET) \
 			     $(HOST_OUT_EXECUTABLES)/imgdiff \
 	                     $(HOST_OUT_EXECUTABLES)/bsdiff
 	@echo -e ${CL_CYN}"Construct recovery from boot"${CL_RST}
 	mkdir -p $(dir $@)
-ifeq ($(TARGET_NOT_USE_GZIP_RECOVERY_RAMDISK),true)
-	PATH=$(HOST_OUT_EXECUTABLES):$$PATH $(HOST_OUT_EXECUTABLES)/bsdiff $(INSTALLED_BOOTIMAGE_TARGET) $(INSTALLED_RECOVERYIMAGE_TARGET) $@
-else
 	PATH=$(HOST_OUT_EXECUTABLES):$$PATH $(HOST_OUT_EXECUTABLES)/imgdiff $(INSTALLED_BOOTIMAGE_TARGET) $(INSTALLED_RECOVERYIMAGE_TARGET) $@
 endif
-endif
 
 
 $(INSTALLED_SYSTEMIMAGE): $(BUILT_SYSTEMIMAGE) $(RECOVERY_FROM_BOOT_PATCH) | $(ACP)
@@ -1372,12 +1345,6 @@ vendorimage-nodeps: | $(INTERNAL_USERIMAGES_DEPS)
 
 endif # BOARD_VENDORIMAGE_FILE_SYSTEM_TYPE
 
-# -----------------------------------------------------------------
-# bring in the installer image generation defines if necessary
-ifeq ($(TARGET_USE_DISKINSTALLER),true)
-include bootable/diskinstaller/config.mk
-endif
-
 # -----------------------------------------------------------------
 # host tools needed to build dist and OTA packages
 
@@ -1385,7 +1352,6 @@ DISTTOOLS :=  $(HOST_OUT_EXECUTABLES)/minigzip \
 	  $(HOST_OUT_EXECUTABLES)/adb \
 	  $(HOST_OUT_EXECUTABLES)/mkbootfs \
 	  $(HOST_OUT_EXECUTABLES)/mkbootimg \
-	  $(HOST_OUT_EXECUTABLES)/unpackbootimg \
 	  $(HOST_OUT_EXECUTABLES)/fs_config \
 	  $(HOST_OUT_EXECUTABLES)/mkyaffs2image \
 	  $(HOST_OUT_EXECUTABLES)/zipalign \
@@ -1402,11 +1368,6 @@ DISTTOOLS :=  $(HOST_OUT_EXECUTABLES)/minigzip \
 	  $(HOST_OUT_EXECUTABLES)/append2simg \
 	  $(HOST_OUT_EXECUTABLES)/boot_signer
 
-ifneq (,$(filter linux darwin,$(HOST_OS)))
-  DISTTOOLS += $(HOST_OUT_EXECUTABLES)/mkf2fsuserimg.sh \
-	$(HOST_OUT_EXECUTABLES)/make_f2fs
-endif
-
 OTATOOLS := $(DISTTOOLS) \
 	  $(HOST_OUT_EXECUTABLES)/aapt
 
@@ -1444,12 +1405,8 @@ built_ota_tools := \
 	$(call intermediates-dir-for,EXECUTABLES,applypatch)/applypatch \
 	$(call intermediates-dir-for,EXECUTABLES,applypatch_static)/applypatch_static \
 	$(call intermediates-dir-for,EXECUTABLES,check_prereq)/check_prereq \
-	$(call intermediates-dir-for,EXECUTABLES,sqlite3)/sqlite3
-ifeq ($(TARGET_ARCH),arm64)
-	built_ota_tools += $(call intermediates-dir-for,EXECUTABLES,updater,,,32)/updater
-else
-	built_ota_tools += $(call intermediates-dir-for,EXECUTABLES,updater)/updater
-endif
+	$(call intermediates-dir-for,EXECUTABLES,sqlite3)/sqlite3 \
+	$(call intermediates-dir-for,EXECUTABLES,updater)/updater
 $(BUILT_TARGET_FILES_PACKAGE): PRIVATE_OTA_TOOLS := $(built_ota_tools)
 
 $(BUILT_TARGET_FILES_PACKAGE): PRIVATE_RECOVERY_API_VERSION := $(RECOVERY_API_VERSION)
@@ -1479,7 +1436,6 @@ endif
 
 # Depending on the various images guarantees that the underlying
 # directories are up-to-date.
-include $(BUILD_SYSTEM)/tasks/oem_image.mk
 $(BUILT_TARGET_FILES_PACKAGE): \
 		$(INSTALLED_BOOTIMAGE_TARGET) \
 		$(INSTALLED_RADIOIMAGE_TARGET) \
@@ -1488,7 +1444,6 @@ $(BUILT_TARGET_FILES_PACKAGE): \
 		$(INSTALLED_USERDATAIMAGE_TARGET) \
 		$(INSTALLED_CACHEIMAGE_TARGET) \
 		$(INSTALLED_VENDORIMAGE_TARGET) \
-		$(INSTALLED_OEMIMAGE_TARGET) \
 		$(INSTALLED_ANDROID_INFO_TXT_TARGET) \
 		$(SELINUX_FC) \
 		$(built_ota_tools) \
@@ -1502,8 +1457,6 @@ $(BUILT_TARGET_FILES_PACKAGE): \
 	$(hide) mkdir -p $(zip_root)/RECOVERY
 	$(hide) $(call package_files-copy-root, \
 		$(TARGET_RECOVERY_ROOT_OUT),$(zip_root)/RECOVERY/RAMDISK)
-	@# OTA install helpers
-	$(hide) $(call package_files-copy-root, $(OUT)/install, $(zip_root)/INSTALL)
 ifdef INSTALLED_KERNEL_TARGET
 	$(hide) $(ACP) $(INSTALLED_KERNEL_TARGET) $(zip_root)/RECOVERY/kernel
 endif
@@ -1530,7 +1483,12 @@ ifdef BOARD_RAMDISK_OFFSET
 	$(hide) echo "$(BOARD_RAMDISK_OFFSET)" > $(zip_root)/RECOVERY/ramdisk_offset
 endif
 ifeq ($(strip $(BOARD_KERNEL_SEPARATED_DT)),true)
-	$(hide) $(ACP) $(INSTALLED_DTIMAGE_TARGET) $(zip_root)/RECOVERY/dt
+	$(hide) echo "$(INSTALLED_DTIMAGE_TARGET)" > $(zip_root)/RECOVERY/dt_args
+endif
+ifdef TARGET_USE_BUILT_BOOTIMAGE
+	@# Copy boot.img to bootable_images
+	$(hide) mkdir -p $(zip_root)/BOOTABLE_IMAGES
+	$(hide) $(ACP) $(INSTALLED_BOOTIMAGE_TARGET) $(zip_root)/BOOTABLE_IMAGES
 endif
 	@# Components of the boot image
 	$(hide) mkdir -p $(zip_root)/BOOT
@@ -1564,7 +1522,7 @@ ifdef BOARD_RAMDISK_OFFSET
 endif
 
 ifeq ($(strip $(BOARD_KERNEL_SEPARATED_DT)),true)
-	$(hide) $(ACP) $(INSTALLED_DTIMAGE_TARGET) $(zip_root)/BOOT/dt
+	$(hide) echo "$(INSTALLED_DTIMAGE_TARGET)" > $(zip_root)/BOOT/dt_args
 endif
 ifdef ZIP_SAVE_UBOOTIMG_ARGS
 	$(hide) echo "$(ZIP_SAVE_UBOOTIMG_ARGS)" > $(zip_root)/BOOT/ubootargs
@@ -1572,9 +1530,6 @@ endif
 	$(hide) $(foreach t,$(INSTALLED_RADIOIMAGE_TARGET),\
 	            mkdir -p $(zip_root)/RADIO; \
 	            $(ACP) $(t) $(zip_root)/RADIO/$(notdir $(t));)
-	$(hide) $(foreach fi,$(PRODUCT_FACTORYIMAGE_FILES),\
-		mkdir -p $(zip_root)/FACTORY; \
-		$(ACP) $(fi) $(zip_root)/FACTORY/$(notdir $(fi));)
 	@# Contents of the system image
 	$(hide) $(call package_files-copy-root, \
 		$(SYSTEMIMAGE_SOURCE_DIR),$(zip_root)/SYSTEM)
@@ -1591,11 +1546,6 @@ ifdef BOARD_VENDORIMAGE_FILE_SYSTEM_TYPE
 	@# Contents of the vendor image
 	$(hide) $(call package_files-copy-root, \
 		$(TARGET_OUT_VENDOR),$(zip_root)/VENDOR)
-endif
-ifdef BOARD_OEMIMAGE_FILE_SYSTEM_TYPE
-	@# Contents of the oem image
-	$(call package_files-copy-root, \
-		$(TARGET_OUT_OEM),$(zip_root)/OEM)
 endif
 	@# Extra contents of the OTA package
 	$(hide) mkdir -p $(zip_root)/OTA/bin
@@ -1633,16 +1583,12 @@ endif
 	$(hide) echo "use_set_metadata=1" >> $(zip_root)/META/misc_info.txt
 	$(hide) echo "multistage_support=1" >> $(zip_root)/META/misc_info.txt
 	$(hide) echo "update_rename_support=1" >> $(zip_root)/META/misc_info.txt
-	$(hide) echo "blockimgdiff_versions=1,2,3" >> $(zip_root)/META/misc_info.txt
 ifneq ($(OEM_THUMBPRINT_PROPERTIES),)
 	# OTA scripts are only interested in fingerprint related properties
 	$(hide) echo "oem_fingerprint_properties=$(OEM_THUMBPRINT_PROPERTIES)" >> $(zip_root)/META/misc_info.txt
 endif
 ifdef BUILD_NO
 	$(hide) echo "build_number=$(BUILD_NO)" >> $(zip_root)/META/misc_info.txt
-endif
-ifdef TARGET_RELEASETOOL_FACTORY_FROM_TARGET_SCRIPT
-	$(hide) echo "factory_from_target_script=$(TARGET_RELEASETOOL_FACTORY_FROM_TARGET_SCRIPT)" >> $(zip_root)/META/misc_info.txt
 endif
 	$(call generate-userimage-prop-dictionary, $(zip_root)/META/misc_info.txt)
 ifeq ($(TARGET_RELEASETOOL_MAKE_RECOVERY_PATCH_SCRIPT),)
@@ -1726,8 +1672,6 @@ $(INTERNAL_OTA_PACKAGE_TARGET): $(BUILT_TARGET_FILES_PACKAGE) $(DISTTOOLS)
 	@echo -e ${CL_YLW}"Package OTA:"${CL_RST}" $@"
 	$(hide) MKBOOTIMG=$(MKBOOTIMG) \
 	   $(OTA_FROM_TARGET_SCRIPT) -v \
-	   --block \
-	   $(if $(WITH_LZMA_OTA), -z) \
 	   -p $(HOST_OUT) \
 	   -k $(KEY_CERT_PAIR) \
 	   --backup=$(backuptool) \
@@ -1744,35 +1688,6 @@ bacon: otapackage
 	$(hide) $(MD5SUM) $(CM_TARGET_PACKAGE) > $(CM_TARGET_PACKAGE).md5sum
 	@echo -e ${CL_CYN}"Package Complete: $(CM_TARGET_PACKAGE)"${CL_RST}
 
-# -----------------------------------------------------------------
-# The factory package
-
-name := $(TARGET_PRODUCT)-factory-$(FILE_NAME_TAG)
-
-INTERNAL_FACTORY_PACKAGE_TARGET := $(PRODUCT_OUT)/$(name).zip
-
-ifeq ($(TARGET_RELEASETOOLS_EXTENSIONS),)
-# default to common dir for device vendor
-$(INTERNAL_FACTORY_PACKAGE_TARGET): extensions := $(TARGET_DEVICE_DIR)/../common
-else
-$(INTERNAL_FACTORY_PACKAGE_TARGET): extensions := $(TARGET_RELEASETOOLS_EXTENSIONS)
-endif
-
-$(INTERNAL_FACTORY_PACKAGE_TARGET): $(BUILT_TARGET_FILES_PACKAGE) $(DISTTOOLS)
-	@echo -e ${CL_YLW}"Package:"${CL_RST}" $@"
-	if [ -z $(TARGET_RELEASETOOL_FACTORY_FROM_TARGET_SCRIPT) ]; then \
-          echo "Error: Factory script is not defined by target"; \
-          exit 1; \
-	fi
-	MKBOOTIMG=$(BOARD_CUSTOM_BOOTIMG_MK) \
-	$(TARGET_RELEASETOOL_FACTORY_FROM_TARGET_SCRIPT) -v \
-	   -s $(extensions) \
-	   -p $(HOST_OUT) \
-	   $(BUILT_TARGET_FILES_PACKAGE) $@
-
-.PHONY: factorypackage
-factorypackage: $(INTERNAL_FACTORY_PACKAGE_TARGET)
-
 endif    # recovery_fstab is defined
 endif    # TARGET_NO_KERNEL != true
 endif    # TARGET_DEVICE != generic*
diff --git a/core/android_manifest.mk b/core/android_manifest.mk
index c641b75..21b95c2 100644
--- a/core/android_manifest.mk
+++ b/core/android_manifest.mk
@@ -11,27 +11,13 @@ else
   full_android_manifest := $(LOCAL_PATH)/$(LOCAL_MANIFEST_FILE)
 endif
 
-my_full_libs_manifest_files := $(LOCAL_FULL_LIBS_MANIFEST_FILES)
-my_full_libs_manifest_deps := $(LOCAL_FULL_LIBS_MANIFEST_FILES)
-
-# Set up dependency on aar libraries
-ifdef LOCAL_STATIC_JAVA_AAR_LIBRARIES
-my_full_libs_manifest_deps += $(foreach lib, $(LOCAL_STATIC_JAVA_AAR_LIBRARIES),\
-  $(call intermediates-dir-for,JAVA_LIBRARIES,$(lib),,COMMON)/aar/classes.jar)
-my_full_libs_manifest_files += $(foreach lib, $(LOCAL_STATIC_JAVA_AAR_LIBRARIES),\
-  $(call intermediates-dir-for,JAVA_LIBRARIES,$(lib),,COMMON)/aar/AndroidManifest.xml)
-
-LOCAL_RESOURCE_DIR += $(foreach lib, $(LOCAL_STATIC_JAVA_AAR_LIBRARIES),\
-  $(call intermediates-dir-for,JAVA_LIBRARIES,$(lib),,COMMON)/aar/res)
-endif
-
 # Set up rules to merge library manifest files
-ifdef my_full_libs_manifest_files
+ifdef LOCAL_FULL_LIBS_MANIFEST_FILES
 main_android_manifest := $(full_android_manifest)
 full_android_manifest := $(intermediates.COMMON)/AndroidManifest.xml
-$(full_android_manifest): PRIVATE_LIBS_MANIFESTS := $(my_full_libs_manifest_files)
-$(full_android_manifest) : $(main_android_manifest) $(my_full_libs_manifest_deps)
-	@echo "Merge android manifest files: $@ <-- $< $(PRIVATE_LIBS_MANIFESTS)"
+$(full_android_manifest): PRIVATE_LIBS_MANIFESTS := $(LOCAL_FULL_LIBS_MANIFEST_FILES)
+$(full_android_manifest) : $(main_android_manifest) $(LOCAL_FULL_LIBS_MANIFEST_FILES)
+	@echo "Merge android manifest files: $@ <-- $^"
 	@mkdir -p $(dir $@)
 	$(hide) $(ANDROID_MANIFEST_MERGER) --main $< --libs $(PRIVATE_LIBS_MANIFESTS) \
 	    --out $@
diff --git a/core/apicheck_msg_current.txt b/core/apicheck_msg_current.txt
index 99fd547..0603c18 100644
--- a/core/apicheck_msg_current.txt
+++ b/core/apicheck_msg_current.txt
@@ -7,76 +7,8 @@ To make these errors go away, you have two choices:
       errors above.
 
    2) You can update current.txt by executing the following command:
-         make update-api
-
+         make %UPDATE_API%
       ^^^^^^^^^^^^^^^^^^
-    CONGRATS YOU EARNED A QUAIL STAR!
-
-                                              M                                                                        
-                                               MM                                                                       
-                                              MMM                                                                       
-                                              M.MM                                                                      
-                                             MM  M                                                                      
-                                            7M   MM                                                                     
-                                       MMMMM       MMMMM                                                                
-                                     MMMMM           .MMMMM                                                             
-                                          MMMM   MMMM                                                                   
-                                             MM  M                                                                      
-                          MM                  M .M                                                                      
-                          M+M                 MMMM                                                                      
-                         .M++MM               .MM                                                                       
-                         MM+++MM               MM                                                                       
-               8NNNNN   MM+++++MM                                                                                       
-               NNNN  $Z8. MM+++++MM                                                        MM                           
-               MM   $Z8M7IMNN+++++MM                                                       MM                           
-                  .$$$D ~NNMNN+++++MM                                                     MMMM                          
-                    INNNNM NMNM++++++M                                                    M  M                          
-                      NNO:NI=MM+++++++MM                                                 MM  MM                         
-                      8M$MMMMMD?+++++++MM       .MMMMMMMMMMMMMMM                     MMMMN    MMMMM                     
-                     M$$NMMMMMM$++++++++MMMMMMM=+++++++++++++MM                 MMMMM              MMMMM                
-                    M77$IMMMMMN.,+++++++++++++++++++++++++++MM                      .MMMMM    MMMMM                     
-                   .??I8,?M777OM.?+++++++++++++++++++++++++MM                            MM  MM                         
-                  O==?M7MM$MMI7$.~M+++++++++++++++++++++++MM                             .M  M                          
-              NMMM+~M??MMMMMMMMMMMI$$++++++++++++++++++++MM                               MMMM                          
-          MMMM++++MM~=+I$OMMMOO?7M$Z$$$+++++++++++++++++MM                                 MM                           
-      NMMM++++++++~~MO~7$OM8O8OMZZ$Z$M$$M++++++++++++++MM7MMM                              MM                           
-  MMMM++++++++++++==D~M~:8N88MMOMMZDM$$Z$$M+++++++++++MM77777MMM                                                        
-MMM+++++++++++++++~MM~~M $O,NM88MOMMZ$$MM$$$+++++++++MM777777777MMMM                                                    
-   MMM++++++++++++M~M~IMMMO888NMOMMOZM$ZZDZ$$+++++++MM7777777777777OMMZ                                                 
-     MMM+++++++++++~~M~~MDOOMMO8NOOOOZZ$$Z.Z$$M++++MM77777777777777777MMM                                               
-        MMM++++++++M.Z, D+ 8O88M8D,OOMDZZ$D.$$$N+++M7MMMMMD77777777777777MMM                                            
-          .MM+++++++MM:.D:ZMMM8888OOOOOOZZ$ND$$$M++MM777777MMMM7777777777777MMD                                         
-             MMM+++++~M.$.M~,~7M8?MON MOOZZ$$N$$$M++MD777777777MMMM77777777777MMM                                       
-                MM=+++=ZMZ.MM MMZOOOO88OOZM$M.$$$$+++M7777777777777MMMM7777777777MM                                     
-                  MMM++MM~,,$M.+~M$OOMOOMZMI$$$$$$$++MM7777777777777777MMM777777777MM                                   
-                   MM++++=. ~$$.$.M~M$MZOM7MMZ$$$$$$++MMMMMMD7777777777777MMMI7777777MMM                                
-                   .M++++++MM+OMI$7M??N+OZM8MMMD$$M$$++M77777MMMMN77777777777MMM7777777MMM                              
-                    M++++++++M+=?+++++++++++MNMZN$$N$$+MM777777777MMMM7777777777MMM777777MM,                            
-                    M+++++M=?7$$M+++++++++++++++$NO$$$$+M7777777777777MMMM777777777MMM77777MM                           
-                    M++~M$M$M+++++M++MMM++++++++++M=$$D$MMMMMMMM7777777777MMM$7777777MMM77777MM                         
-                    M+M$$$M+++++++++MM   MMMMM+++++++M$Z$$M     MMMMMI7777777MMMM7777777MM77777MM                       
-                    M++7NMIN++Z++NMM           MMMMM+++N$M$M          MMMM7777777MMM777777MM$777MM                      
-                    M=++8+++++++MM                  MMMMMZ$M$M            MMMM777777MMM77777MMZ777MM                    
-                    MM++++++++MM                          MM$                 MMM77777MMM77777MM7777MM                  
-                    MM++++++MM                                                   MMMM7777MMM7777MM777MM                 
-                    MM++++MMM                                                       .MMM7777MM7777MM77$M                
-                    MM+++MM                                       M                     MMM777MMN777MM77MM              
-                    NM+MM                                         M                        MMM77MMM77NMM7MM             
-                     MM                                          MM                          MMM77MMM77MM77M            
-                                                                .MMM                            MMM7MMM7IMM7MM          
-                                                                MM M                              MMM7MMM7MM7MM         
-                                                                M  MM                                MM7MMN7MMMM        
-                                                             MMMM   MMMM                               MMMMMIMMMM       
-                                                        MMMM.           MMMMM                            MMMMMMMMM      
-                                                          MMMMM       MMMMM                                MMMMMMMM     
-                                                               MM   MM                                       OMMMMMM    
-                                                                M  MM                                          MMMMMM   
-                                                                MM M                                             MMMMM  
-                                                                 MMM                                               MMM  
-                                                                 MM                                                  MM 
-                                                                  M                                                     
-
-
       NO. NO. STOP BEING LAZY. SERIOUSLY.
       DO NOT DO THIS in CM. THIS IS A LIE. IT WILL BREAK THINGS.
 
diff --git a/core/base_rules.mk b/core/base_rules.mk
index eea1007..0deb781 100644
--- a/core/base_rules.mk
+++ b/core/base_rules.mk
@@ -181,7 +181,7 @@ ifneq (true,$(LOCAL_UNINSTALLABLE_MODULE))
   # Apk and its attachments reside in its own subdir.
   ifeq ($(LOCAL_MODULE_CLASS),APPS)
   # framework-res.apk doesn't like the additional layer.
-  ifeq ($(filter true,$(LOCAL_NO_STANDARD_LIBRARIES) $(LOCAL_IGNORE_SUBDIR)),)
+  ifneq ($(LOCAL_NO_STANDARD_LIBRARIES),true)
     my_module_path := $(my_module_path)/$(LOCAL_MODULE)
   endif
   endif
diff --git a/core/binary.mk b/core/binary.mk
index 170e20c..88e45ef 100644
--- a/core/binary.mk
+++ b/core/binary.mk
@@ -184,11 +184,9 @@ endif
 
 my_compiler_dependencies :=
 
-##################################################################
+####################################################
 ## Add FDO flags if FDO is turned on and supported
-## Please note that we will do option filtering during FDO build.
-## i.e. Os->O2, remove -fno-early-inline and -finline-limit.
-##################################################################
+####################################################
 ifeq ($(strip $(LOCAL_FDO_SUPPORT)), true)
   ifeq ($(strip $(LOCAL_IS_HOST_MODULE)),)
     my_cflags += $($(LOCAL_2ND_ARCH_VAR_PREFIX)TARGET_FDO_CFLAGS)
@@ -929,21 +927,6 @@ my_asflags := $(call $(LOCAL_2ND_ARCH_VAR_PREFIX)convert-to-$(my_host)clang-flag
 my_ldflags := $(call $(LOCAL_2ND_ARCH_VAR_PREFIX)convert-to-$(my_host)clang-flags,$(my_ldflags))
 endif
 
-ifeq ($(LOCAL_FDO_SUPPORT), true)
-  build_with_fdo := false
-  ifeq ($(BUILD_FDO_INSTRUMENT), true)
-    build_with_fdo := true
-  endif
-  ifeq ($(BUILD_FDO_OPTIMIZE), true)
-    build_with_fdo := true
-  endif
-  ifeq ($(build_with_fdo), true)
-    my_cflags := $(patsubst -Os,-O2,$(my_cflags))
-    fdo_incompatible_flags=-fno-early-inlining -finline-limit=%
-    my_cflags := $(filter-out $(fdo_incompatible_flags),$(my_cflags))
-  endif
-endif
-
 $(LOCAL_INTERMEDIATE_TARGETS): PRIVATE_YACCFLAGS := $(LOCAL_YACCFLAGS)
 $(LOCAL_INTERMEDIATE_TARGETS): PRIVATE_ASFLAGS := $(my_asflags)
 $(LOCAL_INTERMEDIATE_TARGETS): PRIVATE_CONLYFLAGS := $(LOCAL_CONLYFLAGS)
diff --git a/core/clang/HOST_x86_common.mk b/core/clang/HOST_x86_common.mk
index 77547b7..cec0ca9 100644
--- a/core/clang/HOST_x86_common.mk
+++ b/core/clang/HOST_x86_common.mk
@@ -7,9 +7,8 @@ endif
 ifeq ($(HOST_OS),linux)
 CLANG_CONFIG_x86_LINUX_HOST_EXTRA_ASFLAGS := \
   --gcc-toolchain=$($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG) \
-  --sysroot=$($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG)/sysroot \
-  -B$($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG)/x86_64-linux/bin \
-  -no-integrated-as
+  --sysroot $($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG)/sysroot \
+  -B$($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG)/x86_64-linux/bin
 
 CLANG_CONFIG_x86_LINUX_HOST_EXTRA_CFLAGS := \
   --gcc-toolchain=$($(clang_2nd_arch_prefix)HOST_TOOLCHAIN_FOR_CLANG) \
diff --git a/core/clang/config.mk b/core/clang/config.mk
index 9c11797..5b2aea5 100644
--- a/core/clang/config.mk
+++ b/core/clang/config.mk
@@ -42,11 +42,6 @@ CLANG_CONFIG_EXTRA_CFLAGS += \
 CLANG_CONFIG_EXTRA_CFLAGS += \
   -Werror=int-conversion
 
-# Workaround for ccache with clang.
-# See http://petereisentraut.blogspot.com/2011/05/ccache-and-clang.html.
-CLANG_CONFIG_EXTRA_CFLAGS += \
-  -Wno-unused-command-line-argument
-
 CLANG_CONFIG_UNKNOWN_CFLAGS := \
   -funswitch-loops \
   -fno-tree-sra \
diff --git a/core/cleanbuild.mk b/core/cleanbuild.mk
index 8d19993..48c4b7b 100644
--- a/core/cleanbuild.mk
+++ b/core/cleanbuild.mk
@@ -132,11 +132,23 @@ endif  # if not ONE_SHOT_MAKEFILE dont_bother
 
 previous_build_config_file := $(PRODUCT_OUT)/previous_build_config.mk
 
+# TODO: this special case for the sdk is only necessary while "sdk"
+# is a valid make target.  Eventually, it will just be a product, at
+# which point TARGET_PRODUCT will handle it and we can avoid this check
+# of MAKECMDGOALS.  The "addprefix" is just to keep things pretty.
+ifneq ($(TARGET_PRODUCT),sdk)
+  building_sdk := $(addprefix -,$(filter sdk,$(MAKECMDGOALS)))
+else
+  # Don't bother with this extra part when explicitly building the sdk product.
+  building_sdk :=
+endif
+
 # A change in the list of aapt configs warrants an installclean, too.
 aapt_config_list := $(strip $(PRODUCT_AAPT_CONFIG) $(PRODUCT_AAPT_PREF_CONFIG))
 
 current_build_config := \
-    $(TARGET_PRODUCT)-$(TARGET_BUILD_VARIANT)-{$(aapt_config_list)}
+    $(TARGET_PRODUCT)-$(TARGET_BUILD_VARIANT)$(building_sdk)-{$(aapt_config_list)}
+building_sdk :=
 aapt_config_list :=
 force_installclean := false
 
@@ -209,7 +221,6 @@ installclean_files := \
 	$(PRODUCT_OUT)/obj/JAVA_LIBRARIES \
 	$(PRODUCT_OUT)/obj/FAKE \
 	$(PRODUCT_OUT)/obj/EXECUTABLES/adbd_intermediates \
-	$(PRODUCT_OUT)/obj/STATIC_LIBRARIES/libfs_mgr_intermediates \
 	$(PRODUCT_OUT)/obj/EXECUTABLES/init_intermediates \
 	$(PRODUCT_OUT)/obj/ETC/mac_permissions.xml_intermediates \
 	$(PRODUCT_OUT)/obj/ETC/sepolicy_intermediates \
diff --git a/core/clear_vars.mk b/core/clear_vars.mk
index b2891ed..f23c4a6 100644
--- a/core/clear_vars.mk
+++ b/core/clear_vars.mk
@@ -30,7 +30,6 @@ LOCAL_MODULE_TAGS:=
 LOCAL_SRC_FILES:=
 LOCAL_PREBUILT_OBJ_FILES:=
 LOCAL_STATIC_JAVA_LIBRARIES:=
-LOCAL_STATIC_JAVA_AAR_LIBRARIES:=
 LOCAL_STATIC_LIBRARIES:=
 # Group static libraries with "-Wl,--start-group" and "-Wl,--end-group" when linking.
 LOCAL_GROUP_STATIC_LIBRARIES:=
@@ -124,7 +123,6 @@ LOCAL_RENDERSCRIPT_SKIP_INSTALL:=
 LOCAL_RENDERSCRIPT_TARGET_API:=
 LOCAL_DEX_PREOPT:= # '',true,false,nostripping
 LOCAL_DEX_PREOPT_IMAGE_LOCATION:=
-LOCAL_DEX_PREOPT_FLAGS:=
 LOCAL_PROTOC_OPTIMIZE_TYPE:= # lite(default),micro,nano,full
 LOCAL_PROTOC_FLAGS:=
 LOCAL_PROTO_JAVA_OUTPUT_PARAMS:=
@@ -159,8 +157,6 @@ LOCAL_MODULE_TARGET_ARCH_WARN:=
 LOCAL_MODULE_UNSUPPORTED_TARGET_ARCH:=
 LOCAL_MODULE_UNSUPPORTED_TARGET_ARCH_WARN:=
 LOCAL_MODULE_HOST_ARCH:=
-LOCAL_DPI_VARIANTS:=
-LOCAL_DPI_FILE_STEM:=
 
 # arch specific variables
 LOCAL_SRC_FILES_$(TARGET_ARCH):=
@@ -249,9 +245,6 @@ LOCAL_MODULE_STEM_64:=
 LOCAL_CLANG_32:=
 LOCAL_CLANG_64:=
 
-# Include any vendor specific clear_vars.mk file
--include $(TOPDIR)vendor/*/build/core/clear_vars.mk
-
 # Trim MAKEFILE_LIST so that $(call my-dir) doesn't need to
 # iterate over thousands of entries every time.
 # Leave the current makefile to make sure we don't break anything
diff --git a/core/combo/HOST_darwin-x86.mk b/core/combo/HOST_darwin-x86.mk
index fb79c61..44e3063 100644
--- a/core/combo/HOST_darwin-x86.mk
+++ b/core/combo/HOST_darwin-x86.mk
@@ -37,14 +37,11 @@ $(combo_2nd_arch_prefix)HOST_TOOLCHAIN_PREFIX := $($(combo_2nd_arch_prefix)HOST_
 ifneq (,$(strip $(wildcard $($(combo_2nd_arch_prefix)HOST_TOOLCHAIN_PREFIX)-gcc)))
 $(combo_2nd_arch_prefix)HOST_CC  := $($(combo_2nd_arch_prefix)HOST_TOOLCHAIN_PREFIX)-gcc
 $(combo_2nd_arch_prefix)HOST_CXX := $($(combo_2nd_arch_prefix)HOST_TOOLCHAIN_PREFIX)-g++
-
-ifeq ($(mac_sdk_version), $(filter $(mac_sdk_version), 10.8 10.9 10.10 10.11))
-# Mac SDK 10.8 and later does not have stdarg.h, etc
-host_toolchain_header := $(HOST_TOOLCHAIN_ROOT)/lib/gcc/i686-apple-darwin$(gcc_darwin_version)/4.2.1/include
+ifeq ($(mac_sdk_version),10.8)
+# Mac SDK 10.8 no longer has stdarg.h, etc
+host_toolchain_header := $($(combo_2nd_arch_prefix)HOST_TOOLCHAIN_ROOT)/lib/gcc/i686-apple-darwin$(gcc_darwin_version)/4.2.1/include
 $(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += -isystem $(host_toolchain_header)
-$(combo_2nd_arch_prefix)HOST_GLOBAL_CPPFLAGS += -isystem $(mac_sdk_root)/usr/include/c++/4.2.1
-endif # $(mac_sdk_version)
-
+endif
 else
 $(combo_2nd_arch_prefix)HOST_CC := gcc
 $(combo_2nd_arch_prefix)HOST_CXX := g++
@@ -69,7 +66,7 @@ $(combo_2nd_arch_prefix)HOST_JNILIB_SUFFIX := .jnilib
 $(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += \
     -include $(call select-android-config-h,darwin-x86)
 
-ifneq ($(filter 10.7 10.7.% 10.8 10.8.% 10.9 10.9.% 10.10 10.10.% 10.11 10.11.%, $(build_mac_version)),)
+ifneq ($(filter 10.7 10.7.% 10.8 10.8.%, $(build_mac_version)),)
        $(combo_2nd_arch_prefix)HOST_RUN_RANLIB_AFTER_COPYING := false
 else
        $(combo_2nd_arch_prefix)HOST_RUN_RANLIB_AFTER_COPYING := true
diff --git a/core/combo/HOST_darwin-x86_64.mk b/core/combo/HOST_darwin-x86_64.mk
index dfdb704..0bc0227 100644
--- a/core/combo/HOST_darwin-x86_64.mk
+++ b/core/combo/HOST_darwin-x86_64.mk
@@ -37,14 +37,11 @@ HOST_TOOLCHAIN_PREFIX := $(HOST_TOOLCHAIN_ROOT)/bin/i686-apple-darwin$(gcc_darwi
 ifneq (,$(strip $(wildcard $(HOST_TOOLCHAIN_PREFIX)-gcc)))
 HOST_CC  := $(HOST_TOOLCHAIN_PREFIX)-gcc
 HOST_CXX := $(HOST_TOOLCHAIN_PREFIX)-g++
-
-ifeq ($(mac_sdk_version), $(filter $(mac_sdk_version), 10.8 10.9 10.10 10.11))
-# Mac SDK 10.8 and later does not have stdarg.h, etc
+ifeq ($(mac_sdk_version),10.8)
+# Mac SDK 10.8 no longer has stdarg.h, etc
 host_toolchain_header := $(HOST_TOOLCHAIN_ROOT)/lib/gcc/i686-apple-darwin$(gcc_darwin_version)/4.2.1/include
 HOST_GLOBAL_CFLAGS += -isystem $(host_toolchain_header)
-HOST_GLOBAL_CPPFLAGS += -isystem $(mac_sdk_root)/usr/include/c++/4.2.1
-endif # $(mac_sdk_version)
-
+endif
 else
 HOST_CC := gcc
 HOST_CXX := g++
@@ -68,7 +65,7 @@ HOST_JNILIB_SUFFIX := .jnilib
 HOST_GLOBAL_CFLAGS += \
     -include $(call select-android-config-h,darwin-x86)
 
-ifneq ($(filter 10.7 10.7.% 10.8 10.8.% 10.9 10.9.% 10.10 10.10.% 10.11 10.11.%, $(build_mac_version)),)
+ifneq ($(filter 10.7 10.7.% 10.8 10.8.%, $(build_mac_version)),)
        HOST_RUN_RANLIB_AFTER_COPYING := false
 else
        HOST_RUN_RANLIB_AFTER_COPYING := true
diff --git a/core/combo/HOST_windows-x86.mk b/core/combo/HOST_windows-x86.mk
index 00e1974..fdb72a7 100644
--- a/core/combo/HOST_windows-x86.mk
+++ b/core/combo/HOST_windows-x86.mk
@@ -27,7 +27,7 @@ ifneq ($(findstring Linux,$(UNAME)),)
 ifneq ($(strip $(USE_MINGW)),)
 HOST_ACP_UNAVAILABLE := true
 TOOLS_EXE_SUFFIX :=
-$(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += -DUSE_MINGW -DWIN32_LEAN_AND_MEAN
+$(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += -DUSE_MINGW
 $(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += -Wno-unused-parameter
 $(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += --sysroot=prebuilts/gcc/linux-x86/host/x86_64-w64-mingw32-4.8/x86_64-w64-mingw32
 $(combo_2nd_arch_prefix)HOST_GLOBAL_CFLAGS += -m32
diff --git a/core/combo/TARGET_linux-x86.mk b/core/combo/TARGET_linux-x86.mk
index 15ee249..0af3948 100644
--- a/core/combo/TARGET_linux-x86.mk
+++ b/core/combo/TARGET_linux-x86.mk
@@ -149,7 +149,7 @@ define $(combo_2nd_arch_prefix)transform-o-to-shared-lib-inner
 $(hide) $(PRIVATE_CXX) \
 	$(PRIVATE_TARGET_GLOBAL_LDFLAGS) \
 	 -nostdlib -Wl,-soname,$(notdir $@) \
-	 -shared -Bsymbolic \
+	$(if $(filter true,$(PRIVATE_CLANG)),-shared,-Wl,-shared) \
 	$(PRIVATE_TARGET_GLOBAL_LD_DIRS) \
 	$(if $(filter true,$(PRIVATE_NO_CRT)),,$(PRIVATE_TARGET_CRTBEGIN_SO_O)) \
 	$(PRIVATE_ALL_OBJECTS) \
diff --git a/core/combo/TARGET_linux-x86_64.mk b/core/combo/TARGET_linux-x86_64.mk
index 82b1f77..33d6a56 100644
--- a/core/combo/TARGET_linux-x86_64.mk
+++ b/core/combo/TARGET_linux-x86_64.mk
@@ -154,7 +154,7 @@ define transform-o-to-shared-lib-inner
 $(hide) $(PRIVATE_CXX) \
 	$(PRIVATE_TARGET_GLOBAL_LDFLAGS) \
 	 -nostdlib -Wl,-soname,$(notdir $@) \
-	 -shared -Bsymbolic \
+	$(if $(filter true,$(PRIVATE_CLANG)),-shared,-Wl,-shared) \
 	$(PRIVATE_TARGET_GLOBAL_LD_DIRS) \
 	$(if $(filter true,$(PRIVATE_NO_CRT)),,$(PRIVATE_TARGET_CRTBEGIN_SO_O)) \
 	$(PRIVATE_ALL_OBJECTS) \
diff --git a/core/combo/arch/arm/armv7-a-neon.mk b/core/combo/arch/arm/armv7-a-neon.mk
index a49b7f0..4c25e22 100644
--- a/core/combo/arch/arm/armv7-a-neon.mk
+++ b/core/combo/arch/arm/armv7-a-neon.mk
@@ -6,40 +6,19 @@ ARCH_ARM_HAVE_VFP               := true
 ARCH_ARM_HAVE_VFP_D32           := true
 ARCH_ARM_HAVE_NEON              := true
 
-ifneq (,$(filter cortex-a15 denver krait,$(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)))
+ifneq (,$(filter cortex-a15 denver,$(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)))
 	arch_variant_cflags := -mcpu=cortex-a15
-
-	# Fake an ARM compiler flag as these processors support LPAE which GCC/clang
-	# don't advertise.
-	arch_variant_cflags += -D__ARM_FEATURE_LPAE=1
-	arch_variant_ldflags := \
-		-Wl,--no-fix-cortex-a8
 else
 ifeq ($(strip $(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)),cortex-a9)
 	arch_variant_cflags := -mcpu=cortex-a9
-	arch_variant_ldflags := \
-		-Wl,--no-fix-cortex-a8
 else
 ifneq (,$(filter cortex-a8 scorpion,$(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)))
 	arch_variant_cflags := -mcpu=cortex-a8
-	arch_variant_ldflags := \
-		-Wl,--fix-cortex-a8
 else
 ifeq ($(strip $(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)),cortex-a7)
 	arch_variant_cflags := -mcpu=cortex-a7
-	arch_variant_ldflags := \
-		-Wl,--no-fix-cortex-a8
-else
-ifeq ($(strip $(TARGET_$(combo_2nd_arch_prefix)CPU_VARIANT)),cortex-a5)
-	arch_variant_cflags := -mcpu=cortex-a7
-	arch_variant_ldflags := \
-		-Wl,--no-fix-cortex-a8
 else
 	arch_variant_cflags := -march=armv7-a
-	# Generic ARM might be a Cortex A8 -- better safe than sorry
-	arch_variant_ldflags := \
-		-Wl,--fix-cortex-a8
-endif
 endif
 endif
 endif
@@ -48,3 +27,6 @@ endif
 arch_variant_cflags += \
     -mfloat-abi=softfp \
     -mfpu=neon
+
+arch_variant_ldflags := \
+	-Wl,--fix-cortex-a8
diff --git a/core/combo/arch/arm64/armv8-a.mk b/core/combo/arch/arm64/armv8-a.mk
index 642fe91..edc0497 100644
--- a/core/combo/arch/arm64/armv8-a.mk
+++ b/core/combo/arch/arm64/armv8-a.mk
@@ -1,13 +1 @@
 arch_variant_cflags :=
-
-# If the chip uses a53 cores, enable the errata workarounds
-ifneq ($(filter $(TARGET_CPU_VARIANT) $(TARGET_2ND_CPU_VARIANT),cortex-a53),)
-    TARGET_CPU_CORTEX_A53 ?= true
-endif
-
-# Leave the flag so devices that need the workaround but don't fit in
-# the check above can still enable it.
-ifeq ($(TARGET_CPU_CORTEX_A53),true)
-arch_variant_ldflags := -Wl,--fix-cortex-a53-843419 \
-                        -Wl,--fix-cortex-a53-835769
-endif
diff --git a/core/combo/mac_version.mk b/core/combo/mac_version.mk
index d18e4c5..b49feee 100644
--- a/core/combo/mac_version.mk
+++ b/core/combo/mac_version.mk
@@ -9,19 +9,17 @@ ifndef build_mac_version
 
 build_mac_version := $(shell sw_vers -productVersion)
 
-# Caution: Do not add 10.10 to this list until the prebuilts/darwin-x86 toolchains are updated.
-#          In the meantime, setting mac_sdk_version to 10.9 works on Yosemite (verified on 10.10.1).
-mac_sdk_versions_supported := 10.6 10.7 10.8 10.9
+mac_sdk_versions_supported :=  10.6 10.7 10.8
 ifneq ($(strip $(MAC_SDK_VERSION)),)
 mac_sdk_version := $(MAC_SDK_VERSION)
 ifeq ($(filter $(mac_sdk_version),$(mac_sdk_versions_supported)),)
 $(warning ****************************************************************)
-$(warning * MAC_SDK_VERSION $(MAC_SDK_VERSION) isn\'t one of the supported $(mac_sdk_versions_supported))
+$(warning * MAC_SDK_VERSION $(MAC_SDK_VERSION) isn't one of the supported $(mac_sdk_versions_supported))
 $(warning ****************************************************************)
 $(error Stop.)
 endif
 else
-mac_sdk_versions_installed := $(shell xcodebuild -showsdks 2> /dev/null | grep macosx | sort | sed -e "s/.*macosx//g")
+mac_sdk_versions_installed := $(shell xcodebuild -showsdks | grep macosx | sort | sed -e "s/.*macosx//g")
 mac_sdk_version := $(firstword $(filter $(mac_sdk_versions_installed), $(mac_sdk_versions_supported)))
 ifeq ($(mac_sdk_version),)
 mac_sdk_version := $(firstword $(mac_sdk_versions_supported))
@@ -29,18 +27,6 @@ endif
 endif
 
 mac_sdk_path := $(shell xcode-select -print-path)
-
-ifeq ($(strip "$(mac_sdk_path)"), "/Library/Developer/CommandLineTools")
-# Accept any modern version of Apple Command Line Tools
-mac_sdk_root := /
-
-# Override mac_sdk_version with build_mac_version (aka the version of the OSX host), but assume the latest
-# supported mac_sdk_version if the build_mac_version is not recognized.
-mac_sdk_version := $(shell echo $(build_mac_version) | cut -d '.' -f 1,2)
-ifeq ($(filter $(mac_sdk_version),$(mac_sdk_versions_supported)),)
-mac_sdk_version := $(lastword $(mac_sdk_versions_supported))
-endif
-else
 # try /Applications/Xcode*.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.?.sdk
 #  or /Volume/Xcode/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.?.sdk
 mac_sdk_root := $(mac_sdk_path)/Platforms/MacOSX.platform/Developer/SDKs/MacOSX$(mac_sdk_version).sdk
@@ -54,7 +40,6 @@ $(warning * Can not find SDK $(mac_sdk_version) at $(mac_sdk_root))
 $(warning *****************************************************)
 $(error Stop.)
 endif
-endif # $(mac_sdk_path)
 
 ifeq ($(mac_sdk_version),10.6)
   gcc_darwin_version := 10
diff --git a/core/combo/select.mk b/core/combo/select.mk
index ec10dd2..d66156c 100644
--- a/core/combo/select.mk
+++ b/core/combo/select.mk
@@ -104,13 +104,7 @@ ifneq ($(USE_CCACHE),)
     ifndef CXX_WRAPPER
       CXX_WRAPPER := $(ccache)
     endif
-    ifeq ($(ANDROID_CCACHE_DIR), $(CCACHE_DIR))
-      ifneq ($(ANDROID_CCACHE_SIZE),)
-        ACCSIZE_RESULT := $(shell $(ccache) -M$(ANDROID_CCACHE_SIZE))
-      endif
-    endif
     ccache =
-    ACCSIZE_RESULT =
   endif
 endif
 
diff --git a/core/config.mk b/core/config.mk
index ddc4007..f6d9ddc 100644
--- a/core/config.mk
+++ b/core/config.mk
@@ -369,11 +369,8 @@ endif
 
 # ---------------------------------------------------------------
 # Generic tools.
-ifeq ($(USE_HOST_LEX),yes)
-    LEX := flex
-else
-    LEX := prebuilts/misc/$(BUILD_OS)-$(HOST_PREBUILT_ARCH)/flex/flex-2.5.39
-endif
+
+LEX := prebuilts/misc/$(BUILD_OS)-$(HOST_PREBUILT_ARCH)/flex/flex-2.5.39
 # The default PKGDATADIR built in the prebuilt bison is a relative path
 # external/bison/data.
 # To run bison from elsewhere you need to set up enviromental variable
@@ -564,10 +561,6 @@ DEX2OAT_TARGET_CPU_VARIANT := $(TARGET_CPU_VARIANT)
 DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := default
 ifneq (,$(filter $(DEX2OAT_TARGET_CPU_VARIANT),cortex-a7 cortex-a15 krait denver generic cortex-a53))
   DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := div
-  ifneq (,$(filter $(DEX2OAT_TARGET_CPU_VARIANT),cortex-a53))
-    DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES += needfix_835769
-    DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := $(subst $(space),$(comma),$(strip $(DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES)))
-  endif
 endif
 
 ifdef TARGET_2ND_ARCH
@@ -576,10 +569,6 @@ $(TARGET_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_CPU_VARIANT := $(TARGET_2ND_CPU_VARI
 $(TARGET_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := default
 ifneq (,$(filter $($(TARGET_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_CPU_VARIANT),cortex-a7 cortex-a15 krait denver cortex-a53))
   $(TARGET_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := div
-  ifneq (,$(filter $($(TARGET_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_CPU_VARIANT),cortex-a53))
-    DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES += needfix_835769
-    DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES := $(subst $(space),$(comma),$(strip $(DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES)))
-  endif
 endif
 endif
 
@@ -650,13 +639,6 @@ ifneq ($(CM_BUILD),)
 ## We need to be sure the global selinux policies are included
 ## last, to avoid accidental resetting by device configs
 $(eval include vendor/cm/sepolicy/sepolicy.mk)
-
-# Include any vendor specific config.mk file
--include $(TOPDIR)vendor/*/build/core/config.mk
-
-# Include any vendor specific apicheck.mk file
--include $(TOPDIR)vendor/*/build/core/apicheck.mk
-
 endif
 
 include $(BUILD_SYSTEM)/dumpvar.mk
diff --git a/core/definitions.mk b/core/definitions.mk
index 9085b24..2c3a5c1 100644
--- a/core/definitions.mk
+++ b/core/definitions.mk
@@ -355,10 +355,6 @@ define find-other-java-files
 	$(call find-subdir-files,$(1) -name "*.java" -and -not -name ".*")
 endef
 
-define find-other-aidl-files
-	$(call find-subdir-files,$(1) -name "*.aidl" -and -not -name ".*")
-endef
-
 define find-other-html-files
 	$(call find-subdir-files,$(1) -name "*.html" -and -not -name ".*")
 endef
@@ -1194,16 +1190,8 @@ $(hide) ldir=$(PRIVATE_INTERMEDIATES_DIR)/WHOLE/$(basename $(notdir $(1)))_objs;
 
 endef
 
-# $(1): the full path of the source static library.
-define extract-and-include-whole-static-libs-first
-$(if $(strip $(1)),
-@echo "preparing StaticLib: $(PRIVATE_MODULE) [including $(strip $(1))]"
-$(hide) cp $(1) $@)
-endef
-
 define extract-and-include-target-whole-static-libs
-$(call extract-and-include-whole-static-libs-first, $(firstword $(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES)))
-$(foreach lib,$(wordlist 2,999,$(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES)), \
+$(foreach lib,$(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES), \
     $(call _extract-and-include-single-target-whole-static-lib, $(lib)))
 endef
 
@@ -1240,8 +1228,7 @@ $(hide) ldir=$(PRIVATE_INTERMEDIATES_DIR)/WHOLE/$(basename $(notdir $(1)))_objs;
 endef
 
 define extract-and-include-host-whole-static-libs
-$(call extract-and-include-whole-static-libs-first, $(firstword $(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES)))
-$(foreach lib,$(wordlist 2,999,$(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES)), \
+$(foreach lib,$(PRIVATE_ALL_WHOLE_STATIC_LIBRARIES), \
     $(call _extract-and-include-single-host-whole-static-lib, $(lib)))
 endef
 
@@ -2237,7 +2224,6 @@ include $(BUILD_SYSTEM)/distdir.mk
 
 # Include any vendor specific definitions.mk file
 -include $(TOPDIR)vendor/*/build/core/definitions.mk
--include $(TOPDIR)device/*/build/core/definitions.mk
 
 # broken:
 #	$(foreach file,$^,$(if $(findstring,.a,$(suffix $file)),-l$(file),$(file)))
diff --git a/core/dex_preopt_libart.mk b/core/dex_preopt_libart.mk
index 6f7d14d..5af2be2 100644
--- a/core/dex_preopt_libart.mk
+++ b/core/dex_preopt_libart.mk
@@ -16,13 +16,7 @@ DEX2OAT_DEPENDENCY += $(DEX2OAT)
 DEX2OATD_DEPENDENCY := $(DEX2OAT_DEPENDENCY)
 DEX2OATD_DEPENDENCY += $(DEX2OATD)
 
-# Use the first preloaded-classes file in PRODUCT_COPY_FILES.
-PRELOADED_CLASSES := $(call word-colon,1,$(firstword \
-    $(filter %system/etc/preloaded-classes,$(PRODUCT_COPY_FILES))))
-
-# Use the first compiled-classes file in PRODUCT_COPY_FILES.
-COMPILED_CLASSES := $(call word-colon,1,$(firstword \
-    $(filter %system/etc/compiled-classes,$(PRODUCT_COPY_FILES))))
+PRELOADED_CLASSES := frameworks/base/preloaded-classes
 
 # start of image reserved address space
 LIBART_IMG_HOST_BASE_ADDRESS   := 0x60000000
@@ -58,13 +52,6 @@ define get-odex-file-path
 $(dir $(2))$(1)/$(basename $(notdir $(2))).odex
 endef
 
-# Returns the path to the .odex.gz file
-# $(1): the arch name.
-# $(2): the full path (including file name) of the corresponding .jar or .apk.
-define get-odex-comp-path
-$(dir $(2))$(1)/$(basename $(notdir $(2))).odex.gz
-endef
-
 # Returns the path to the image file (such as "/system/framework/<arch>/boot.art"
 # $(1): the arch name (such as "arm")
 # $(2): the image location (such as "/system/framework/boot.art")
@@ -104,6 +91,5 @@ $(hide) $(DEX2OATD) \
 	--android-root=$(PRODUCT_OUT)/system \
 	--instruction-set=$($(PRIVATE_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_ARCH) \
 	--instruction-set-features=$($(PRIVATE_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES) \
-	--include-patch-information --runtime-arg -Xnorelocate --no-include-debug-symbols \
-	$(PRIVATE_DEX_PREOPT_FLAGS)
+	--include-patch-information --runtime-arg -Xnorelocate --no-include-debug-symbols
 endef
diff --git a/core/dex_preopt_libart_boot.mk b/core/dex_preopt_libart_boot.mk
index 6c6e0ad..fe4c5a4 100644
--- a/core/dex_preopt_libart_boot.mk
+++ b/core/dex_preopt_libart_boot.mk
@@ -32,17 +32,6 @@ ifneq ($(PRODUCT_DEX_PREOPT_IMAGE_IN_DATA),true)
 $(my_2nd_arch_prefix)DEFAULT_DEX_PREOPT_INSTALLED_IMAGE := $(PRODUCT_OUT)$($(my_2nd_arch_prefix)LIBART_BOOT_IMAGE_FILENAME)
 endif
 
-# Compile boot.oat as position-independent code if WITH_DEXPREOPT_PIC=true
-ifeq (true,$(WITH_DEXPREOPT_PIC))
-  PRODUCT_DEX_PREOPT_BOOT_FLAGS += --compile-pic
-endif
-
-# If we have a compiled-classes file, create a parameter.
-COMPILED_CLASSES_FLAGS :=
-ifneq ($(COMPILED_CLASSES),)
-  COMPILED_CLASSES_FLAGS := --compiled-classes=$(COMPILED_CLASSES)
-endif
-
 # The rule to install boot.art and boot.oat
 $($(my_2nd_arch_prefix)DEFAULT_DEX_PREOPT_INSTALLED_IMAGE) : $($(my_2nd_arch_prefix)DEFAULT_DEX_PREOPT_BUILT_IMAGE_FILENAME) | $(ACP)
 	$(call copy-file-to-target)
@@ -64,5 +53,4 @@ $($(my_2nd_arch_prefix)DEFAULT_DEX_PREOPT_BUILT_IMAGE_FILENAME) : $(LIBART_TARGE
 		--image=$@ --base=$(LIBART_IMG_TARGET_BASE_ADDRESS) \
 		--instruction-set=$($(PRIVATE_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_ARCH) \
 		--instruction-set-features=$($(PRIVATE_2ND_ARCH_VAR_PREFIX)DEX2OAT_TARGET_INSTRUCTION_SET_FEATURES) \
-		--android-root=$(PRODUCT_OUT)/system --include-patch-information --runtime-arg -Xnorelocate --no-include-debug-symbols \
-		$(PRODUCT_DEX_PREOPT_BOOT_FLAGS) $(COMPILED_CLASSES_FLAGS)
+		--android-root=$(PRODUCT_OUT)/system --include-patch-information --runtime-arg -Xnorelocate --no-include-debug-symbols
diff --git a/core/dex_preopt_odex_install.mk b/core/dex_preopt_odex_install.mk
index 6109553..741f9a3 100644
--- a/core/dex_preopt_odex_install.mk
+++ b/core/dex_preopt_odex_install.mk
@@ -11,12 +11,7 @@ else # WITH_DEXPREOPT=true
     ifndef LOCAL_DEX_PREOPT # LOCAL_DEX_PREOPT undefined
       ifneq ($(filter $(TARGET_OUT)/%,$(my_module_path)),) # Installed to system.img.
         ifeq (,$(LOCAL_APK_LIBRARIES)) # LOCAL_APK_LIBRARIES empty
-          # If we have product-specific config for this module?
-          ifeq (disable,$(DEXPREOPT.$(TARGET_PRODUCT).$(LOCAL_MODULE).CONFIG))
-            LOCAL_DEX_PREOPT := false
-          else
-            LOCAL_DEX_PREOPT := $(DEX_PREOPT_DEFAULT)
-          endif
+          LOCAL_DEX_PREOPT := $(DEX_PREOPT_DEFAULT)
         else # LOCAL_APK_LIBRARIES not empty
           LOCAL_DEX_PREOPT := nostripping
         endif # LOCAL_APK_LIBRARIES not empty
@@ -46,7 +41,6 @@ endif
 
 built_odex :=
 installed_odex :=
-compressed_odex :=
 built_installed_odex :=
 ifdef LOCAL_DEX_PREOPT
 dexpreopt_boot_jar_module := $(filter $(DEXPREOPT_BOOT_JARS_MODULES),$(LOCAL_MODULE))
@@ -100,45 +94,20 @@ endif  # libart
 endif  # boot jar
 
 ifdef built_odex
-ifndef LOCAL_DEX_PREOPT_FLAGS
-LOCAL_DEX_PREOPT_FLAGS := $(DEXPREOPT.$(TARGET_PRODUCT).$(LOCAL_MODULE).CONFIG)
-ifndef LOCAL_DEX_PREOPT_FLAGS
-LOCAL_DEX_PREOPT_FLAGS := $(PRODUCT_DEX_PREOPT_DEFAULT_FLAGS)
-endif
-endif
-
-# Compile apps with position-independent code if WITH_DEXPREOPT_PIC=true
-ifeq (true,$(WITH_DEXPREOPT_PIC))
-ifeq (false,$(WITH_DEXPREOPT_COMP))
-  LOCAL_DEX_PREOPT_FLAGS += --compile-pic
-endif
-endif
-
-$(built_odex): PRIVATE_DEX_PREOPT_FLAGS := $(LOCAL_DEX_PREOPT_FLAGS)
-
 # Use pattern rule - we may have multiple installed odex files.
 # Ugly syntax - See the definition get-odex-file-path.
 $(installed_odex) : $(dir $(LOCAL_INSTALLED_MODULE))%$(notdir $(word 1,$(installed_odex))) \
                   : $(dir $(LOCAL_BUILT_MODULE))%$(notdir $(word 1,$(built_odex))) \
     | $(ACP)
-	@echo -e ${CL_CYN}"Install: $@"${CL_RST}
+	@echo "Install: $@"
 	$(copy-file-to-target)
-
-# Ugly syntax - See the definition get-odex-comp-path.
-$(compressed_odex) : $(dir $(LOCAL_INSTALLED_MODULE))%$(notdir $(word 1,$(compressed_odex))) \
-                   : $(dir $(LOCAL_BUILT_MODULE))%$(notdir $(word 1,$(built_odex))) \
-    | $(MINIGZIP)
-	$(hide) mkdir -p $(dir $@)
-	$(MINIGZIP) -9 < $< > $@
 endif
 
-# Add the installed_odex and compressed_odex to the list of installed files for this module.
-ALL_MODULES.$(my_register_name).INSTALLED += $(compressed_odex)
+# Add the installed_odex to the list of installed files for this module.
 ALL_MODULES.$(my_register_name).INSTALLED += $(installed_odex)
 ALL_MODULES.$(my_register_name).BUILT_INSTALLED += $(built_installed_odex)
 
 # Make sure to install the .odex when you run "make <module_name>"
-$(my_register_name): $(compressed_odex)
 $(my_register_name): $(installed_odex)
 
 endif # LOCAL_DEX_PREOPT
diff --git a/core/dumpvar.mk b/core/dumpvar.mk
index 555da1d..47ba476 100644
--- a/core/dumpvar.mk
+++ b/core/dumpvar.mk
@@ -84,10 +84,8 @@ $(info   HOST_OS_EXTRA=$(HOST_OS_EXTRA))
 $(info   HOST_BUILD_TYPE=$(HOST_BUILD_TYPE))
 $(info   BUILD_ID=$(BUILD_ID))
 $(info   OUT_DIR=$(OUT_DIR))
-ifneq (,$(filter true, $(CYNGN_TARGET) $(EXTERNAL_CLEAN_TARGET)))
-ifeq ($(CYNGN_TARGET), true)
+ifeq ($(CYNGN_TARGET),true)
 $(info   CYNGN_TARGET=$(CYNGN_TARGET))
-endif
 $(info   CYNGN_FEATURES=$(CYNGN_FEATURES))
 endif
 $(info ============================================)
diff --git a/core/dynamic_binary.mk b/core/dynamic_binary.mk
index aefaea6..a77bf13 100644
--- a/core/dynamic_binary.mk
+++ b/core/dynamic_binary.mk
@@ -17,13 +17,16 @@ endif
 # know its results before base_rules.mk is included.
 include $(BUILD_SYSTEM)/configure_module_stem.mk
 
-intermediates := $(call local-intermediates-dir,,$(LOCAL_2ND_ARCH_VAR_PREFIX))
+# base_rules.make defines $(intermediates), but we need its value
+# before we include base_rules.  Make a guess, and verify that
+# it's correct once the real value is defined.
+guessed_intermediates := $(call local-intermediates-dir,,$(LOCAL_2ND_ARCH_VAR_PREFIX))
 
 # Define the target that is the unmodified output of the linker.
 # The basename of this target must be the same as the final output
 # binary name, because it's used to set the "soname" in the binary.
 # The includer of this file will define a rule to build this target.
-linked_module := $(intermediates)/LINKED/$(my_built_module_stem)
+linked_module := $(guessed_intermediates)/LINKED/$(my_built_module_stem)
 
 ALL_ORIGINAL_DYNAMIC_BINARIES += $(linked_module)
 
@@ -38,6 +41,11 @@ LOCAL_INTERMEDIATE_TARGETS := $(linked_module)
 include $(BUILD_SYSTEM)/binary.mk
 ###################################
 
+# Make sure that our guess at the value of intermediates was correct.
+ifneq ($(intermediates),$(guessed_intermediates))
+$(error Internal error: guessed path '$(guessed_intermediates)' doesn't match '$(intermediates))
+endif
+
 ###########################################################
 ## Compress
 ###########################################################
diff --git a/core/generate_extra_images.mk b/core/generate_extra_images.mk
index 319fb1f..8cd18fd 100644
--- a/core/generate_extra_images.mk
+++ b/core/generate_extra_images.mk
@@ -80,6 +80,39 @@ $(INSTALLED_PERSISTIMAGE_TARGET): $(MKEXTUSERIMG) $(MAKE_EXT4FS) $(INTERNAL_PERS
 ALL_DEFAULT_INSTALLED_MODULES += $(INSTALLED_PERSISTIMAGE_TARGET)
 ALL_MODULES.$(LOCAL_MODULE).INSTALLED += $(INSTALLED_PERSISTIMAGE_TARGET)
 
+
+#----------------------------------------------------------------------
+# Generate device tree image (dt.img)
+#----------------------------------------------------------------------
+ifeq ($(strip $(BOARD_KERNEL_SEPARATED_DT)),true)
+ifeq ($(strip $(BUILD_TINY_ANDROID)),true)
+include device/qcom/common/dtbtool/Android.mk
+endif
+
+ifeq ($(strip $(TARGET_CUSTOM_DTBTOOL)),)
+DTBTOOL_NAME := dtbToolCM
+else
+DTBTOOL_NAME := $(TARGET_CUSTOM_DTBTOOL)
+endif
+
+DTBTOOL := $(HOST_OUT_EXECUTABLES)/$(DTBTOOL_NAME)$(HOST_EXECUTABLE_SUFFIX)
+
+INSTALLED_DTIMAGE_TARGET := $(PRODUCT_OUT)/dt.img
+
+define build-dtimage-target
+    $(call pretty,"Target dt image: $(INSTALLED_DTIMAGE_TARGET)")
+    $(hide) $(DTBTOOL) -o $@ -s $(BOARD_KERNEL_PAGESIZE) -p $(KERNEL_OUT)/scripts/dtc/ $(KERNEL_OUT)/arch/arm/boot/
+    $(hide) chmod a+r $@
+endef
+
+$(INSTALLED_DTIMAGE_TARGET): $(DTBTOOL) $(INSTALLED_KERNEL_TARGET)
+	$(build-dtimage-target)
+
+ALL_DEFAULT_INSTALLED_MODULES += $(INSTALLED_DTIMAGE_TARGET)
+ALL_MODULES.$(LOCAL_MODULE).INSTALLED += $(INSTALLED_DTIMAGE_TARGET)
+endif
+
+
 #----------------------------------------------------------------------
 # Generate 1GB userdata image for 8930
 #----------------------------------------------------------------------
@@ -106,35 +139,6 @@ ALL_MODULES.$(LOCAL_MODULE).INSTALLED += $(INSTALLED_1G_USERDATAIMAGE_TARGET)
 endif
 
 
-#----------------------------------------------------------------------
-# Generate extra userdata images (for variants with multiple mmc sizes)
-#----------------------------------------------------------------------
-ifneq ($(BOARD_USERDATAEXTRAIMAGE_PARTITION_SIZE),)
-
-ifndef BOARD_USERDATAEXTRAIMAGE_PARTITION_NAME
-  BOARD_USERDATAEXTRAIMAGE_PARTITION_NAME := extra
-endif
-
-BUILT_USERDATAEXTRAIMAGE_TARGET := $(PRODUCT_OUT)/userdata_$(BOARD_USERDATAEXTRAIMAGE_PARTITION_NAME).img
-
-define build-userdataextraimage-target
-    $(call pretty,"Target EXTRA userdata fs image: $(INSTALLED_USERDATAEXTRAIMAGE_TARGET)")
-    @mkdir -p $(TARGET_OUT_DATA)
-    $(hide) $(MKEXTUSERIMG) -s $(TARGET_OUT_DATA) $@ ext4 data $(BOARD_USERDATAEXTRAIMAGE_PARTITION_SIZE)
-    $(hide) chmod a+r $@
-    $(hide) $(call assert-max-image-size,$@,$(BOARD_USERDATAEXTRAIMAGE_PARTITION_SIZE),yaffs)
-endef
-
-INSTALLED_USERDATAEXTRAIMAGE_TARGET := $(BUILT_USERDATAEXTRAIMAGE_TARGET)
-$(INSTALLED_USERDATAEXTRAIMAGE_TARGET): $(INSTALLED_USERDATAIMAGE_TARGET)
-	$(build-userdataextraimage-target)
-
-ALL_DEFAULT_INSTALLED_MODULES += $(INSTALLED_USERDATAEXTRAIMAGE_TARGET)
-ALL_MODULES.$(LOCAL_MODULE).INSTALLED += $(INSTALLED_USERDATAEXTRAIMAGE_TARGET)
-
-endif
-
-
 #----------------------------------------------------------------------
 # Generate NAND images
 #----------------------------------------------------------------------
diff --git a/core/install_jni_libs_internal.mk b/core/install_jni_libs_internal.mk
index c28bb15..944420b 100644
--- a/core/install_jni_libs_internal.mk
+++ b/core/install_jni_libs_internal.mk
@@ -100,15 +100,9 @@ endif
 my_prebuilt_jni_libs := $(addprefix $(LOCAL_PATH)/, \
     $(filter-out @%, $(my_prebuilt_jni_libs)))
 ifdef my_prebuilt_jni_libs
-ifdef my_embed_jni
-# Embed my_prebuilt_jni_libs to the apk
-my_jni_shared_libraries += $(my_prebuilt_jni_libs)
-else # not my_embed_jni
-# Install my_prebuilt_jni_libs as separate files.
 $(foreach lib, $(my_prebuilt_jni_libs), \
     $(eval $(call copy-one-file, $(lib), $(my_app_lib_path)/$(notdir $(lib)))))
 
 $(LOCAL_INSTALLED_MODULE) : | $(addprefix $(my_app_lib_path)/, $(notdir $(my_prebuilt_jni_libs)))
-endif  # my_embed_jni
 endif  # inner my_prebuilt_jni_libs
 endif  # outer my_prebuilt_jni_libs
diff --git a/core/java.mk b/core/java.mk
index 3f5a9fb..730c504 100644
--- a/core/java.mk
+++ b/core/java.mk
@@ -52,9 +52,6 @@ else
 endif
 endif
 
-# LOCAL_STATIC_JAVA_AAR_LIBRARIES are special LOCAL_STATIC_JAVA_LIBRARIES
-LOCAL_STATIC_JAVA_LIBRARIES := $(strip $(LOCAL_STATIC_JAVA_LIBRARIES) $(LOCAL_STATIC_JAVA_AAR_LIBRARIES))
-
 LOCAL_JAVA_LIBRARIES := $(sort $(LOCAL_JAVA_LIBRARIES))
 
 LOCAL_BUILT_MODULE_STEM := $(strip $(LOCAL_BUILT_MODULE_STEM))
diff --git a/core/main.mk b/core/main.mk
index ce910fb..dca7d60 100644
--- a/core/main.mk
+++ b/core/main.mk
@@ -101,9 +101,6 @@ include $(BUILD_SYSTEM)/cleanbuild.mk
 # Bring in Qualcomm helper macros
 include $(BUILD_SYSTEM)/qcom_utils.mk
 
-# Bring in Mediatek helper macros too
-include $(BUILD_SYSTEM)/mtk_utils.mk
-
 # Include the google-specific config
 -include vendor/google/build/config.mk
 
@@ -143,8 +140,8 @@ $(warning ************************************************************)
 $(error Directory names containing spaces not supported)
 endif
 
-java_version_str := $(shell unset _JAVA_OPTIONS JAVA_TOOL_OPTIONS && java -version 2>&1)
-javac_version_str := $(shell unset _JAVA_OPTIONS JAVA_TOOL_OPTIONS && javac -version 2>&1)
+java_version_str := $(shell unset _JAVA_OPTIONS && java -version 2>&1)
+javac_version_str := $(shell unset _JAVA_OPTIONS && javac -version 2>&1)
 
 # Check for the correct version of java, should be 1.7 by
 # default, and 1.6 if LEGACY_USE_JAVA6 is set.
@@ -491,12 +488,7 @@ endif
 ifneq ($(ONE_SHOT_MAKEFILE),)
 # We've probably been invoked by the "mm" shell function
 # with a subdirectory's makefile.
-
-# No Makefiles to include if we are performing a mms/short-circuit build. Only
-# the targets mentioned by main.mk and tasks/* are built (kernel, boot.img etc)
-ifneq ($(ONE_SHOT_MAKEFILE),__none__)
 include $(ONE_SHOT_MAKEFILE)
-endif
 # Change CUSTOM_MODULES to include only modules that were
 # defined by this makefile; this will install all of those
 # modules as a side-effect.  Do this after including ONE_SHOT_MAKEFILE
diff --git a/core/package_internal.mk b/core/package_internal.mk
index 101b442..bb458d4 100644
--- a/core/package_internal.mk
+++ b/core/package_internal.mk
@@ -123,8 +123,7 @@ endif
 
 all_res_assets := $(strip $(all_assets) $(all_resources))
 
-intermediates.COMMON := $(call local-intermediates-dir,COMMON)
-
+package_expected_intermediates_COMMON := $(call local-intermediates-dir,COMMON)
 # If no assets or resources were found, clear the directory variables so
 # we don't try to build them.
 ifneq (true,$(need_compile_asset))
@@ -137,7 +136,7 @@ else
 # Make sure that R_file_stamp inherits the proper PRIVATE vars.
 # If R.stamp moves, be sure to update the framework makefile,
 # which has intimate knowledge of its location.
-R_file_stamp := $(intermediates.COMMON)/src/R.stamp
+R_file_stamp := $(package_expected_intermediates_COMMON)/src/R.stamp
 LOCAL_INTERMEDIATE_TARGETS += $(R_file_stamp)
 endif
 
@@ -157,7 +156,7 @@ endif
 proguard_options_file :=
 ifneq ($(LOCAL_PROGUARD_ENABLED),custom)
 ifeq ($(need_compile_res),true)
-    proguard_options_file := $(intermediates.COMMON)/proguard_options
+    proguard_options_file := $(package_expected_intermediates_COMMON)/proguard_options
 endif # need_compile_res
 endif # !custom
 LOCAL_PROGUARD_FLAGS := $(addprefix -include ,$(proguard_options_file)) $(LOCAL_PROGUARD_FLAGS)
@@ -193,8 +192,6 @@ endif # LOCAL_EMMA_INSTRUMENT
 
 rs_compatibility_jni_libs :=
 
-include $(BUILD_SYSTEM)/android_manifest.mk
-
 #################################
 include $(BUILD_SYSTEM)/java.mk
 #################################
@@ -204,6 +201,8 @@ ifeq ($(LOCAL_SDK_RES_VERSION),)
   LOCAL_SDK_RES_VERSION:=$(LOCAL_SDK_VERSION)
 endif
 
+include $(BUILD_SYSTEM)/android_manifest.mk
+
 $(LOCAL_INTERMEDIATE_TARGETS): \
     PRIVATE_ANDROID_MANIFEST := $(full_android_manifest)
 ifneq (,$(filter-out current system_current, $(LOCAL_SDK_VERSION)))
@@ -224,6 +223,11 @@ ifeq ($(need_compile_res),true)
 # At the same time, this will copy the R.java file to a central
 # 'R' directory to make it easier to add the files to an IDE.
 #
+#TODO: use PRIVATE_SOURCE_INTERMEDIATES_DIR instead of
+#      $(intermediates.COMMON)/src
+ifneq ($(package_expected_intermediates_COMMON),$(intermediates.COMMON))
+  $(error $(LOCAL_MODULE): internal error: expected intermediates.COMMON "$(package_expected_intermediates_COMMON)" != intermediates.COMMON "$(intermediates.COMMON)")
+endif
 
 $(R_file_stamp): PRIVATE_RESOURCE_PUBLICS_OUTPUT := \
 			$(intermediates.COMMON)/public_resources.xml
@@ -297,24 +301,11 @@ framework_res_package_export_deps := $(framework_res_package_export)
 else # LOCAL_SDK_RES_VERSION
 framework_res_package_export := \
     $(call intermediates-dir-for,APPS,framework-res,,COMMON)/package-export.apk
-
-# Avoid possible circular dependency with our platform-res
-ifneq ($(LOCAL_IGNORE_SUBDIR), true)
-cm_plat_res_package_export := \
-    $(call intermediates-dir-for,APPS,org.cyanogenmod.platform-res,,COMMON)/package-export.apk
-endif # LOCAL_IGNORE_SUBDIR
-
 # We can't depend directly on the export.apk file; it won't get its
 # PRIVATE_ vars set up correctly if we do.  Instead, depend on the
 # corresponding R.stamp file, which lists the export.apk as a dependency.
 framework_res_package_export_deps := \
     $(dir $(framework_res_package_export))src/R.stamp
-
-ifneq ($(LOCAL_IGNORE_SUBDIR), true)
-cm_plat_res_package_export_deps := \
-    $(dir $(cm_plat_res_package_export))src/R.stamp
-endif # LOCAL_IGNORE_SUBDIR
-
 endif # LOCAL_SDK_RES_VERSION
 all_library_res_package_exports := \
     $(framework_res_package_export) \
@@ -326,13 +317,6 @@ all_library_res_package_export_deps := \
     $(foreach lib,$(LOCAL_RES_LIBRARIES),\
         $(call intermediates-dir-for,APPS,$(lib),,COMMON)/src/R.stamp)
 
-ifneq ($(LOCAL_IGNORE_SUBDIR), true)
-all_library_res_package_exports += \
-    $(cm_plat_res_package_export)
-all_library_res_package_export_deps += \
-    $(cm_plat_res_package_export_deps)
-endif # LOCAL_IGNORE_SUBDIR
-
 $(resource_export_package) $(R_file_stamp) $(LOCAL_BUILT_MODULE): $(all_library_res_package_export_deps)
 $(LOCAL_INTERMEDIATE_TARGETS): \
     PRIVATE_AAPT_INCLUDES := $(all_library_res_package_exports)
@@ -422,16 +406,6 @@ endif
 	@# Alignment must happen after all other zip operations.
 	$(align-package)
 
-###############################
-## Build dpi-specific apks, if it's apps_only build.
-ifdef TARGET_BUILD_APPS
-ifdef LOCAL_DPI_VARIANTS
-$(foreach d, $(LOCAL_DPI_VARIANTS), \
-  $(eval my_dpi := $(d)) \
-  $(eval include $(BUILD_SYSTEM)/dpi_specific_apk.mk))
-endif
-endif
-
 ###############################
 ## Rule to build the odex file
 ifdef LOCAL_DEX_PREOPT
diff --git a/core/pathmap.mk b/core/pathmap.mk
index 84d2b9d..80b0322 100644
--- a/core/pathmap.mk
+++ b/core/pathmap.mk
@@ -116,6 +116,7 @@ FRAMEWORKS_BASE_SUBDIRS := \
 	    sax \
 	    telecomm \
 	    telephony \
+	    phone \
 	    wifi \
 	    keystore \
 	    rs \
diff --git a/core/prebuilt.mk b/core/prebuilt.mk
index ba0e757..33f5dc6 100644
--- a/core/prebuilt.mk
+++ b/core/prebuilt.mk
@@ -17,7 +17,6 @@ include $(BUILD_SYSTEM)/multilib.mk
 my_skip_non_preferred_arch :=
 
 # check if first arch is supported
-LOCAL_2ND_ARCH_VAR_PREFIX :=
 include $(BUILD_SYSTEM)/module_arch_supported.mk
 ifeq ($(my_module_arch_supported),true)
 # first arch is supported
diff --git a/core/prebuilt_internal.mk b/core/prebuilt_internal.mk
index b9adb62..284884c 100644
--- a/core/prebuilt_internal.mk
+++ b/core/prebuilt_internal.mk
@@ -114,19 +114,6 @@ endif  # LOCAL_STRIP_MODULE not true
 ifeq ($(LOCAL_MODULE_CLASS),APPS)
 PACKAGES.$(LOCAL_MODULE).OVERRIDES := $(strip $(LOCAL_OVERRIDES_PACKAGES))
 
-# Select dpi-specific source
-ifdef LOCAL_DPI_VARIANTS
-my_dpi := $(firstword $(filter $(LOCAL_DPI_VARIANTS),$(PRODUCT_AAPT_PREF_CONFIG) $(PRODUCT_AAPT_PREBUILT_DPI)))
-ifdef my_dpi
-ifdef LOCAL_DPI_FILE_STEM
-my_prebuilt_dpi_file_stem := $(LOCAL_DPI_FILE_STEM)
-else
-my_prebuilt_dpi_file_stem := $(LOCAL_MODULE)_%.apk
-endif
-my_prebuilt_src_file := $(dir $(my_prebuilt_src_file))$(subst %,$(my_dpi),$(my_prebuilt_dpi_file_stem))
-endif  # my_dpi
-endif  # LOCAL_DPI_VARIANTS
-
 rs_compatibility_jni_libs :=
 include $(BUILD_SYSTEM)/install_jni_libs.mk
 
@@ -225,7 +212,7 @@ $(built_apk_splits) : $(built_module_path)/%.apk : $(my_src_dir)/%.apk | $(ACP)
 
 # Rules to install the split apks.
 $(installed_apk_splits) : $(my_module_path)/%.apk : $(built_module_path)/%.apk | $(ACP)
-	@echo -e ${CL_CYN}"Install: $@"${CL_RST}
+	@echo "Install: $@"
 	$(copy-file-to-new-target)
 
 # Register the additional built and installed files.
@@ -260,26 +247,10 @@ ifeq ($(LOCAL_IS_HOST_MODULE)$(LOCAL_MODULE_CLASS),JAVA_LIBRARIES)
 # while the deps should be in the common dir, so we make a copy in the common dir.
 # For nonstatic library, $(common_javalib_jar) is the dependency file,
 # while $(common_classes_jar) is used to link.
-common_classes_jar := $(intermediates.COMMON)/classes.jar
-common_javalib_jar := $(intermediates.COMMON)/javalib.jar
-
-$(common_classes_jar) $(common_javalib_jar): PRIVATE_MODULE := $(LOCAL_MODULE)
-
-ifneq ($(filter %.aar, $(my_prebuilt_src_file)),)
-# This is .aar file, archive of classes.jar and Android resources.
-my_src_jar := $(intermediates.COMMON)/aar/classes.jar
+common_classes_jar := $(call intermediates-dir-for,JAVA_LIBRARIES,$(LOCAL_MODULE),,COMMON)/classes.jar
+common_javalib_jar := $(dir $(common_classes_jar))javalib.jar
 
-$(my_src_jar) : $(my_prebuilt_src_file)
-	$(hide) rm -rf $(dir $@) && mkdir -p $(dir $@)
-	$(hide) unzip -qo -d $(dir $@) $<
-	# Make sure the extracted classes.jar has a new timestamp.
-	$(hide) touch $@
-
-else
-# This is jar file.
-my_src_jar := $(my_prebuilt_src_file)
-endif
-$(common_classes_jar) : $(my_src_jar) | $(ACP)
+$(common_classes_jar) : $(my_prebuilt_src_file) | $(ACP)
 	$(transform-prebuilt-to-target)
 
 $(common_javalib_jar) : $(common_classes_jar) | $(ACP)
diff --git a/core/product.mk b/core/product.mk
index feeba56..f235a42 100644
--- a/core/product.mk
+++ b/core/product.mk
@@ -72,7 +72,6 @@ _product_var_list := \
     PRODUCT_LOCALES \
     PRODUCT_AAPT_CONFIG \
     PRODUCT_AAPT_PREF_CONFIG \
-    PRODUCT_AAPT_PREBUILT_DPI \
     PRODUCT_PACKAGES \
     PRODUCT_PACKAGES_DEBUG \
     PRODUCT_PACKAGES_ENG \
@@ -104,18 +103,14 @@ _product_var_list := \
     PRODUCT_FACTORY_BUNDLE_MODULES \
     PRODUCT_RUNTIMES \
     PRODUCT_BOOT_JARS \
+    PRODUCT_DEX_PREOPT_IMAGE_IN_DATA \
     PRODUCT_SUPPORTS_VERITY \
     PRODUCT_OEM_PROPERTIES \
     PRODUCT_SYSTEM_PROPERTY_BLACKLIST \
     PRODUCT_SYSTEM_SERVER_JARS \
     PRODUCT_VERITY_SIGNING_KEY \
     PRODUCT_SYSTEM_VERITY_PARTITION \
-    PRODUCT_VENDOR_VERITY_PARTITION \
-    PRODUCT_DEX_PREOPT_IMAGE_IN_DATA \
-    PRODUCT_DEX_PREOPT_MODULE_CONFIGS \
-    PRODUCT_DEX_PREOPT_DEFAULT_FLAGS \
-    PRODUCT_DEX_PREOPT_BOOT_FLAGS \
-
+    PRODUCT_VENDOR_VERITY_PARTITION
 
 define dump-product
 $(info ==== $(1) ====)\
@@ -128,66 +123,24 @@ define dump-products
 $(foreach p,$(PRODUCTS),$(call dump-product,$(p)))
 endef
 
-#
-# Internal function. Appends inherited product variables to an existing one.
-#
-# $(1): Product variable to operate on
-# $(2): Value to append
-#
-define inherit-product_append-var
-  $(eval $(1) := $($(1)) $(INHERIT_TAG)$(strip $(2)))
-endef
-
-#
-# Internal function. Prepends inherited product variables to an existing one.
-#
-# $(1): Product variable to operate on
-# $(2): Value to prepend
-#
-define inherit-product_prepend-var
-  $(eval $(1) := $(INHERIT_TAG)$(strip $(2)) $($(1)))
-endef
-
-#
-# Internal function. Tracks visited notes during inheritance resolution.
-#
-# $(1): Product being inherited
-#
-define inherit-product_track-node
-  $(eval inherit_var := \
-      PRODUCTS.$(strip $(word 1,$(_include_stack))).INHERITS_FROM) \
-  $(eval $(inherit_var) := $(sort $($(inherit_var)) $(strip $(1)))) \
-  $(eval inherit_var:=) \
-  $(eval ALL_PRODUCTS := $(sort $(ALL_PRODUCTS) $(word 1,$(_include_stack))))
-endef
-
 #
 # $(1): product to inherit
 #
 # Does three things:
-#  1. Inherits all of the variables from $1, prioritizing existing settings.
+#  1. Inherits all of the variables from $1.
 #  2. Records the inheritance in the .INHERITS_FROM variable
 #  3. Records that we've visited this node, in ALL_PRODUCTS
 #
 define inherit-product
   $(foreach v,$(_product_var_list), \
-      $(call inherit-product_append-var,$(v),$(1))) \
-  $(call inherit-product_track-node,$(1))
+      $(eval $(v) := $($(v)) $(INHERIT_TAG)$(strip $(1)))) \
+  $(eval inherit_var := \
+      PRODUCTS.$(strip $(word 1,$(_include_stack))).INHERITS_FROM) \
+  $(eval $(inherit_var) := $(sort $($(inherit_var)) $(strip $(1)))) \
+  $(eval inherit_var:=) \
+  $(eval ALL_PRODUCTS := $(sort $(ALL_PRODUCTS) $(word 1,$(_include_stack))))
 endef
 
-#
-# $(1): product to inherit
-#
-# Does three things:
-#  1. Inherits all of the variables from $1, prioritizing inherited settings.
-#  2. Records the inheritance in the .INHERITS_FROM variable
-#  3. Records that we've visited this node, in ALL_PRODUCTS
-#
-define prepend-product
-  $(foreach v,$(_product_var_list), \
-      $(call inherit-product_prepend-var,$(v),$(1))) \
-  $(call inherit-product_track-node,$(1))
-endef
 
 #
 # Do inherit-product only if $(1) exists
@@ -196,13 +149,6 @@ define inherit-product-if-exists
   $(if $(wildcard $(1)),$(call inherit-product,$(1)),)
 endef
 
-#
-# Do inherit-product-prepend only if $(1) exists
-#
-define prepend-product-if-exists
-  $(if $(wildcard $(1)),$(call prepend-product,$(1)),)
-endef
-
 #
 # $(1): product makefile list
 #
@@ -356,14 +302,3 @@ endef
 define add-to-product-copy-files-if-exists
 $(if $(wildcard $(word 1,$(subst :, ,$(1)))),$(1))
 endef
-
-# whitespace placeholder when we record module's dex-preopt config.
-_PDPMC_SP_PLACE_HOLDER := |@SP@|
-# Set up dex-preopt config for a module.
-# $(1) list of module names
-# $(2) the modules' dex-preopt config
-define add-product-dex-preopt-module-config
-$(eval _c := $(subst $(space),$(_PDPMC_SP_PLACE_HOLDER),$(strip $(2))))\
-$(eval PRODUCT_DEX_PREOPT_MODULE_CONFIGS += \
-  $(foreach m,$(1),$(m)=$(_c)))
-endef
diff --git a/core/product_config.mk b/core/product_config.mk
index 640bb40..5c8a06c 100644
--- a/core/product_config.mk
+++ b/core/product_config.mk
@@ -181,7 +181,7 @@ include $(BUILD_SYSTEM)/device.mk
 
 # A CM build needs only the CM product makefiles.
 ifneq ($(CM_BUILD),)
-  all_product_configs := $(shell find device -path "*/$(CM_BUILD)/cm.mk")
+  all_product_configs := $(shell ls device/*/$(CM_BUILD)/cm.mk)
 else
   ifneq ($(strip $(TARGET_BUILD_APPS)),)
   # An unbundled app build needs only the core product makefiles.
@@ -318,9 +318,19 @@ ifneq (,$(extra_locales))
 endif
 
 # Add PRODUCT_LOCALES to PRODUCT_AAPT_CONFIG
-PRODUCT_AAPT_CONFIG := $(strip $(PRODUCT_LOCALES) $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_AAPT_CONFIG))
+PRODUCT_AAPT_CONFIG := $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_AAPT_CONFIG))
+PRODUCT_AAPT_CONFIG := $(PRODUCT_LOCALES) $(PRODUCT_AAPT_CONFIG)
 PRODUCT_AAPT_PREF_CONFIG := $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_AAPT_PREF_CONFIG))
-PRODUCT_AAPT_PREBUILT_DPI := $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_AAPT_PREBUILT_DPI))
+
+# Default to medium-density assets.
+# (Can be overridden in the device config, e.g.: PRODUCT_AAPT_CONFIG += hdpi)
+PRODUCT_AAPT_CONFIG := $(strip \
+    $(PRODUCT_AAPT_CONFIG) \
+    $(if $(filter %dpi,$(PRODUCT_AAPT_CONFIG)),,mdpi))
+PRODUCT_AAPT_PREF_CONFIG := $(strip $(PRODUCT_AAPT_PREF_CONFIG))
+
+# Everyone gets nodpi and anydpi assets which are density-independent.
+PRODUCT_AAPT_CONFIG += nodpi anydpi
 
 # Keep a copy of the space-separated config
 PRODUCT_AAPT_CONFIG_SP := $(PRODUCT_AAPT_CONFIG)
@@ -328,6 +338,8 @@ PRODUCT_AAPT_CONFIG_SP := $(PRODUCT_AAPT_CONFIG)
 # Convert spaces to commas.
 PRODUCT_AAPT_CONFIG := \
     $(subst $(space),$(comma),$(strip $(PRODUCT_AAPT_CONFIG)))
+PRODUCT_AAPT_PREF_CONFIG := \
+    $(subst $(space),$(comma),$(strip $(PRODUCT_AAPT_PREF_CONFIG)))
 
 # product-scoped aapt flags
 PRODUCT_AAPT_FLAGS :=
@@ -443,21 +455,3 @@ PRODUCT_EXTRA_RECOVERY_KEYS := $(sort \
 # If there is no room in /system for the image, place it in /data
 PRODUCT_DEX_PREOPT_IMAGE_IN_DATA := \
     $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_DEX_PREOPT_IMAGE_IN_DATA))
-
-PRODUCT_DEX_PREOPT_DEFAULT_FLAGS := \
-    $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_DEX_PREOPT_DEFAULT_FLAGS))
-PRODUCT_DEX_PREOPT_BOOT_FLAGS := \
-    $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_DEX_PREOPT_BOOT_FLAGS))
-# Resolve and setup per-module dex-preopot configs.
-PRODUCT_DEX_PREOPT_MODULE_CONFIGS := \
-    $(strip $(PRODUCTS.$(INTERNAL_PRODUCT).PRODUCT_DEX_PREOPT_MODULE_CONFIGS))
-# If a module has multiple setups, the first takes precedence.
-_pdpmc_modules :=
-$(foreach c,$(PRODUCT_DEX_PREOPT_MODULE_CONFIGS),\
-  $(eval m := $(firstword $(subst =,$(space),$(c))))\
-  $(if $(filter $(_pdpmc_modules),$(m)),,\
-    $(eval _pdpmc_modules += $(m))\
-    $(eval cf := $(patsubst $(m)=%,%,$(c)))\
-    $(eval cf := $(subst $(_PDPMC_SP_PLACE_HOLDER),$(space),$(cf)))\
-    $(eval DEXPREOPT.$(TARGET_PRODUCT).$(m).CONFIG := $(cf))))
-_pdpmc_modules :=
diff --git a/core/qcom_target.mk b/core/qcom_target.mk
index 83c1490..825ddbb 100644
--- a/core/qcom_target.mk
+++ b/core/qcom_target.mk
@@ -1,16 +1,13 @@
 # Target-specific configuration
 
 # Populate the qcom hardware variants in the project pathmap.
+define qcom-set-path-variant
+$(call project-set-path-variant,qcom-$(2),TARGET_QCOM_$(1)_VARIANT,hardware/qcom/$(2))
+endef
 define ril-set-path-variant
 $(call project-set-path-variant,ril,TARGET_RIL_VARIANT,hardware/$(1))
 endef
 
-# Set device-specific HALs into project pathmap
-define set-device-specific-path
-$(call project-set-path,qcom-$(2),$(strip $(if $(USE_DEVICE_SPECIFIC_$(1)), \
-    $(TARGET_DEVICE_DIR)/$(2), $(3))))
-endef
-
 ifeq ($(BOARD_USES_QCOM_HARDWARE),true)
 
     qcom_flags := -DQCOM_HARDWARE
@@ -28,11 +25,6 @@ ifeq ($(BOARD_USES_QCOM_HARDWARE),true)
         qcom_flags += -DQCOM_BSP_LEGACY
     endif
 
-    # Enable extra offloading for post-805 targets
-    ifneq ($(filter msm8992 msm8994,$(TARGET_BOARD_PLATFORM)),)
-        qcom_flags += -DHAS_EXTRA_FLAC_METADATA
-    endif
-
     TARGET_GLOBAL_CFLAGS += $(qcom_flags)
     TARGET_GLOBAL_CPPFLAGS += $(qcom_flags)
     CLANG_TARGET_GLOBAL_CFLAGS += $(qcom_flags)
@@ -44,37 +36,23 @@ ifeq ($(BOARD_USES_QCOM_HARDWARE),true)
     2ND_CLANG_TARGET_GLOBAL_CFLAGS += $(qcom_flags)
     2ND_CLANG_TARGET_GLOBAL_CPPFLAGS += $(qcom_flags)
 
-    ifeq ($(QCOM_HARDWARE_VARIANT),)
-        ifneq ($(filter msm8610 msm8226 msm8974,$(TARGET_BOARD_PLATFORM)),)
-            QCOM_HARDWARE_VARIANT := msm8974
-        else
-        ifneq ($(filter msm8909 msm8916,$(TARGET_BOARD_PLATFORM)),)
-            QCOM_HARDWARE_VARIANT := msm8916
-        else
-        ifneq ($(filter msm8992 msm8994,$(TARGET_BOARD_PLATFORM)),)
-            QCOM_HARDWARE_VARIANT := msm8994
-        else
-            QCOM_HARDWARE_VARIANT := $(TARGET_BOARD_PLATFORM)
-        endif
-        endif
-        endif
-    endif
-
-$(call project-set-path,qcom-audio,hardware/qcom/audio-caf/$(QCOM_HARDWARE_VARIANT))
-$(call set-device-specific-path,CAMERA,camera,hardware/qcom/camera)
-$(call project-set-path,qcom-display,hardware/qcom/display-caf/$(QCOM_HARDWARE_VARIANT))
-$(call set-device-specific-path,GPS,gps,hardware/qcom/gps)
-$(call project-set-path,qcom-media,hardware/qcom/media-caf/$(QCOM_HARDWARE_VARIANT))
-$(call set-device-specific-path,SENSORS,sensors,hardware/qcom/sensors)
+$(call project-set-path,qcom-audio,hardware/qcom/audio-caf/$(TARGET_BOARD_PLATFORM))
+ifeq ($(USE_DEVICE_SPECIFIC_CAMERA),true)
+$(call project-set-path,qcom-camera,$(TARGET_DEVICE_DIR)/camera)
+else
+$(call qcom-set-path-variant,CAMERA,camera)
+endif
+$(call project-set-path,qcom-display,hardware/qcom/display-caf/$(TARGET_BOARD_PLATFORM))
+$(call qcom-set-path-variant,GPS,gps)
+$(call project-set-path,qcom-media,hardware/qcom/media-caf/$(TARGET_BOARD_PLATFORM))
+$(call qcom-set-path-variant,SENSORS,sensors)
 $(call ril-set-path-variant,ril)
-$(call set-device-specific-path,LOC_API,loc-api,vendor/qcom/opensource/location)
 else
 $(call project-set-path,qcom-audio,hardware/qcom/audio/default)
-$(call project-set-path,qcom-camera,hardware/qcom/camera)
+$(call qcom-set-path-variant,CAMERA,camera)
 $(call project-set-path,qcom-display,hardware/qcom/display/$(TARGET_BOARD_PLATFORM))
-$(call project-set-path,qcom-gps,hardware/qcom/gps)
+$(call qcom-set-path-variant,GPS,gps)
 $(call project-set-path,qcom-media,hardware/qcom/media/default)
-$(call project-set-path,qcom-sensors,hardware/qcom/sensors)
+$(call qcom-set-path-variant,SENSORS,sensors)
 $(call ril-set-path-variant,ril)
-$(call project-set-path,loc-api,vendor/qcom/opensource/location)
 endif
diff --git a/core/qcom_utils.mk b/core/qcom_utils.mk
index bb1202c..1177ad4 100644
--- a/core/qcom_utils.mk
+++ b/core/qcom_utils.mk
@@ -4,13 +4,10 @@ QCOM_BOARD_PLATFORMS += msm7x30
 QCOM_BOARD_PLATFORMS += msm8226
 QCOM_BOARD_PLATFORMS += msm8610
 QCOM_BOARD_PLATFORMS += msm8660
-QCOM_BOARD_PLATFORMS += msm8909
 QCOM_BOARD_PLATFORMS += msm8916
 QCOM_BOARD_PLATFORMS += msm8960
 QCOM_BOARD_PLATFORMS += msm8974
 QCOM_BOARD_PLATFORMS += mpq8092
-QCOM_BOARD_PLATFORMS += msm8992
-QCOM_BOARD_PLATFORMS += msm8994
 QCOM_BOARD_PLATFORMS += msm_bronze
 QCOM_BOARD_PLATFORMS += apq8084
 
diff --git a/core/setup_one_odex.mk b/core/setup_one_odex.mk
index 7d87e0e..ec8a28a 100644
--- a/core/setup_one_odex.mk
+++ b/core/setup_one_odex.mk
@@ -18,16 +18,7 @@
 # Input variables: my_2nd_arch_prefix
 # Output(modified) variables: built_odex, installed_odex, built_installed_odex
 
-my_local_odex_comp :=
-
-ifneq (false,$(WITH_DEXPREOPT_COMP))
-ifneq ($(filter %.apk,$(LOCAL_INSTALLED_MODULE)),)
-my_local_odex_comp := true
-endif
-endif
-
 my_built_odex := $(call get-odex-file-path,$($(my_2nd_arch_prefix)DEX2OAT_TARGET_ARCH),$(LOCAL_BUILT_MODULE))
-
 ifdef LOCAL_DEX_PREOPT_IMAGE_LOCATION
 my_dex_preopt_image_location := $(LOCAL_DEX_PREOPT_IMAGE_LOCATION)
 else
@@ -41,16 +32,8 @@ $(my_built_odex) : $($(my_2nd_arch_prefix)DEXPREOPT_ONE_FILE_DEPENDENCY_BUILT_BO
     $(DEXPREOPT_ONE_FILE_DEPENDENCY_TOOLS) \
     $(my_dex_preopt_image_filename)
 
-ifeq (true,$(my_local_odex_comp))
-my_installed_odex := $(call get-odex-comp-path,$($(my_2nd_arch_prefix)DEX2OAT_TARGET_ARCH),$(LOCAL_INSTALLED_MODULE))
-else
 my_installed_odex := $(call get-odex-file-path,$($(my_2nd_arch_prefix)DEX2OAT_TARGET_ARCH),$(LOCAL_INSTALLED_MODULE))
-endif
 
 built_odex += $(my_built_odex)
-ifeq (true,$(my_local_odex_comp))
-compressed_odex += $(my_installed_odex)
-else
 installed_odex += $(my_installed_odex)
-endif
 built_installed_odex += $(my_built_odex):$(my_installed_odex)
diff --git a/core/tasks/apicheck.mk b/core/tasks/apicheck.mk
index f109527..356faa8 100644
--- a/core/tasks/apicheck.mk
+++ b/core/tasks/apicheck.mk
@@ -31,11 +31,6 @@ last_released_sdk_version := $(lastword $(call numerically_sort, \
              )\
         ))
 
-.PHONY: check-public-api
-checkapi : check-public-api
-
-.PHONY: update-api
-
 # INTERNAL_PLATFORM_API_FILE is the one build by droiddoc.
 # Note that since INTERNAL_PLATFORM_API_FILE is the byproduct of api-stubs module,
 # (See frameworks/base/Android.mk)
@@ -44,7 +39,7 @@ checkapi : check-public-api
 # Check that the API we're building hasn't broken the last-released
 # SDK version.
 $(eval $(call check-api, \
-    checkpublicapi-last, \
+    checkapi-last, \
     $(SRC_API_DIR)/$(last_released_sdk_version).txt, \
     $(INTERNAL_PLATFORM_API_FILE), \
     frameworks/base/api/removed.txt, \
@@ -53,14 +48,14 @@ $(eval $(call check-api, \
     -error 7 -error 8 -error 9 -error 10 -error 11 -error 12 -error 13 -error 14 -error 15 \
     -error 16 -error 17 -error 18 , \
     cat $(BUILD_SYSTEM)/apicheck_msg_last.txt, \
-    check-public-api, \
+    checkapi, \
     $(call doc-timestamp-for,api-stubs) \
     ))
 
 # Check that the API we're building hasn't changed from the not-yet-released
 # SDK version.
 $(eval $(call check-api, \
-    checkpublicapi-current, \
+    checkapi-current, \
     frameworks/base/api/current.txt, \
     $(INTERNAL_PLATFORM_API_FILE), \
     frameworks/base/api/removed.txt, \
@@ -69,23 +64,21 @@ $(eval $(call check-api, \
     -error 7 -error 8 -error 9 -error 10 -error 11 -error 12 -error 13 -error 14 -error 15 \
     -error 16 -error 17 -error 18 -error 19 -error 20 -error 21 -error 23 -error 24 \
     -error 25 -error 26 -error 27, \
-    cat $(BUILD_SYSTEM)/apicheck_msg_current.txt, \
-    check-public-api, \
+    sed -e 's/%UPDATE_API%/update-api/g' $(BUILD_SYSTEM)/apicheck_msg_current.txt, \
+    checkapi, \
     $(call doc-timestamp-for,api-stubs) \
     ))
 
-.PHONY: update-public-api
-update-public-api: $(INTERNAL_PLATFORM_API_FILE) | $(ACP)
+.PHONY: update-api
+update-api: $(INTERNAL_PLATFORM_API_FILE) | $(ACP)
 	@echo -e ${CL_GRN}"Copying current.txt"${CL_RST}
 	$(hide) $(ACP) $(INTERNAL_PLATFORM_API_FILE) frameworks/base/api/current.txt
 	@echo -e ${CL_GRN}"Copying removed.txt"${CL_RST}
 	$(hide) $(ACP) $(INTERNAL_PLATFORM_REMOVED_API_FILE) frameworks/base/api/removed.txt
 
-update-api : update-public-api
 
 #####################Check System API#####################
-.PHONY: check-system-api
-checkapi : check-system-api
+.PHONY: checksystemapi
 
 # Check that the System API we're building hasn't broken the last-released
 # SDK version.
@@ -99,7 +92,7 @@ $(eval $(call check-api, \
     -error 7 -error 8 -error 9 -error 10 -error 11 -error 12 -error 13 -error 14 -error 15 \
     -error 16 -error 17 -error 18 , \
     cat $(BUILD_SYSTEM)/apicheck_msg_last.txt, \
-    check-system-api, \
+    checksystemapi, \
     $(call doc-timestamp-for,system-api-stubs) \
     ))
 
@@ -115,14 +108,12 @@ $(eval $(call check-api, \
     -error 7 -error 8 -error 9 -error 10 -error 11 -error 12 -error 13 -error 14 -error 15 \
     -error 16 -error 17 -error 18 -error 19 -error 20 -error 21 -error 23 -error 24 \
     -error 25 -error 26 -error 27, \
-    cat $(BUILD_SYSTEM)/apicheck_msg_current.txt, \
-    check-system-api, \
+    sed -e 's/%UPDATE_API%/update-system-api/g' $(BUILD_SYSTEM)/apicheck_msg_current.txt, \
+    checksystemapi, \
     $(call doc-timestamp-for,system-api-stubs) \
     ))
 
 .PHONY: update-system-api
-update-api : update-system-api
-
 update-system-api: $(INTERNAL_PLATFORM_SYSTEM_API_FILE) | $(ACP)
 	@echo Copying system-current.txt
 	$(hide) $(ACP) $(INTERNAL_PLATFORM_SYSTEM_API_FILE) frameworks/base/api/system-current.txt
diff --git a/core/tasks/factory_bundle.mk b/core/tasks/factory_bundle.mk
index 51531cb..054a52a 100644
--- a/core/tasks/factory_bundle.mk
+++ b/core/tasks/factory_bundle.mk
@@ -46,7 +46,7 @@ copied_files := \
     $(eval _fb_m_name := $(word 1,$(_fb_m_tuple))) \
     $(eval _fb_dests := $(wordlist 2,999,$(_fb_m_tuple))) \
     $(eval _fb_m_built := $(filter $(HOST_OUT)/%, $(ALL_MODULES.$(_fb_m_name).BUILT))) \
-    $(if $(_fb_m_built),,$(warning no built file in requested_modules for '$(_fb_m_built)'))\
+    $(if $(_fb_m_built),,$(error no built file in requested_modules for '$(_fb_m_built)'))\
     $(foreach _fb_f,$(_fb_dests),$(eval $(call copy-one-file,$(_fb_m_built),$(root_dir)/$(_fb_f))))\
     $(addprefix $(root_dir)/,$(_fb_dests)) \
     )) \
@@ -82,3 +82,4 @@ endif
 
 endif # TARGET_BUILD_PDK
 endif # ONE_SHOT_MAKEFILE
+
diff --git a/core/tasks/factory_ramdisk.mk b/core/tasks/factory_ramdisk.mk
index d65d931..00fcdde 100644
--- a/core/tasks/factory_ramdisk.mk
+++ b/core/tasks/factory_ramdisk.mk
@@ -44,7 +44,7 @@ $(if $(filter 1,$(words $(_iofrm_src))), \
     $(eval _fulldest := $(TARGET_FACTORY_RAMDISK_OUT)/$(1)) \
     $(eval $(call copy-one-file,$(_iofrm_src),$(_fulldest))) \
     $(eval INTERNAL_FACTORY_RAMDISK_EXTRA_MODULES_FILES += $(_fulldest)), \
-    $(warning Warning: Cannot find built file in "$(2)" for "$(1)") \
+    $(error Error: Cannot find match in "$(2)" for "$(1)") \
     )
 endef
 
diff --git a/core/tasks/kernel.mk b/core/tasks/kernel.mk
index 4283dbb..e6be63a 100644
--- a/core/tasks/kernel.mk
+++ b/core/tasks/kernel.mk
@@ -28,71 +28,13 @@ SELINUX_DEFCONFIG := $(TARGET_KERNEL_SELINUX_CONFIG)
 ## Internal variables
 KERNEL_OUT := $(TARGET_OUT_INTERMEDIATES)/KERNEL_OBJ
 KERNEL_CONFIG := $(KERNEL_OUT)/.config
-KERNEL_OUT_STAMP := $(KERNEL_OUT)/.mkdir_stamp
-
-TARGET_KERNEL_ARCH := $(strip $(TARGET_KERNEL_ARCH))
-ifeq ($(TARGET_KERNEL_ARCH),)
-KERNEL_ARCH := $(TARGET_ARCH)
-else
-KERNEL_ARCH := $(TARGET_KERNEL_ARCH)
-endif
-
-ifeq ($(KERNEL_ARCH),x86_64)
-KERNEL_DEFCONFIG_ARCH := x86
-else
-KERNEL_DEFCONFIG_ARCH := $(KERNEL_ARCH)
-endif
-KERNEL_DEFCONFIG_SRC := $(KERNEL_SRC)/arch/$(KERNEL_DEFCONFIG_ARCH)/configs/$(KERNEL_DEFCONFIG)
-
-TARGET_KERNEL_HEADER_ARCH := $(strip $(TARGET_KERNEL_HEADER_ARCH))
-ifeq ($(TARGET_KERNEL_HEADER_ARCH),)
-KERNEL_HEADER_ARCH := $(KERNEL_ARCH)
-else
-KERNEL_HEADER_ARCH := $(TARGET_KERNEL_HEADER_ARCH)
-endif
-
-KERNEL_HEADER_DEFCONFIG := $(strip $(KERNEL_HEADER_DEFCONFIG))
-ifeq ($(KERNEL_HEADER_DEFCONFIG),)
-KERNEL_HEADER_DEFCONFIG := $(KERNEL_DEFCONFIG)
-endif
-
 
 ifneq ($(BOARD_KERNEL_IMAGE_NAME),)
-  TARGET_PREBUILT_INT_KERNEL_TYPE := $(BOARD_KERNEL_IMAGE_NAME)
+	TARGET_PREBUILT_INT_KERNEL_TYPE := $(BOARD_KERNEL_IMAGE_NAME)
+	TARGET_PREBUILT_INT_KERNEL := $(KERNEL_OUT)/arch/$(TARGET_ARCH)/boot/$(TARGET_PREBUILT_INT_KERNEL_TYPE)
 else
-  ifeq ($(TARGET_USES_UNCOMPRESSED_KERNEL),true)
-    TARGET_PREBUILT_INT_KERNEL_TYPE := Image
-  else
-    ifeq ($(KERNEL_ARCH),arm64)
-      TARGET_PREBUILT_INT_KERNEL_TYPE := Image.gz
-    else
-      TARGET_PREBUILT_INT_KERNEL_TYPE := zImage
-    endif
-  endif
-endif
-
-TARGET_PREBUILT_INT_KERNEL := $(KERNEL_OUT)/arch/$(KERNEL_ARCH)/boot/$(TARGET_PREBUILT_INT_KERNEL_TYPE)
-
-# Clear this first to prevent accidental poisoning from env
-MAKE_FLAGS :=
-
-ifeq ($(KERNEL_ARCH),arm64)
-  # Avoid "unsupported RELA relocation: 311" errors (R_AARCH64_ADR_GOT_PAGE)
-  MAKE_FLAGS += CFLAGS_MODULE="-fno-pic"
-  ifeq ($(TARGET_ARCH),arm)
-    KERNEL_CONFIG_OVERRIDE := CONFIG_ANDROID_BINDER_IPC_32BIT=y
-  endif
-endif
-
-ifneq ($(TARGET_KERNEL_ADDITIONAL_CONFIG),)
-KERNEL_ADDITIONAL_CONFIG := $(TARGET_KERNEL_ADDITIONAL_CONFIG)
-KERNEL_ADDITIONAL_CONFIG_SRC := $(KERNEL_SRC)/arch/$(KERNEL_ARCH)/configs/$(KERNEL_ADDITIONAL_CONFIG)
-    ifeq ("$(wildcard $(KERNEL_ADDITIONAL_CONFIG_SRC))","")
-        $(warning TARGET_KERNEL_ADDITIONAL_CONFIG '$(TARGET_KERNEL_ADDITIONAL_CONFIG)' doesn't exist)
-        KERNEL_ADDITIONAL_CONFIG_SRC := /dev/null
-    endif
-else
-    KERNEL_ADDITIONAL_CONFIG_SRC := /dev/null
+	TARGET_PREBUILT_INT_KERNEL := $(KERNEL_OUT)/arch/$(TARGET_ARCH)/boot/zImage
+	TARGET_PREBUILT_INT_KERNEL_TYPE := zImage
 endif
 
 ## Do be discontinued in a future version. Notify builder about target
@@ -155,62 +97,34 @@ else
     else
         #$(info Kernel source found, building it)
         FULL_KERNEL_BUILD := true
-        KERNEL_BIN := $(TARGET_PREBUILT_INT_KERNEL)
+        ifeq ($(TARGET_USES_UNCOMPRESSED_KERNEL),true)
+        $(info Using uncompressed kernel)
+            KERNEL_BIN := $(KERNEL_OUT)/piggy
+        else
+            KERNEL_BIN := $(TARGET_PREBUILT_INT_KERNEL)
+        endif
     endif
 endif
 
-ifeq ($(BOARD_HAS_MTK_HARDWARE),true)
-  ifeq ($(BOARD_USES_MTK_KERNELBUILD),true)
-    include $(CLEAR_VARS)
-    $(shell rm -f $(TARGET_PREBUILT_INT_KERNEL))
-    FULL_KERNEL_BUILD := false
-    PROJECT_NAME := $(TARGET_KERNEL_CONFIG)
-$(TARGET_PREBUILT_INT_KERNEL):
-	cd $(TARGET_KERNEL_SOURCE) && env -i PATH=$(PATH) ./makeMtk -t -o=OUT_DIR=$(OUT_DIR),TARGET_BUILD_VARIANT=$(TARGET_BUILD_VARIANT) $(PROJECT_NAME) r k
-	-cd $(TARGET_KERNEL_SOURCE) && git clean -fd
-
-  endif
-endif
-
 ifeq ($(FULL_KERNEL_BUILD),true)
 
 KERNEL_HEADERS_INSTALL := $(KERNEL_OUT)/usr
-KERNEL_HEADERS_INSTALL_STAMP := $(KERNEL_OUT)/.headers_install_stamp
 KERNEL_MODULES_INSTALL := system
 KERNEL_MODULES_OUT := $(TARGET_OUT)/lib/modules
 
-TARGET_KERNEL_CROSS_COMPILE_PREFIX := $(strip $(TARGET_KERNEL_CROSS_COMPILE_PREFIX))
-ifeq ($(TARGET_KERNEL_CROSS_COMPILE_PREFIX),)
-ifeq ($(KERNEL_TOOLCHAIN_PREFIX),)
-KERNEL_TOOLCHAIN_PREFIX := arm-eabi-
-endif
-else
-KERNEL_TOOLCHAIN_PREFIX := $(TARGET_KERNEL_CROSS_COMPILE_PREFIX)
-endif
-
 ifeq ($(KERNEL_TOOLCHAIN),)
-KERNEL_TOOLCHAIN_PATH := $(KERNEL_TOOLCHAIN_PREFIX)
-else
-ifneq ($(KERNEL_TOOLCHAIN_PREFIX),)
-KERNEL_TOOLCHAIN_PATH := $(KERNEL_TOOLCHAIN)/$(KERNEL_TOOLCHAIN_PREFIX)
-endif
+KERNEL_TOOLCHAIN := $(ARM_EABI_TOOLCHAIN)
 endif
-
-ifneq ($(USE_CCACHE),)
-    ccache := $(ANDROID_BUILD_TOP)/prebuilts/misc/$(HOST_PREBUILT_TAG)/ccache/ccache
-    # Check that the executable is here.
-    ccache := $(strip $(wildcard $(ccache)))
+ifeq ($(KERNEL_TOOLCHAIN_PREFIX),)
+KERNEL_TOOLCHAIN_PREFIX := arm-eabi-
 endif
 
-KERNEL_CROSS_COMPILE := CROSS_COMPILE="$(ccache) $(KERNEL_TOOLCHAIN_PATH)"
-ccache =
-
 define mv-modules
     mdpath=`find $(KERNEL_MODULES_OUT) -type f -name modules.order`;\
     if [ "$$mdpath" != "" ];then\
         mpath=`dirname $$mdpath`;\
         ko=`find $$mpath/kernel -type f -name *.ko`;\
-        for i in $$ko; do $(KERNEL_TOOLCHAIN_PATH)strip --strip-unneeded $$i;\
+        for i in $$ko; do $(KERNEL_TOOLCHAIN)/$(KERNEL_TOOLCHAIN_PREFIX)strip --strip-unneeded $$i;\
         mv $$i $(KERNEL_MODULES_OUT)/; done;\
     fi
 endef
@@ -222,57 +136,41 @@ define clean-module-folder
     fi
 endef
 
+ifeq ($(TARGET_ARCH),arm)
+    ifneq ($(USE_CCACHE),)
+      ccache := $(ANDROID_BUILD_TOP)/prebuilts/misc/$(HOST_PREBUILT_TAG)/ccache/ccache
+      # Check that the executable is here.
+      ccache := $(strip $(wildcard $(ccache)))
+    endif
+    ARM_CROSS_COMPILE:=CROSS_COMPILE="$(ccache) $(KERNEL_TOOLCHAIN)/$(KERNEL_TOOLCHAIN_PREFIX)"
+    ccache = 
+endif
+
 ifeq ($(HOST_OS),darwin)
-  MAKE_FLAGS += C_INCLUDE_PATH=$(ANDROID_BUILD_TOP)/external/elfutils/0.153/libelf/
+  MAKE_FLAGS := C_INCLUDE_PATH=$(ANDROID_BUILD_TOP)/external/elfutils/0.153/libelf/
 endif
 
 ifeq ($(TARGET_KERNEL_MODULES),)
     TARGET_KERNEL_MODULES := no-external-modules
 endif
 
-$(KERNEL_OUT_STAMP):
-	$(hide) mkdir -p $(KERNEL_OUT)
-	$(hide) mkdir -p $(KERNEL_MODULES_OUT)
-	$(hide) touch $@
-
-KERNEL_ADDITIONAL_CONFIG_OUT := $(KERNEL_OUT)/.additional_config
-
-.PHONY: force_additional_config
-$(KERNEL_ADDITIONAL_CONFIG_OUT): force_additional_config
-	$(hide) cmp -s $(KERNEL_ADDITIONAL_CONFIG_SRC) $@ || cp $(KERNEL_ADDITIONAL_CONFIG_SRC) $@;
+$(KERNEL_OUT):
+	mkdir -p $(KERNEL_OUT)
+	mkdir -p $(KERNEL_MODULES_OUT)
 
-$(KERNEL_CONFIG): $(KERNEL_OUT_STAMP) $(KERNEL_DEFCONFIG_SRC) $(KERNEL_ADDITIONAL_CONFIG_OUT)
-	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) VARIANT_DEFCONFIG=$(VARIANT_DEFCONFIG) SELINUX_DEFCONFIG=$(SELINUX_DEFCONFIG) $(KERNEL_DEFCONFIG)
-	$(hide) if [ ! -z "$(KERNEL_CONFIG_OVERRIDE)" ]; then \
-			echo "Overriding kernel config with '$(KERNEL_CONFIG_OVERRIDE)'"; \
-			echo $(KERNEL_CONFIG_OVERRIDE) >> $(KERNEL_OUT)/.config; \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) oldconfig; fi
-	$(hide) if [ ! -z "$(KERNEL_ADDITIONAL_CONFIG)" ]; then \
-			echo "Using additional config '$(KERNEL_ADDITIONAL_CONFIG)'"; \
-			$(KERNEL_SRC)/scripts/kconfig/merge_config.sh -m -O $(KERNEL_OUT) $(KERNEL_OUT)/.config $(KERNEL_SRC)/arch/$(KERNEL_ARCH)/configs/$(KERNEL_ADDITIONAL_CONFIG); \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) KCONFIG_ALLCONFIG=$(KERNEL_OUT)/.config alldefconfig; fi
+$(KERNEL_CONFIG): $(KERNEL_OUT)
+	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) VARIANT_DEFCONFIG=$(VARIANT_DEFCONFIG) SELINUX_DEFCONFIG=$(SELINUX_DEFCONFIG) $(KERNEL_DEFCONFIG)
 
-TARGET_KERNEL_BINARIES: $(KERNEL_OUT_STAMP) $(KERNEL_CONFIG) $(KERNEL_HEADERS_INSTALL_STAMP)
-	@echo -e ${CL_GRN}"Building Kernel"${CL_RST}
-	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) $(TARGET_PREBUILT_INT_KERNEL_TYPE)
-	$(hide) if grep -q 'CONFIG_OF=y' $(KERNEL_CONFIG) ; \
-			then \
-				echo -e ${CL_GRN}"Building DTBs"${CL_RST} ; \
-				$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) dtbs ; \
-			else \
-				echo "DTBs not enabled" ; \
-			fi ;
-	$(hide) if grep -q 'CONFIG_MODULES=y' $(KERNEL_CONFIG) ; \
-			then \
-				echo -e ${CL_GRN}"Building Kernel Modules"${CL_RST} ; \
-				$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) modules && \
-				$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) INSTALL_MOD_PATH=../../$(KERNEL_MODULES_INSTALL) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) modules_install && \
-				$(mv-modules) && \
-				$(clean-module-folder) ; \
-			else \
-				echo "Kernel Modules not enabled" ; \
-			fi ;
+$(KERNEL_OUT)/piggy : $(TARGET_PREBUILT_INT_KERNEL)
+	$(hide) gunzip -c $(KERNEL_OUT)/arch/$(TARGET_ARCH)/boot/compressed/piggy.gzip > $(KERNEL_OUT)/piggy
 
+TARGET_KERNEL_BINARIES: $(KERNEL_OUT) $(KERNEL_CONFIG) $(KERNEL_HEADERS_INSTALL)
+	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) $(TARGET_PREBUILT_INT_KERNEL_TYPE)
+	-$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) dtbs
+	-$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) modules
+	-$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) INSTALL_MOD_PATH=../../$(KERNEL_MODULES_INSTALL) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) modules_install
+	$(mv-modules)
+	$(clean-module-folder)
 
 $(TARGET_KERNEL_MODULES): TARGET_KERNEL_BINARIES
 
@@ -280,47 +178,8 @@ $(TARGET_PREBUILT_INT_KERNEL): $(TARGET_KERNEL_MODULES)
 	$(mv-modules)
 	$(clean-module-folder)
 
-$(KERNEL_HEADERS_INSTALL_STAMP): $(KERNEL_OUT_STAMP) $(KERNEL_CONFIG)
-	$(hide) if [ ! -z "$(KERNEL_HEADER_DEFCONFIG)" ]; then \
-			rm -f ../$(KERNEL_CONFIG); \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_HEADER_ARCH) $(KERNEL_CROSS_COMPILE) VARIANT_DEFCONFIG=$(VARIANT_DEFCONFIG) SELINUX_DEFCONFIG=$(SELINUX_DEFCONFIG) $(KERNEL_HEADER_DEFCONFIG); \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_HEADER_ARCH) $(KERNEL_CROSS_COMPILE) headers_install; fi
-	$(hide) if [ "$(KERNEL_HEADER_DEFCONFIG)" != "$(KERNEL_DEFCONFIG)" ]; then \
-			echo "Used a different defconfig for header generation"; \
-			rm -f ../$(KERNEL_CONFIG); \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) VARIANT_DEFCONFIG=$(VARIANT_DEFCONFIG) SELINUX_DEFCONFIG=$(SELINUX_DEFCONFIG) $(KERNEL_DEFCONFIG); fi
-	$(hide) if [ ! -z "$(KERNEL_CONFIG_OVERRIDE)" ]; then \
-			echo "Overriding kernel config with '$(KERNEL_CONFIG_OVERRIDE)'"; \
-			echo $(KERNEL_CONFIG_OVERRIDE) >> $(KERNEL_OUT)/.config; \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) oldconfig; fi
-	$(hide) if [ ! -z "$(KERNEL_ADDITIONAL_CONFIG)" ]; then \
-			echo "Using additional config '$(KERNEL_ADDITIONAL_CONFIG)'"; \
-			$(KERNEL_SRC)/scripts/kconfig/merge_config.sh -m -O $(KERNEL_OUT) $(KERNEL_OUT)/.config $(KERNEL_SRC)/arch/$(KERNEL_ARCH)/configs/$(KERNEL_ADDITIONAL_CONFIG); \
-			$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) KCONFIG_ALLCONFIG=$(KERNEL_OUT)/.config alldefconfig; fi
-	$(hide) touch $@
-
-# provide this rule because there are dependencies on this throughout the repo
-$(KERNEL_HEADERS_INSTALL): $(KERNEL_HEADERS_INSTALL_STAMP)
-
-kerneltags: $(KERNEL_OUT_STAMP) $(KERNEL_CONFIG)
-	$(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) tags
-
-kernelconfig:  KERNELCONFIG_MODE := menuconfig
-kernelxconfig: KERNELCONFIG_MODE := xconfig
-kernelxconfig kernelconfig: $(KERNEL_OUT_STAMP)
-	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) VARIANT_DEFCONFIG=$(VARIANT_DEFCONFIG) SELINUX_DEFCONFIG=$(SELINUX_DEFCONFIG) $(KERNEL_DEFCONFIG)
-	$(hide) if [ ! -z "$(KERNEL_CONFIG_OVERRIDE)" ]; then \
-			echo "Overriding kernel config with '$(KERNEL_CONFIG_OVERRIDE)'"; \
-			echo $(KERNEL_CONFIG_OVERRIDE) >> $(KERNEL_OUT)/.config; fi
-	env KCONFIG_NOTIMESTAMP=true \
-		 $(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) $(KERNELCONFIG_MODE)
-	env KCONFIG_NOTIMESTAMP=true \
-		 $(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) savedefconfig
-	cp $(KERNEL_OUT)/defconfig $(KERNEL_DEFCONFIG_SRC)
-
-alldefconfig: $(KERNEL_OUT_STAMP)
-	env KCONFIG_NOTIMESTAMP=true \
-		 $(MAKE) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(KERNEL_ARCH) $(KERNEL_CROSS_COMPILE) alldefconfig
+$(KERNEL_HEADERS_INSTALL): $(KERNEL_OUT) $(KERNEL_CONFIG)
+	$(MAKE) $(MAKE_FLAGS) -C $(KERNEL_SRC) O=$(KERNEL_OUT) ARCH=$(TARGET_ARCH) $(ARM_CROSS_COMPILE) headers_install
 
 endif # FULL_KERNEL_BUILD
 
diff --git a/core/tasks/oem_image.mk b/core/tasks/oem_image.mk
index c68a341..26b9aba 100644
--- a/core/tasks/oem_image.mk
+++ b/core/tasks/oem_image.mk
@@ -15,16 +15,7 @@
 #
 
 # We build oem.img only if it's asked for.
-skip_oem_image := true
 ifneq ($(filter $(MAKECMDGOALS),oem_image),)
-    skip_oem_image := false
-endif
-
-ifneq ($(BOARD_OEMIMAGE_FILE_SYSTEM_TYPE),)
-    skip_oem_image := false
-endif
-
-ifneq ($(skip_oem_image),true)
 ifndef BOARD_OEMIMAGE_PARTITION_SIZE
 $(error BOARD_OEMIMAGE_PARTITION_SIZE is not set.)
 endif
@@ -52,4 +43,4 @@ $(INSTALLED_OEMIMAGE_TARGET) : $(INTERNAL_USERIMAGES_DEPS) $(INTERNAL_OEMIMAGE_F
 oem_image : $(INSTALLED_OEMIMAGE_TARGET)
 $(call dist-for-goals, oem_image, $(INSTALLED_OEMIMAGE_TARGET))
 
-endif
+endif  # oem_image in $(MAKECMDGOALS)
diff --git a/envsetup.sh b/envsetup.sh
index d37d3c7..ae38022 100644
--- a/envsetup.sh
+++ b/envsetup.sh
@@ -13,8 +13,6 @@ Invoke ". build/envsetup.sh" from your shell to add the following functions to y
 - mmp:     Builds all of the modules in the current directory and pushes them to the device.
 - mmmp:    Builds all of the modules in the supplied directories and pushes them to the device.
 - mmma:    Builds all of the modules in the supplied directories, and their dependencies.
-- mms:     Short circuit builder. Quickly re-build the kernel, rootfs, boot and system images
-           without deep dependencies. Requires the full build to have run before.
 - cgrep:   Greps on all local C/C++ files.
 - ggrep:   Greps on all local Gradle files.
 - jgrep:   Greps on all local Java files.
@@ -29,7 +27,6 @@ Invoke ". build/envsetup.sh" from your shell to add the following functions to y
 - mka:      Builds using SCHED_BATCH on all processors
 - mkap:     Builds the module(s) using mka and pushes them to the device.
 - cmka:     Cleans and builds using mka.
-- repolastsync: Prints date and time of last repo sync.
 - reposync: Parallel repo sync using ionice and SCHED_BATCH
 - repopick: Utility to fetch changes from Gerrit.
 - installboot: Installs a boot.img to the connected device.
@@ -38,9 +35,12 @@ Invoke ". build/envsetup.sh" from your shell to add the following functions to y
 Look at the source to view more functions. The complete list is:
 EOF
     T=$(gettop)
+    local A
+    A=""
     for i in `cat $T/build/envsetup.sh | sed -n "/^[ \t]*function /s/function \([a-z_]*\).*/\1/p" | sort | uniq`; do
-      echo "$i"
-    done | column
+      A="$A $i"
+    done
+    echo $A
 }
 
 # Get the value of a build variable as an absolute path.
@@ -230,10 +230,6 @@ function setpaths()
     unset ANDROID_HOST_OUT
     export ANDROID_HOST_OUT=$(get_abs_build_var HOST_OUT)
 
-    if [ -n "$ANDROID_CCACHE_DIR" ]; then
-        export CCACHE_DIR=$ANDROID_CCACHE_DIR
-    fi
-
     # needed for building linux on MacOS
     # TODO: fix the path
     #export HOST_EXTRACFLAGS="-I "$T/system/kernel_headers/host_include
@@ -573,7 +569,6 @@ alias bib=breakfast
 function lunch()
 {
     local answer
-    LUNCH_MENU_CHOICES=($(for l in ${LUNCH_MENU_CHOICES[@]}; do echo "$l"; done | sort))
 
     if [ "$1" ] ; then
         answer=$1
@@ -676,8 +671,7 @@ function tapas()
 {
     local arch="$(echo $* | xargs -n 1 echo | \grep -E '^(arm|x86|mips|armv5|arm64|x86_64|mips64)$' | xargs)"
     local variant="$(echo $* | xargs -n 1 echo | \grep -E '^(user|userdebug|eng)$' | xargs)"
-    local density="$(echo $* | xargs -n 1 echo | \grep -E '^(ldpi|mdpi|tvdpi|hdpi|xhdpi|xxhdpi|xxxhdpi|alldpi)$' | xargs)"
-    local apps="$(echo $* | xargs -n 1 echo | \grep -E -v '^(user|userdebug|eng|arm|x86|mips|armv5|arm64|x86_64|mips64|ldpi|mdpi|tvdpi|hdpi|xhdpi|xxhdpi|xxxhdpi|alldpi)$' | xargs)"
+    local apps="$(echo $* | xargs -n 1 echo | \grep -E -v '^(user|userdebug|eng|arm|x86|mips|armv5|arm64|x86_64|mips64)$' | xargs)"
 
     if [ $(echo $arch | wc -w) -gt 1 ]; then
         echo "tapas: Error: Multiple build archs supplied: $arch"
@@ -687,10 +681,6 @@ function tapas()
         echo "tapas: Error: Multiple build variants supplied: $variant"
         return
     fi
-    if [ $(echo $density | wc -w) -gt 1 ]; then
-        echo "tapas: Error: Multiple densities supplied: $density"
-        return
-    fi
 
     local product=full
     case $arch in
@@ -707,13 +697,9 @@ function tapas()
     if [ -z "$apps" ]; then
         apps=all
     fi
-    if [ -z "$density" ]; then
-        density=alldpi
-    fi
 
     export TARGET_PRODUCT=$product
     export TARGET_BUILD_VARIANT=$variant
-    export TARGET_BUILD_DENSITY=$density
     export TARGET_BUILD_TYPE=release
     export TARGET_BUILD_APPS=$apps
 
@@ -740,7 +726,7 @@ function eat()
             done
             echo "Device Found.."
         fi
-    if (adb shell getprop ro.cm.device | grep -q "$CM_BUILD");
+    if (adb shell cat /system/build.prop | grep -q "ro.cm.device=$CM_BUILD");
     then
         # if adbd isn't root we can't write to /cache/recovery/
         adb root
@@ -912,12 +898,7 @@ function mmm()
                 case $DIR in
                   showcommands | snod | dist | incrementaljavac) ARGS="$ARGS $DIR";;
                   GET-INSTALL-PATH) GET_INSTALL_PATH=$DIR;;
-                  *) if [ -d $DIR ]; then
-                         echo "No Android.mk in $DIR.";
-                     else
-                         echo "Couldn't locate the directory $DIR";
-                     fi
-                     return 1;;
+                  *) echo "No Android.mk in $DIR."; return 1;;
                 esac
             fi
         done
@@ -1064,85 +1045,6 @@ function pid()
     fi
 }
 
-# coredump_setup - enable core dumps globally for any process
-#                  that has the core-file-size limit set correctly
-#
-# NOTE: You must call also coredump_enable for a specific process
-#       if its core-file-size limit is not set already.
-# NOTE: Core dumps are written to ramdisk; they will not survive a reboot!
-
-function coredump_setup()
-{
-	echo "Getting root...";
-	adb root;
-	adb wait-for-device;
-
-	echo "Remounting root parition read-write...";
-	adb shell mount -w -o remount -t rootfs rootfs;
-	sleep 1;
-	adb wait-for-device;
-	adb shell mkdir -p /cores;
-	adb shell mount -t tmpfs tmpfs /cores;
-	adb shell chmod 0777 /cores;
-
-	echo "Granting SELinux permission to dump in /cores...";
-	adb shell restorecon -R /cores;
-
-	echo "Set core pattern.";
-	adb shell 'echo /cores/core.%p > /proc/sys/kernel/core_pattern';
-
-	echo "Done."
-}
-
-# coredump_enable - enable core dumps for the specified process
-# $1 = PID of process (e.g., $(pid mediaserver))
-#
-# NOTE: coredump_setup must have been called as well for a core
-#       dump to actually be generated.
-
-function coredump_enable()
-{
-	local PID=$1;
-	if [ -z "$PID" ]; then
-		printf "Expecting a PID!\n";
-		return;
-	fi;
-	echo "Setting core limit for $PID to infinite...";
-	adb shell prlimit $PID 4 -1 -1
-}
-
-# core - send SIGV and pull the core for process
-# $1 = PID of process (e.g., $(pid mediaserver))
-#
-# NOTE: coredump_setup must be called once per boot for core dumps to be
-#       enabled globally.
-
-function core()
-{
-	local PID=$1;
-
-	if [ -z "$PID" ]; then
-		printf "Expecting a PID!\n";
-		return;
-	fi;
-
-	local CORENAME=core.$PID;
-	local COREPATH=/cores/$CORENAME;
-	local SIG=SEGV;
-
-	coredump_enable $1;
-
-	local done=0;
-	while [ $(adb shell "[ -d /proc/$PID ] && echo -n yes") ]; do
-		printf "\tSending SIG%s to %d...\n" $SIG $PID;
-		adb shell kill -$SIG $PID;
-		sleep 1;
-	done;
-
-	adb shell "while [ ! -f $COREPATH ] ; do echo waiting for $COREPATH to be generated; sleep 1; done"
-	echo "Done: core is under $COREPATH on device.";
-}
-
 # systemstack - dump the current stack trace of all threads in the system process
 # to the usual ANR traces file
 function systemstack()
@@ -1226,151 +1128,10 @@ function is64bit()
     fi
 }
 
-function adb_get_product_device() {
-  echo `adb shell getprop ro.product.device | sed s/.$//`
-}
-
-# returns 0 when process is not traced
-function adb_get_traced_by() {
-  echo `adb shell cat /proc/$1/status | grep -e "^TracerPid:" | sed "s/^TracerPid:\t//" | sed s/.$//`
-}
-
-function gdbclient() {
-  # TODO:
-  # 1. Check for ANDROID_SERIAL/multiple devices
-  local PROCESS_NAME="n/a"
-  local PID=$1
-  local PORT=5039
-  if [ -z "$PID" ]; then
-    echo "Usage: gdbclient <pid|processname> [port number]"
-    return -1
-  fi
-  local DEVICE=$(adb_get_product_device)
-
-  if [ -z "$DEVICE" ]; then
-    echo "Error: Unable to get device name. Please check if device is connected and ANDROID_SERIAL is set."
-    return -2
-  fi
-
-  if [ -n "$2" ]; then
-    PORT=$2
-  fi
-
-  local ROOT=$(gettop)
-  if [ -z "$ROOT" ]; then
-    # This is for the situation with downloaded symbols (from the build server)
-    # we check if they are available.
-    ROOT=`realpath .`
-  fi
-
-  local OUT_ROOT="$ANDROID_PRODUCT_OUT"
-  local SYMBOLS_DIR="$OUT_ROOT/symbols"
-
-  if [ ! -d $SYMBOLS_DIR ]; then
-    echo "Error: couldn't find symbols: $SYMBOLS_DIR does not exist or is not a directory."
-    return -3
-  fi
-
-  # let's figure out which executable we are about to debug
-
-  # check if user specified a name -> resolve to pid
-  if [[ ! "$PID" =~ ^[0-9]+$ ]] ; then
-    PROCESS_NAME=$PID
-    PID=$(pid --exact $PROCESS_NAME)
-    if [ -z "$PID" ]; then
-      echo "Error: couldn't resolve pid by process name: $PROCESS_NAME"
-      return -4
-    fi
-  fi
-
-  local EXE=`adb shell readlink /proc/$PID/exe | sed s/.$//`
-  # TODO: print error in case there is no such pid
-  local LOCAL_EXE_PATH=$SYMBOLS_DIR$EXE
-
-  if [ ! -f $LOCAL_EXE_PATH ]; then
-    echo "Error: unable to find symbols for executable $EXE: file $LOCAL_EXE_PATH does not exist"
-    return -5
-  fi
-
-  local USE64BIT=""
-
-  if [[ "$(file $LOCAL_EXE_PATH)" =~ 64-bit ]]; then
-    USE64BIT="64"
-  fi
-
-  local GDB=
-  local GDB64=
-  local CPU_ABI=`adb shell getprop ro.product.cpu.abilist | sed s/.$//`
-  # TODO: we assume these are available via $PATH
-  if [[ $CPU_ABI =~ (^|,)arm64 ]]; then
-    GDB=arm-linux-androideabi-gdb
-    GDB64=aarch64-linux-android-gdb
-  elif [[ $CPU_ABI =~ (^|,)arm ]]; then
-    GDB=arm-linux-androideabi-gdb
-  elif [[ $CPU_ABI =~ (^|,)x86_64 ]]; then
-    GDB=x86_64-linux-androideabi-gdb
-  elif [[ $CPU_ABI =~ (^|,)x86 ]]; then
-    GDB=x86_64-linux-androideabi-gdb
-  elif [[ $CPU_ABI =~ (^|,)mips64 ]]; then
-    GDB=mipsel-linux-android-gdb
-    GDB64=mips64el-linux-android-gdb
-  elif [[ $CPU_ABI =~ (^|,)mips ]]; then
-    GDB=mipsel-linux-android-gdb
-  else
-    echo "Error: unrecognized cpu.abilist: $CPU_ABI"
-    return -6
-  fi
-
-  # TODO: check if tracing process is gdbserver and not some random strace...
-  if [ $(adb_get_traced_by $PID) -eq 0 ]; then
-    # start gdbserver
-    echo "Starting gdbserver..."
-    # TODO: check if adb is already listening $PORT
-    # to avoid unnecessary calls
-    echo ". adb forward for port=$PORT..."
-    adb forward tcp:$PORT tcp:$PORT
-    echo ". starting gdbserver to attach to pid=$PID..."
-    adb shell gdbserver$USE64BIT :$PORT --attach $PID &
-    echo ". give it couple of seconds to start..."
-    sleep 2
-    echo ". done"
-  else
-    echo "It looks like gdbserver is already attached to $PID (process is traced), trying to connect to it using local port=$PORT"
-  fi
-
-  local OUT_SO_SYMBOLS=$SYMBOLS_DIR/system/lib$USE64BIT
-  local OUT_VENDOR_SO_SYMBOLS=$SYMBOLS_DIR/vendor/lib$USE64BIT
-  local ART_CMD=""
-
-  echo >|"$OUT_ROOT/gdbclient.cmds" "set solib-absolute-prefix $SYMBOLS_DIR"
-  echo >>"$OUT_ROOT/gdbclient.cmds" "set solib-search-path $OUT_SO_SYMBOLS:$OUT_SO_SYMBOLS/hw:$OUT_SO_SYMBOLS/ssl/engines:$OUT_SO_SYMBOLS/drm:$OUT_SO_SYMBOLS/egl:$OUT_SO_SYMBOLS/soundfx:$OUT_VENDOR_SO_SYMBOLS:$OUT_VENDOR_SO_SYMBOLS/hw:$OUT_VENDOR_SO_SYMBOLS/egl"
-  local DALVIK_GDB_SCRIPT=$ROOT/development/scripts/gdb/dalvik.gdb
-  if [ -f $DALVIK_GDB_SCRIPT ]; then
-    echo >>"$OUT_ROOT/gdbclient.cmds" "source $DALVIK_GDB_SCRIPT"
-    ART_CMD="art-on"
-  else
-    echo "Warning: couldn't find $DALVIK_GDB_SCRIPT - ART debugging options will not be available"
-  fi
-  echo >>"$OUT_ROOT/gdbclient.cmds" "target remote :$PORT"
-  if [[ $EXE =~ (^|/)(app_process|dalvikvm)(|32|64)$ ]]; then
-    echo >> "$OUT_ROOT/gdbclient.cmds" $ART_CMD
-  fi
-
-  echo >>"$OUT_ROOT/gdbclient.cmds" ""
-
-  local WHICH_GDB=$GDB
-
-  if [ -n "$USE64BIT" -a -n "$GDB64" ]; then
-    WHICH_GDB=$GDB64
-  fi
-
-  gdbwrapper $WHICH_GDB "$OUT_ROOT/gdbclient.cmds" "$LOCAL_EXE_PATH"
-}
-
 # gdbclient now determines whether the user wants to debug a 32-bit or 64-bit
 # executable, set up the approriate gdbserver, then invokes the proper host
 # gdb.
-function gdbclient_old()
+function gdbclient()
 {
    local OUT_ROOT=$(get_abs_build_var PRODUCT_OUT)
    local OUT_SYMBOLS=$(get_abs_build_var TARGET_OUT_UNSTRIPPED)
@@ -1984,7 +1745,7 @@ function installboot()
     sleep 1
     adb wait-for-online shell mount /system 2>&1 > /dev/null
     adb wait-for-online remount
-    if (adb shell getprop ro.cm.device | grep -q "$CM_BUILD");
+    if (adb shell cat /system/build.prop | grep -q "ro.cm.device=$CM_BUILD");
     then
         adb push $OUT/boot.img /cache/
         for i in $OUT/system/lib/modules/*;
@@ -2029,7 +1790,7 @@ function installrecovery()
     sleep 1
     adb wait-for-online shell mount /system 2>&1 >> /dev/null
     adb wait-for-online remount
-    if (adb shell getprop ro.cm.device | grep -q "$CM_BUILD");
+    if (adb shell cat /system/build.prop | grep -q "ro.cm.device=$CM_BUILD");
     then
         adb push $OUT/recovery.img /cache/
         adb shell dd if=/cache/recovery.img of=$PARTITION
@@ -2336,20 +2097,14 @@ function cmrebase() {
 }
 
 function mka() {
-    local T=$(gettop)
-    if [ "$T" ]; then
-        case `uname -s` in
-            Darwin)
-                make -C $T -j `sysctl hw.ncpu|cut -d" " -f2` "$@"
-                ;;
-            *)
-                mk_timer schedtool -B -n 1 -e ionice -n 1 make -C $T -j$(cat /proc/cpuinfo | grep "^processor" | wc -l) "$@"
-                ;;
-        esac
-
-    else
-        echo "Couldn't locate the top of the tree.  Try setting TOP."
-    fi
+    case `uname -s` in
+        Darwin)
+            make -j `sysctl hw.ncpu|cut -d" " -f2` "$@"
+            ;;
+        *)
+            schedtool -B -n 1 -e ionice -n 1 make -j$(cat /proc/cpuinfo | grep "^processor" | wc -l) "$@"
+            ;;
+    esac
 }
 
 function cmka() {
@@ -2372,37 +2127,6 @@ function cmka() {
     fi
 }
 
-function mms() {
-    local T=$(gettop)
-    if [ -z "$T" ]
-    then
-        echo "Couldn't locate the top of the tree.  Try setting TOP."
-        return 1
-    fi
-
-    case `uname -s` in
-        Darwin)
-            local NUM_CPUS=$(sysctl hw.ncpu|cut -d" " -f2)
-            ONE_SHOT_MAKEFILE="__none__" \
-                make -C $T -j $NUM_CPUS "$@"
-            ;;
-        *)
-            local NUM_CPUS=$(cat /proc/cpuinfo | grep "^processor" | wc -l)
-            ONE_SHOT_MAKEFILE="__none__" \
-                mk_timer schedtool -B -n 1 -e ionice -n 1 \
-                make -C $T -j $NUM_CPUS "$@"
-            ;;
-    esac
-}
-
-
-function repolastsync() {
-    RLSPATH="$ANDROID_BUILD_TOP/.repo/.repo_fetchtimes.json"
-    RLSLOCAL=$(date -d "$(stat -c %z $RLSPATH)" +"%e %b %Y, %T %Z")
-    RLSUTC=$(date -d "$(stat -c %z $RLSPATH)" -u +"%e %b %Y, %T %Z")
-    echo "Last repo sync: $RLSLOCAL / $RLSUTC"
-}
-
 function reposync() {
     case `uname -s` in
         Darwin)
@@ -2439,7 +2163,7 @@ function dopush()
         echo "Device Found."
     fi
 
-    if (adb shell getprop ro.cm.device | grep -q "$CM_BUILD") || [ "$FORCE_PUSH" == "true" ];
+    if (adb shell cat /system/build.prop | grep -q "ro.cm.device=$CM_BUILD");
     then
     # retrieve IP and PORT info if we're using a TCP connection
     TCPIPPORT=$(adb devices | egrep '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+:[0-9]+[^0-9]+' \
@@ -2457,11 +2181,7 @@ function dopush()
     adb remount &> /dev/null
 
     mkdir -p $OUT
-    ($func $*|tee $OUT/.log;return ${PIPESTATUS[0]})
-    ret=$?;
-    if [ $ret -ne 0 ]; then
-        rm -f $OUT/.log;return $ret
-    fi
+    $func $* | tee $OUT/.log
 
     # Install: <file>
     LOC="$(cat $OUT/.log | sed -r 's/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g' | grep '^Install: ' | cut -d ':' -f 2)"
@@ -2635,10 +2355,10 @@ function get_make_command()
   echo command make
 }
 
-function mk_timer()
+function make()
 {
     local start_time=$(date +"%s")
-    $@
+    $(get_make_command) "$@"
     local ret=$?
     local end_time=$(date +"%s")
     local tdiff=$(($end_time-$start_time))
@@ -2663,11 +2383,6 @@ function mk_timer()
     return $ret
 }
 
-function make()
-{
-    mk_timer $(get_make_command) "$@"
-}
-
 
 
 if [ "x$SHELL" != "x/bin/bash" ]; then
diff --git a/target/board/generic_arm64/BoardConfig.mk b/target/board/generic_arm64/BoardConfig.mk
index 8afd5a8..818f857 100644
--- a/target/board/generic_arm64/BoardConfig.mk
+++ b/target/board/generic_arm64/BoardConfig.mk
@@ -76,7 +76,7 @@ BUILD_EMULATOR_OPENGL := true
 USE_OPENGL_RENDERER := true
 
 TARGET_USERIMAGES_USE_EXT4 := true
-BOARD_SYSTEMIMAGE_PARTITION_SIZE := 943718400
+BOARD_SYSTEMIMAGE_PARTITION_SIZE := 845427200
 BOARD_USERDATAIMAGE_PARTITION_SIZE := 576716800
 BOARD_CACHEIMAGE_PARTITION_SIZE := 69206016
 BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE := ext4
diff --git a/target/board/generic_mips/BoardConfig.mk b/target/board/generic_mips/BoardConfig.mk
index 7cef968..a319ad7 100644
--- a/target/board/generic_mips/BoardConfig.mk
+++ b/target/board/generic_mips/BoardConfig.mk
@@ -50,7 +50,7 @@ BUILD_EMULATOR_OPENGL := true
 USE_OPENGL_RENDERER := true
 
 TARGET_USERIMAGES_USE_EXT4 := true
-BOARD_SYSTEMIMAGE_PARTITION_SIZE := 838860800
+BOARD_SYSTEMIMAGE_PARTITION_SIZE := 786432000
 BOARD_USERDATAIMAGE_PARTITION_SIZE := 576716800
 BOARD_CACHEIMAGE_PARTITION_SIZE := 69206016
 BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE := ext4
diff --git a/target/board/generic_x86_64/BoardConfig.mk b/target/board/generic_x86_64/BoardConfig.mk
index 295ee2b..c4fd958 100644
--- a/target/board/generic_x86_64/BoardConfig.mk
+++ b/target/board/generic_x86_64/BoardConfig.mk
@@ -41,7 +41,7 @@ BUILD_EMULATOR_OPENGL := true
 USE_OPENGL_RENDERER := true
 
 TARGET_USERIMAGES_USE_EXT4 := true
-BOARD_SYSTEMIMAGE_PARTITION_SIZE := 1073741824 # 1GB 
+BOARD_SYSTEMIMAGE_PARTITION_SIZE := 943718400 # 900MB
 BOARD_USERDATAIMAGE_PARTITION_SIZE := 576716800
 BOARD_CACHEIMAGE_PARTITION_SIZE := 69206016
 BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE := ext4
diff --git a/target/product/base.mk b/target/product/base.mk
index 751677a..9713330 100644
--- a/target/product/base.mk
+++ b/target/product/base.mk
@@ -110,19 +110,9 @@ PRODUCT_PACKAGES += \
     settings \
     svc \
     tc \
-    tm \
     vdc \
     vold \
     wm
 
 
-PRODUCT_COPY_FILES := $(call add-to-product-copy-files-if-exists,\
-    frameworks/base/preloaded-classes:system/etc/preloaded-classes)
-
-# Note: it is acceptable to not have a compiled-classes file. In that case, all boot classpath
-#       classes will be compiled.
-PRODUCT_COPY_FILES += $(call add-to-product-copy-files-if-exists,\
-    frameworks/base/compiled-classes:system/etc/compiled-classes)
-
-
 $(call inherit-product, $(SRC_TARGET_DIR)/product/embedded.mk)
diff --git a/target/product/core.mk b/target/product/core.mk
index a871780..876a536 100644
--- a/target/product/core.mk
+++ b/target/product/core.mk
@@ -15,10 +15,9 @@
 #
 
 # Base configuration for communication-oriented android devices
-# (phones, tablets, etc.).  If you want a change to apply to ALMOST ALL
+# (phones, tablets, etc.).  If you want a change to apply to ALL
 # devices (including non-phones and non-tablets), modify
-# core_minimal.mk instead. If you care about wearables, you need to modify
-# core_tiny.mk in addition to core_minimal.mk.
+# core_minimal.mk instead.
 
 PRODUCT_PACKAGES += \
     BasicDreams \
@@ -39,6 +38,7 @@ PRODUCT_PACKAGES += \
     InputDevices \
     KeyChain \
     Keyguard \
+    LatinIME \
     Launcher2 \
     ManagedProvisioning \
     PicoTts \
diff --git a/target/product/core_base.mk b/target/product/core_base.mk
index 4873ba4..57a63de 100644
--- a/target/product/core_base.mk
+++ b/target/product/core_base.mk
@@ -50,7 +50,6 @@ PRODUCT_PACKAGES += \
     libstagefright_soft_vorbisdec \
     libstagefright_soft_vpxdec \
     libstagefright_soft_vpxenc \
-    libstagefright_soft_dtsdec \
     libvariablespeed \
     libwebrtc_audio_preprocessing \
     mdnsd \
diff --git a/target/product/core_minimal.mk b/target/product/core_minimal.mk
index 89b1cba..d735f94 100644
--- a/target/product/core_minimal.mk
+++ b/target/product/core_minimal.mk
@@ -97,8 +97,6 @@ PRODUCT_BOOT_JARS := \
 
 # The order of PRODUCT_SYSTEM_SERVER_JARS matters.
 PRODUCT_SYSTEM_SERVER_JARS := \
-    org.cyanogenmod.platform \
-    org.cyanogenmod.hardware \
     services \
     ethernet-service \
     wifi-service
diff --git a/target/product/core_tiny.mk b/target/product/core_tiny.mk
index 477bd8f..8523e22 100644
--- a/target/product/core_tiny.mk
+++ b/target/product/core_tiny.mk
@@ -43,7 +43,6 @@ PRODUCT_PACKAGES += \
     DefaultContainerService \
     SettingsProvider \
     Shell \
-    bcc \
     bu \
     com.android.location.provider \
     com.android.location.provider.xml \
diff --git a/target/product/full_base.mk b/target/product/full_base.mk
index f9b8505..b642da0 100644
--- a/target/product/full_base.mk
+++ b/target/product/full_base.mk
@@ -21,6 +21,10 @@
 
 PRODUCT_PACKAGES := \
     libfwdlockengine \
+    OpenWnn \
+    libWnnEngDic \
+    libWnnJpnDic \
+    libwnndict \
     WAPPushManager
 
 # Additional settings used in all AOSP builds
@@ -30,18 +34,17 @@ PRODUCT_PROPERTY_OVERRIDES := \
 # Put en_US first in the list, so make it default.
 PRODUCT_LOCALES := en_US
 
+# Include drawables for all densities
+PRODUCT_AAPT_CONFIG := normal hdpi xhdpi xxhdpi
+
 # Get some sounds
 $(call inherit-product-if-exists, frameworks/base/data/sounds/AllAudio.mk)
 
 # Get the TTS language packs
 $(call inherit-product-if-exists, external/svox/pico/lang/all_pico_languages.mk)
 
-ifeq ($(TARGET_LOCALES),)
 # Get a list of languages.
 $(call inherit-product, $(SRC_TARGET_DIR)/product/locales_full.mk)
-else
-PRODUCT_LOCALES := $(TARGET_LOCALES)
-endif
 
 # Get everything else from the parent package
 $(call inherit-product, $(SRC_TARGET_DIR)/product/generic_no_telephony.mk)
diff --git a/target/product/full_base_telephony.mk b/target/product/full_base_telephony.mk
index 7988e9b..3079919 100644
--- a/target/product/full_base_telephony.mk
+++ b/target/product/full_base_telephony.mk
@@ -19,8 +19,14 @@
 # build quite specifically for the emulator, and might not be
 # entirely appropriate to inherit from for on-device configurations.
 
+PRODUCT_PACKAGES := \
+    VoiceDialer
+
 PRODUCT_PROPERTY_OVERRIDES := \
     keyguard.no_require_sim=true
 
+PRODUCT_COPY_FILES := \
+    frameworks/native/data/etc/handheld_core_hardware.xml:system/etc/permissions/handheld_core_hardware.xml
+
 $(call inherit-product, $(SRC_TARGET_DIR)/product/aosp_base.mk)
 $(call inherit-product, $(SRC_TARGET_DIR)/product/telephony.mk)
diff --git a/target/product/generic_no_telephony.mk b/target/product/generic_no_telephony.mk
index a938a5f..c97da70 100644
--- a/target/product/generic_no_telephony.mk
+++ b/target/product/generic_no_telephony.mk
@@ -59,7 +59,6 @@ $(call inherit-product-if-exists, external/google-fonts/dancing-script/fonts.mk)
 $(call inherit-product-if-exists, external/google-fonts/carrois-gothic-sc/fonts.mk)
 $(call inherit-product-if-exists, external/google-fonts/coming-soon/fonts.mk)
 $(call inherit-product-if-exists, external/google-fonts/cutive-mono/fonts.mk)
-$(call inherit-product-if-exists, external/lohit-fonts/fonts.mk)
 $(call inherit-product-if-exists, external/noto-fonts/fonts.mk)
 $(call inherit-product-if-exists, external/naver-fonts/fonts.mk)
 $(call inherit-product-if-exists, frameworks/base/data/keyboards/keyboards.mk)
diff --git a/target/product/languages_full.mk b/target/product/languages_full.mk
index b4116ca..030777e 100644
--- a/target/product/languages_full.mk
+++ b/target/product/languages_full.mk
@@ -21,7 +21,4 @@
 
 # These are all the locales that have translations and are displayable
 # by TextView in this branch.
-PRODUCT_LOCALES := en_US en_AU en_IN fr_FR it_IT es_ES et_EE de_DE nl_NL cs_CZ pl_PL ja_JP zh_TW zh_CN zh_HK ru_RU ko_KR nb_NO es_US da_DK el_GR tr_TR pt_PT pt_BR rm_CH sv_SE bg_BG ca_ES en_GB fi_FI hi_IN hr_HR hu_HU in_ID iw_IL lt_LT lv_LV ro_RO sk_SK sl_SI sr_RS uk_UA vi_VN tl_PH ar_EG fa_IR th_TH sw_TZ ms_MY af_ZA zu_ZA am_ET hi_IN en_XA ar_XB fr_CA km_KH lo_LA ne_NP si_LK mn_MN hy_AM az_AZ ka_GE my_MM mr_IN ml_IN is_IS mk_MK ky_KG eu_ES gl_ES bn_BD ta_IN kn_IN te_IN uz_UZ ur_PK kk_KZ
-
-# CyanogenMod
-PRODUCT_LOCALES += ast_ES lb_LU ku_IQ
+PRODUCT_LOCALES := en_AU en_US en_IN fr_FR it_IT es_ES et_EE de_DE nl_NL cs_CZ pl_PL ja_JP zh_TW zh_CN zh_HK ru_RU ko_KR nb_NO es_US da_DK el_GR tr_TR pt_PT pt_BR rm_CH sv_SE bg_BG ca_ES en_GB fi_FI hi_IN hr_HR hu_HU in_ID iw_IL lt_LT lv_LV ro_RO sk_SK sl_SI sr_RS uk_UA vi_VN tl_PH ar_EG fa_IR th_TH sw_TZ ms_MY af_ZA zu_ZA am_ET hi_IN en_XA ar_XB fr_CA km_KH lo_LA ne_NP si_LK mn_MN hy_AM az_AZ ka_GE my_MM mr_IN ml_IN is_IS mk_MK ky_KG eu_ES gl_ES bn_BD ta_IN kn_IN te_IN uz_UZ ur_PK kk_KZ
diff --git a/target/product/sdk_base.mk b/target/product/sdk_base.mk
index 451c0b7..8610169 100644
--- a/target/product/sdk_base.mk
+++ b/target/product/sdk_base.mk
@@ -95,10 +95,14 @@ $(call inherit-product, $(SRC_TARGET_DIR)/product/core.mk)
 -include external/svox/pico/lang/PicoLangFrFrInSystem.mk
 -include external/svox/pico/lang/PicoLangItItInSystem.mk
 
-# locale. en_US is both first and in alphabetical order to
+# locale + densities. en_US is both first and in alphabetical order to
 # ensure this is the default locale.
 PRODUCT_LOCALES := \
 	en_US \
+	ldpi \
+	hdpi \
+	mdpi \
+	xhdpi \
 	ar_EG \
 	ar_IL \
 	bg_BG \
diff --git a/target/product/security/bacon.x509.pem b/target/product/security/bacon.x509.pem
new file mode 100644
index 0000000..8f55bd8
--- /dev/null
+++ b/target/product/security/bacon.x509.pem
@@ -0,0 +1,23 @@
+-----BEGIN CERTIFICATE-----
+MIID3zCCAsegAwIBAgIJAMAEaDG2oZE8MA0GCSqGSIb3DQEBBQUAMIGFMQswCQYD
+VQQGEwJVUzETMBEGA1UECAwKV2FzaGluZ3RvbjEQMA4GA1UEBwwHU2VhdHRsZTEX
+MBUGA1UECgwOQ3lhbm9nZW4sIEluYy4xGzAZBgNVBAsMElJlbGVhc2UgTWFuYWdl
+bWVudDEZMBcGA1UEAwwQQ3lhbm9nZW4gUmVsZWFzZTAeFw0xNDA0MzAwMjE4NTFa
+Fw00MTA5MTUwMjE4NTFaMIGFMQswCQYDVQQGEwJVUzETMBEGA1UECAwKV2FzaGlu
+Z3RvbjEQMA4GA1UEBwwHU2VhdHRsZTEXMBUGA1UECgwOQ3lhbm9nZW4sIEluYy4x
+GzAZBgNVBAsMElJlbGVhc2UgTWFuYWdlbWVudDEZMBcGA1UEAwwQQ3lhbm9nZW4g
+UmVsZWFzZTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMLEPlqqjB5b
+fnylUIqCqN40pXNieEavUTIozE/mocMEb57kReaQPCf611MaOe/LlHvCKfPgYZTa
+dzs7c6iFzza5wmeorJW+jnOuCEytIAK1U1t2cRgqezuaUB8GqPbsCy4nP2+3Ch63
+4oaS+m3BTNijEykNBBswCGglxIt+1d3UaUTg9cGf7fCtSnLew4mQ7G6gy12gpoea
+21NSkLp9iRmcvOeFoxgi4SEo/4VR8NzhjvN1ZUCWl9s9tosHcNcW6ibzHz/P3SkD
+AxK1ZCix0OTYVVe+4yjeOgjjLt4QP1ZOSBd0e2u06DhadsuwXivK5DH0vkC6vjpO
+mkAOBV4+oucCAwEAAaNQME4wHQYDVR0OBBYEFLnnevtXiuaduDhDNch66eOzRh6E
+MB8GA1UdIwQYMBaAFLnnevtXiuaduDhDNch66eOzRh6EMAwGA1UdEwQFMAMBAf8w
+DQYJKoZIhvcNAQEFBQADggEBAHx3TDbdacWTq8I89oUyN+HcoUvI6ItlXSc7LUMj
+KBKp5dlweu9BP9Mpv9IlS90nCZZUbQ3WIA8lrEWaZ98LQRsNM7tdgoGniqWSpg4D
+rmUVzIRe0XDZHEWeN3zbASHrSOP84oGGw0ChGJuwdEezJVvBxi6WHeNzjxCqUxd7
+XwDkL/sadRgUSIVc9bkv49TOZuZyq0MUFcNtvFAS1t5LpUp6Zt9vjgAixq7XDLh+
+Sx8cmT0JNYf1ISq24cev1vQYBgrktNpRcjZXw/5q0bMBepmZ6KG86rVmKUDiBqVM
+Rpty9+c2AELMFu1zQp22LDYttn7e8JXijIgOjF7UKlhzmDo=
+-----END CERTIFICATE-----
diff --git a/target/product/security/verity_key b/target/product/security/verity_key
index 31982d95ad57005430b65bb28dbfd39adb231347..8db965facbb8cfc22f5418000acb0d118a18499f 100644
GIT binary patch
literal 524
zcmV+n0`vVq0001OoL)hjGLpw;A2{ZE;8#px_Q5XYJmg6Sn+q22i+3_!<h6;OEfrAr
z&6?4WYe`8&pVUujbo}SweNIoefe$DYD;w*JDZ0QlISxvFQEB_1D!*a56ydSMEYQ_q
z#HfOcQKXVSZBD3Pb}1sa4BWq)6jQ3B?HGa&!-b}S+oPCNOk%&0MDAVZWdmu5Bys;>
z3>7*#JyVz`Cw}yzgzR+mS5>~Zv*(NJVAtv?$+6U)bdQ0Z+=ER#cycyX(HeyfDx_qF
z1_SWm8=E_DiG=qVIJ%VmPD=7CY8*yQRlkNJRvB?F4wI&F$Yf2+=|)7?>`+_tVg}&9
z*E#6bL1Z6m)y2?iCk807a+246%|5~;ET_?botL=gcagYDie0T7lZZ1J<<AI>P<0Wn
zx8uDb>uK1EW6%YK1pV162)M~CFpM67=CzKu(VB&*tPWxNU+JxY%6dzPpPLl>YD_G^
z=?V$THLOdjkSB4@LCPtXxWsPuRWAtwL57lKY`V+%LxPFouOh22H>5xz*jFM$=dUd+
zoykznefa&Gz!~z}>dd#upG{Thv`6xWv@1MR7?S+#`n{Y(uy6G?VzkXFC6RqJ)mJLY
zZozX?_APuge*QJ|LC7fEuZM-Pp^2Ndm>nfSF2nLo>Urs{--A+gqFe<}z2F2?ySxW_
O5(Neb`b5V80097p)B`vG

literal 524
zcmV+n0`vVq00007*!mSo>Tao#&V;50r@F4bKzox<9++;YhGi5$I#idbYH7%!gcSSG
zjZae}y3`boUmbd`5WTVonJYyjH_?}81VAOG?KlZH!bT%-&z#cDqTNRgHqKMN0Be6#
z?V_a5V*#PXP_N7@dpFq#?Nd5PI>zK$CUaFy$QiQ{%|P2wH4m8=>gHUHPxnu1$}IZs
zNz%@6L)vET*7q}=yX%%u%J<B>-5hU2_YhlG{L7_)_Deb!lMK$yeyDyHog5qH$*mC+
zD;$a|osKh5N4pdix_!oqwf(_EPPpQ;nTbN6pz9B2r;9TX>td>c^rm4m<k943Y3JKJ
zvwM1xR822Uc<bo$)V56<?tTX*4ig{!*kQgTEl~d8ITcUHcp^`fs*oiW!<cfce8#<S
z@EvrginISO;oC!4flr3**$(}X<+5yFAk{W>RQO5Icr(+stoA0|`|}7FmS+A=VX_>8
zY{CI-h?eiWdWapYX_c4Z!3_b~H#5fBwAQHXK>snOd)%677I<*-H^|dT5Y^3neCwAV
z5g%<5@KOth19r#9E`RiKU>MT}%f}#VQ~0Nv^RDDd7csx@I@O`AG3{dh1+U@!cT>&$
z`t<&6{lIv<Iq+D{(ZlNCp6ePw%zg@c-P?l)B)GY|`=a7O!m*3$poV4yLIg&)SkhJz
OQmO{;c1c750098P_6ukL

diff --git a/target/product/security/verity_private_dev_key b/target/product/security/verity_private_dev_key
new file mode 100644
index 0000000..92528e9
--- /dev/null
+++ b/target/product/security/verity_private_dev_key
@@ -0,0 +1,28 @@
+-----BEGIN PRIVATE KEY-----
+MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDQxdVrH2RB1eg5
+17/gBmLzW1Ds10RG6ctNZMhxppMOLnEZViKGv1VNRhxqK/JKTv2UujgZ94SJcDub
+G+DwAwaGZKQqDYUa0VU2cng8TYPcnYGPdJ7Usckp6tdg64vns7e+VVf0dOyEovR+
+JyeYUz05OhUMYP9xJIhpA2XnXe5Ekb9iTFSYo9uBpoXDD4IY7aOqUxSbv9wMtyIp
+dl+oTm0+kqRRi4KoxGHV0CzDseEUuWG/Kp/7aVF9Sg45NcC6KYvrGysUKA+Bt09O
+feDn/HRpT9SfRElJa5DRms33UBUtnom15F4yd4vvFgubB0nkPOUuwfZhTFfgeuY4
+H2bHkjKbAgMBAAECggEAMpFYqkPGQvQO9cO+ZALoAM4Dgfp6PTrv1WUt7+lLAUpa
+dqqYXk8F2Fu9EjJm03ziix237QI5Bhk7Nsy/5SK2d+L0qILx1JcTrsZ3PRQBdnRo
+J1k2B4qwkQii9oTXNF4hiWaekUWo7E+ULOJLAuhWkf/xjTgJZ1xT9iuuiSYFSnIa
+9ABNH0vCaKEkW/4ri6fdtXmO26C/ltJlnozl86x07PIFh4uBas7/40E8ykFP00CS
+zdhMh+2DGyCb1Q0eJ1IfGILNatkLNEd2BHgQ7qNBkN9yShZfhvIPblr5gSUlZplX
+diV20ZGLAfByKWgZZWKkwl9KzaisL/J/4dr2UlSVEQKBgQDxAYTsgoTkkP0TKzr3
+i3ljT8OuVOj6TwZVBJYe2MIJ3veivS3gWB53FpsKthbib7y8ifIakn15mQkNCK5R
+7H7F5lvZCNnB6shY5Dz7nLJxKLALcAg+d12l3gTbFQeFDs0iQQJF7P8hs/GPF7kY
+Layb7EF0uzYjyHJCKtFdaZaeZwKBgQDdwvCb7NJVeGTcE97etL+8acu9y4GlqKEF
+o0Vkw8TjNKj/KuDkbkAk9hXxU1ZCmDU3y6r8CVHYl0Sqh08plEhkYB/j3sFy81zY
+3xu/rLFysBwjeJHHlPjRTYkdKr9pABmm8NIEShvu9u8i+mpOhjbX72HxZL+i4Fou
+gz58wEdBrQKBgG8CfyKdn+7UJe3tbLTXRquK8xxauhGJ0uXYPfmpZ/8596C7OOVs
+UWQTQoj1hKb6RtolRCIfNbKL3hJl3D2aDG7Fg6r9m6fpqCzhvIE9FShwUF6EVRfI
+zZb4JA5xqkwMnEpZ3V0uI/p3Mx3xFG3ho+8SLLhC/1YOHysBI/y+BQWjAoGAYiqQ
+PkXYWhOAeleleeqDUdF3al3y1zVNimRbLJ7owjcmdEYz5YrUhEgXMIvWjIY6UKes
+2gL6IynbMK3TIjHM1fojQ8jw04TdXfdtnizBJGbHHgCab8IHXwe2oZ2xu7ZapKbI
+ITP5J5BSDabSdk49attB/Qy/NEeiRCK+/5RSNsUCgYAg6vX9VqMEkhPHeoFfdLGD
+EQPPN6QLrQ4Zif0GKxH96znNSv0rXdNp9t0kyapdgzMuCwIEuOkCSiKgmfjTWnYO
+qh5HMUuD2VbfWwI9jVujQMRmqiaFF7VxxA1bP5j1hJlI6cn1Fjlpi+NsNZN4nm3Q
+92SEwX2vDgjrU0NAtFFL1Q==
+-----END PRIVATE KEY-----
diff --git a/target/product/verity.mk b/target/product/verity.mk
index 0361b64..4a1ca5e 100644
--- a/target/product/verity.mk
+++ b/target/product/verity.mk
@@ -17,11 +17,7 @@
 # Provides dependencies necessary for verified boot
 
 PRODUCT_SUPPORTS_VERITY := true
-
-# The dev key is used to sign boot and recovery images, and the verity
-# metadata table. Actual product deliverables will be re-signed by hand.
-# We expect this file to exist with the suffixes ".x509.pem" and ".pk8".
-PRODUCT_VERITY_SIGNING_KEY := build/target/product/security/verity
+PRODUCT_VERITY_SIGNING_KEY := build/target/product/security/verity_private_dev_key
 
 PRODUCT_PACKAGES += \
         verity_key
diff --git a/tools/buildinfo.sh b/tools/buildinfo.sh
index a549607..6331ee5 100644
--- a/tools/buildinfo.sh
+++ b/tools/buildinfo.sh
@@ -10,15 +10,12 @@ echo "ro.build.version.sdk=$PLATFORM_SDK_VERSION"
 echo "ro.build.version.codename=$PLATFORM_VERSION_CODENAME"
 echo "ro.build.version.all_codenames=$PLATFORM_VERSION_ALL_CODENAMES"
 echo "ro.build.version.release=$PLATFORM_VERSION"
-echo "ro.build.version.security_patch=$PLATFORM_SECURITY_PATCH"
-echo "ro.build.version.base_os=$PLATFORM_BASE_OS"
 echo "ro.build.date=`date`"
 echo "ro.build.date.utc=`date +%s`"
 echo "ro.build.type=$TARGET_BUILD_TYPE"
 echo "ro.build.user=$USER"
 echo "ro.build.host=`hostname`"
 echo "ro.build.tags=$BUILD_VERSION_TAGS"
-echo "ro.build.flavor=$TARGET_BUILD_FLAVOR"
 echo "ro.product.brand=$PRODUCT_BRAND"
 echo "ro.product.name=$PRODUCT_NAME"
 echo "ro.product.board=$TARGET_BOOTLOADER_BOARD_NAME"
diff --git a/tools/device/cm.mk.template b/tools/device/cm.mk.template
index e07898a..ed5ce1d 100644
--- a/tools/device/cm.mk.template
+++ b/tools/device/cm.mk.template
@@ -1,3 +1,6 @@
+## Specify phone tech before including full_phone
+$(call inherit-product, vendor/cm/config/gsm.mk)
+
 # Release name
 PRODUCT_RELEASE_NAME := __DEVICE__
 
diff --git a/tools/droiddoc/templates-sdk/assets/css/default.css b/tools/droiddoc/templates-sdk/assets/css/default.css
index f411d93..96259a7 100644
--- a/tools/droiddoc/templates-sdk/assets/css/default.css
+++ b/tools/droiddoc/templates-sdk/assets/css/default.css
@@ -466,9 +466,6 @@ a.back-link {
     text-transform: uppercase;
 }
 
-.content-header .paging-links {
-  margin-top:-25px;
-}
 .paging-links {
   position: relative;
   height:30px; }
@@ -3065,15 +3062,15 @@ select.ide option {
   font-size:16px;
   font-weight:500;
 }
-/* hide all except studio by default */
-.select-ide.eclipse,
+/* hide all except eclipse by default */
+.select-ide.studio,
 .select-ide.other {
   display:none;
 }
-/* ... unless studio also includes one of the others */
-.select-ide.studio.eclipse,
-.select-ide.studio.other {
-  display:none;
+/* ... unless eclipse also includes one of the others */
+.select-ide.eclipse.studio,
+.select-ide.eclipse.other {
+  display:block;
 }
 
 
@@ -3155,7 +3152,6 @@ div#deprecatedSticker {
   -webkit-box-shadow:-5px 5px 10px #ccc;
 }
 
-div#langMessage,
 div#naMessage {
   display:none;
   width:555px;
@@ -3163,8 +3159,6 @@ div#naMessage {
   margin:0 auto;
 }
 
-
-div#langMessage>div,
 div#naMessage div {
   z-index:99;
   width:450px;
@@ -3178,16 +3172,12 @@ div#naMessage div {
   -webkit-box-shadow:-10px 10px 40px #888;
 }
 /* IE6 can't position fixed */
-* html div#langMessage>div,
 * html div#naMessage div { position:absolute; }
 
 div#naMessage strong {
   font-size:1.1em;
 }
 
-div#langMessage .lang {
-  display:none;
-}
 
 /* --------------------------------------------------------------------------
 Slideshow Controls & Next/Prev
@@ -4066,7 +4056,7 @@ EndColorStr='#ececec');
   height: 38px;
 }
 #header-wrapper #nav-x ul.nav-x li {
-  margin-right: 31px !important;
+  margin-right: 36px !important;
   margin-top: 5px;
   margin-bottom: 0px;
   height: 30px;
@@ -4424,7 +4414,7 @@ body.reference a[name] {
 }
 
 #header-wrap.quicknav {
-  height:216px;
+  height:196px;
 
 }
 
@@ -6950,76 +6940,38 @@ a.landing-button:visited {
   display:none;
   position:fixed;
   top:0;
-  left:0;
-  width:100%;
+  left:-10px;
+  width:102%;
   height:100%;
-  background-color:rgba(0,0,0,0.8);
-  z-index:9999;
+  background-color:rgba(0,0,0,0.7);
+  z-index:99;
 }
 
 #video-frame {
   width:940px;
-  height:100%;
-  margin:72px auto;
+  height:526.4px;
+  margin:80px auto 0;
   display:none;
-  position:relative;
 }
 
 .video-close {
-  cursor: pointer;
-  position: absolute;
-  right: -49px;
-  top: -49px;
-  pointer-events: all;
+cursor: pointer;
+position: relative;
+left: 940px;
+top: 0;
+pointer-events: all;
 }
 
 #icon-video-close {
-  background-image: url("../images/close-white.png");
-  background-image: -webkit-image-set(url(../images/close-white.png) 1x, url(../images/close-white_2x.png) 2x);
-  background-repeat: no-repeat;
-  background-position: 0 0;
-  background-size: 36px 36px;
-  height: 36px;
-  width: 36px;
-  display:block;
+background-image: url("../images/close.png");
+background-position: 0 0;
+height: 36px;
+width: 36px;
+display:block;
 }
 
-#icon-video-close:hover {
-  background-image: url("../images/close-grey.png");
-  background-image: -webkit-image-set(url(../images/close-grey.png) 1x, url(../images/close-grey_2x.png) 2x);
-}
 
-/* Preload the hover images */
-a.video-shadowbox-button.white:after {
-  display:none;
-  content:url("../images/close-grey.png") url("../images/close-grey_2x.png");
-}
 
-a.video-shadowbox-button.white {
-  background-image: url("../images/play-circle-white.png");
-  background-image: -webkit-image-set(url(../images/play-circle-white.png) 1x, url(../images/play-circle-white_2x.png) 2x);
-  background-size: 36px 36px;
-  background-repeat: no-repeat;
-  background-position: right;
-  padding: 16px 42px 16px 8px;
-  font-size: 18px;
-  font-weight: 500;
-  line-height: 24px;
-  color: #fff;
-  text-decoration:none;
-}
-
-a.video-shadowbox-button.white:hover {
-  color:#bababa !important;
-  background-image: url("../images/play-circle-grey.png");
-  background-image: -webkit-image-set(url(../images/play-circle-grey.png) 1x, url(../images/play-circle-grey_2x.png) 2x);
-}
-
-/* Preload the hover images */
-a.video-shadowbox-button.white:after {
-  display:none;
-  content:url("../images/play-circle-grey.png") url("../images/play-circle-grey_2x.png");
-}
 
 /******************
 Styles for d.a.c/index:
@@ -7400,7 +7352,7 @@ a.home-new-cta-btn:hover,
 
 /* Helpouts widget */
 .resource-card-6x2.helpouts-card {
-  width: 255px;
+  width: 220px;
   height: 40px;
   position:absolute;
   z-index:999;
@@ -7437,4 +7389,4 @@ a.home-new-cta-btn:hover,
 
 .resource-card-6x2 > .card-bg.helpouts-card-bg:after {
   display:none;
-}
+}
\ No newline at end of file
diff --git a/tools/droiddoc/templates-sdk/assets/js/docs.js b/tools/droiddoc/templates-sdk/assets/js/docs.js
index 7f4be4e..a556f4f 100644
--- a/tools/droiddoc/templates-sdk/assets/js/docs.js
+++ b/tools/droiddoc/templates-sdk/assets/js/docs.js
@@ -21,17 +21,6 @@ $.ajaxSetup({
 
 $(document).ready(function() {
 
-  // show lang dialog if the URL includes /intl/
-  //if (location.pathname.substring(0,6) == "/intl/") {
-  //  var lang = location.pathname.split('/')[2];
-   // if (lang != getLangPref()) {
-   //   $("#langMessage a.yes").attr("onclick","changeLangPref('" + lang
-   //       + "', true); $('#langMessage').hide(); return false;");
-  //    $("#langMessage .lang." + lang).show();
-   //   $("#langMessage").show();
-   // }
-  //}
-
   // load json file for JD doc search suggestions
   $.getScript(toRoot + 'jd_lists_unified.js');
   // load json file for Android API search suggestions
@@ -234,8 +223,6 @@ $(document).ready(function() {
       $("#nav-x li.engage a").addClass("selected");
     } else if (secondFrag == "monetize") {
       $("#nav-x li.monetize a").addClass("selected");
-    } else if (secondFrag == "analyze") {
-      $("#nav-x li.analyze a").addClass("selected");
     } else if (secondFrag == "tools") {
       $("#nav-x li.disttools a").addClass("selected");
     } else if (secondFrag == "stories") {
@@ -560,147 +547,10 @@ false; // navigate across topic boundaries only in design docs
     cookiePath = "distribute_";
   }
 
-
-  /* setup shadowbox for any videos that want it */
-  var $videoLinks = $("a.video-shadowbox-button, a.notice-developers-video");
-  if ($videoLinks.length) {
-    // if there's at least one, add the shadowbox HTML to the body
-    $('body').prepend(
-'<div id="video-container">'+
-  '<div id="video-frame">'+
-    '<div class="video-close">'+
-      '<span id="icon-video-close" onclick="closeVideo()">&nbsp;</span>'+
-    '</div>'+
-    '<div id="youTubePlayer"></div>'+
-  '</div>'+
-'</div>');
-
-    // loads the IFrame Player API code asynchronously.
-    $.getScript("https://www.youtube.com/iframe_api");
-
-    $videoLinks.each(function() {
-      var videoId = $(this).attr('href').split('?v=')[1];
-      $(this).click(function(event) {
-        event.preventDefault();
-        startYouTubePlayer(videoId);
-      });
-    });
-  }
 });
 // END of the onload event
 
 
-var youTubePlayer;
-function onYouTubeIframeAPIReady() {
-}
-
-/* Returns the height the shadowbox video should be. It's based on the current
-   height of the "video-frame" element, which is 100% height for the window.
-   Then minus the margin so the video isn't actually the full window height. */
-function getVideoHeight() {
-  var frameHeight = $("#video-frame").height();
-  var marginTop = $("#video-frame").css('margin-top').split('px')[0];
-  return frameHeight - (marginTop * 2);
-}
-
-var mPlayerPaused = false;
-
-function startYouTubePlayer(videoId) {
-  $("#video-container").show();
-  $("#video-frame").show();
-  mPlayerPaused = false;
-
-  // compute the size of the player so it's centered in window
-  var maxWidth = 940;  // the width of the web site content
-  var videoAspect = .5625; // based on 1280x720 resolution
-  var maxHeight = maxWidth * videoAspect;
-  var videoHeight = getVideoHeight();
-  var videoWidth = videoHeight / videoAspect;
-  if (videoWidth > maxWidth) {
-    videoWidth = maxWidth;
-    videoHeight = maxHeight;
-  }
-  $("#video-frame").css('width', videoWidth);
-
-  // check if we've already created this player
-  if (youTubePlayer == null) {
-    // check if there's a start time specified
-    var idAndHash = videoId.split("#");
-    var startTime = 0;
-    if (idAndHash.length > 1) {
-      startTime = idAndHash[1].split("t=")[1] != undefined ? idAndHash[1].split("t=")[1] : 0;
-    }
-    // enable localized player
-    var lang = getLangPref();
-    var captionsOn = lang == 'en' ? 0 : 1;
-
-    youTubePlayer = new YT.Player('youTubePlayer', {
-      height: videoHeight,
-      width: videoWidth,
-      videoId: idAndHash[0],
-      playerVars: {start: startTime, hl: lang, cc_load_policy: captionsOn},
-      events: {
-        'onReady': onPlayerReady,
-        'onStateChange': onPlayerStateChange
-      }
-    });
-  } else {
-    // reset the size in case the user adjusted the window since last play
-    youTubePlayer.setSize(videoWidth, videoHeight);
-    // if a video different from the one already playing was requested, cue it up
-    if (videoId != youTubePlayer.getVideoUrl().split('?v=')[1].split('&')[0].split('%')[0]) {
-      youTubePlayer.cueVideoById(videoId);
-    }
-    youTubePlayer.playVideo();
-  }
-}
-
-function onPlayerReady(event) {
-  event.target.playVideo();
-  mPlayerPaused = false;
-}
-
-function closeVideo() {
-  try {
-    youTubePlayer.pauseVideo();
-  } catch(e) {
-  }
-  $("#video-container").fadeOut(200);
-}
-
-/* Track youtube playback for analytics */
-function onPlayerStateChange(event) {
-    // Video starts, send the video ID
-    if (event.data == YT.PlayerState.PLAYING) {
-      if (mPlayerPaused) {
-        ga('send', 'event', 'Videos', 'Resume',
-            youTubePlayer.getVideoUrl().split('?v=')[1].split('&')[0].split('%')[0]);
-      } else {
-        // track the start playing event so we know from which page the video was selected
-        ga('send', 'event', 'Videos', 'Start: ' +
-            youTubePlayer.getVideoUrl().split('?v=')[1].split('&')[0].split('%')[0],
-            'on: ' + document.location.href);
-      }
-      mPlayerPaused = false;
-    }
-    // Video paused, send video ID and video elapsed time
-    if (event.data == YT.PlayerState.PAUSED) {
-      ga('send', 'event', 'Videos', 'Paused',
-            youTubePlayer.getVideoUrl().split('?v=')[1].split('&')[0].split('%')[0],
-            youTubePlayer.getCurrentTime());
-      mPlayerPaused = true;
-    }
-    // Video finished, send video ID and video elapsed time
-    if (event.data == YT.PlayerState.ENDED) {
-      ga('send', 'event', 'Videos', 'Finished',
-            youTubePlayer.getVideoUrl().split('?v=')[1].split('&')[0].split('%')[0],
-            youTubePlayer.getCurrentTime());
-      mPlayerPaused = true;
-    }
-}
-
-
-
 function initExpandableNavItems(rootTag) {
   $(rootTag + ' li.nav-section .nav-section-header').click(function() {
     var section = $(this).closest('li.nav-section');
@@ -808,7 +658,7 @@ function toggleFullscreen(enable) {
     setTimeout(updateSidenavFixedWidth,delay); // need to wait a moment for css to switch
     enabled = false;
   }
-  writeCookie("fullscreen", enabled, null);
+  writeCookie("fullscreen", enabled, null, null);
   setNavBarLeftPos();
   resizeNav(delay);
   updateSideNavPosition();
@@ -969,7 +819,7 @@ function reInitScrollbars() {
 function saveNavPanels() {
   var basePath = getBaseUri(location.pathname);
   var section = basePath.substring(1,basePath.indexOf("/",1));
-  writeCookie("height", resizePackagesNav.css("height"), section);
+  writeCookie("height", resizePackagesNav.css("height"), section, null);
 }
 
 
@@ -1050,12 +900,16 @@ function readCookie(cookie) {
   return 0;
 }
 
-function writeCookie(cookie, val, section) {
+function writeCookie(cookie, val, section, expiration) {
   if (val==undefined) return;
   section = section == null ? "_" : "_"+section+"_";
-  var age = 2*365*24*60*60; // set max-age to 2 years
+  if (expiration == null) {
+    var date = new Date();
+    date.setTime(date.getTime()+(10*365*24*60*60*1000)); // default expiration is one week
+    expiration = date.toGMTString();
+  }
   var cookieValue = cookie_namespace + section + cookie + "=" + val
-                    + "; max-age=" + age +"; path=/";
+                    + "; expires=" + expiration+"; path=/";
   document.cookie = cookieValue;
 }
 
@@ -1295,7 +1149,9 @@ function swapNav() {
     nav_pref = NAV_PREF_TREE;
     init_default_navtree(toRoot);
   }
-  writeCookie("nav", nav_pref, "reference");
+  var date = new Date();
+  date.setTime(date.getTime()+(10*365*24*60*60*1000)); // keep this for 10 years
+  writeCookie("nav", nav_pref, "reference", date.toGMTString());
 
   $("#nav-panels").toggle();
   $("#panel-link").toggle();
@@ -1363,7 +1219,11 @@ function changeNavLang(lang) {
 }
 
 function changeLangPref(lang, submit) {
-  writeCookie("pref_lang", lang, null);
+  var date = new Date();
+  expires = date.toGMTString(date.setTime(date.getTime()+(10*365*24*60*60*1000)));
+  // keep this for 50 years
+  //alert("expires: " + expires)
+  writeCookie("pref_lang", lang, null, expires);
 
   //  #######  TODO:  Remove this condition once we're stable on devsite #######
   //  This condition is only needed if we still need to support legacy GAE server
@@ -1782,8 +1642,8 @@ var gDocsListLength = 0;
 
 function onSuggestionClick(link) {
   // When user clicks a suggested document, track it
-  ga('send', 'event', 'Suggestion Click', 'clicked: ' + $(link).attr('href'),
-                'query: ' + $("#search_autocomplete").val().toLowerCase());
+  ga('send', 'event', 'Suggestion Click', 'clicked: ' + $(link).text(),
+            'from: ' + $("#search_autocomplete").val());
 }
 
 function set_item_selected($li, selected)
@@ -2128,7 +1988,7 @@ function search_changed(e, kd, toroot)
 
 
         // Search for matching JD docs
-        if (text.length >= 2) {
+        if (text.length >= 3) {
           // Regex to match only the beginning of a word
           var textRegex = new RegExp("\\b" + text.toLowerCase(), "g");
 
@@ -2756,8 +2616,8 @@ function addResultClickListeners() {
   $("#searchResults a.gs-title").each(function(index, link) {
     // When user clicks enter for Google search results, track it
     $(link).click(function() {
-      ga('send', 'event', 'Google Click', 'clicked: ' + $(this).attr('href'),
-                'query: ' + $("#search_autocomplete").val().toLowerCase());
+      ga('send', 'event', 'Google Click', 'clicked: ' + $(this).text(),
+                'from: ' + $("#search_autocomplete").val());
     });
   });
 }
@@ -2872,7 +2732,10 @@ function changeApiLevel() {
   selectedLevel = parseInt($("#apiLevelSelector option:selected").val());
   toggleVisisbleApis(selectedLevel, "body");
 
-  writeCookie(API_LEVEL_COOKIE, selectedLevel, null);
+  var date = new Date();
+  date.setTime(date.getTime()+(10*365*24*60*60*1000)); // keep this for 10 years
+  var expiration = date.toGMTString();
+  writeCookie(API_LEVEL_COOKIE, selectedLevel, null, expiration);
 
   if (selectedLevel < minLevel) {
     var thing = ($("#jd-header").html().indexOf("package") != -1) ? "package" : "class";
@@ -4353,4 +4216,4 @@ function showSamples() {
       }
     }
   }
-})();
+})();
\ No newline at end of file
diff --git a/tools/droiddoc/templates-sdk/components/masthead.cs b/tools/droiddoc/templates-sdk/components/masthead.cs
index c09dc02..2dde104 100644
--- a/tools/droiddoc/templates-sdk/components/masthead.cs
+++ b/tools/droiddoc/templates-sdk/components/masthead.cs
@@ -3,52 +3,6 @@
   <?cs call:preview_masthead() ?>
 <?cs else ?>
 <a name="top"></a>
-
-<!-- dialog to prompt lang pref change when loaded from hardcoded URL 
-<div id="langMessage" style="display:none">
-  <div>
-    <div class="lang en">
-      <p>You requested a page in English, would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang es">
-      <p>You requested a page in Spanish (Espanol), would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang ja">
-      <p>You requested a page in Japanese (???), would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang ko">
-      <p>You requested a page in Korean (???), would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang ru">
-      <p>You requested a page in Russian (), would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang zh-cn">
-      <p>You requested a page in Simplified Chinese (????), would you like to proceed with this language setting?</p>
-    </div>
-    <div class="lang zh-tw">
-      <p>You requested a page in Traditional Chinese (????), would you like to proceed with this language setting?</p>
-    </div>
-    <a href="#" class="button yes" onclick="return false;">
-      <span class="lang en">Yes</span>
-      <span class="lang es">Si</span>
-      <span class="lang ja">Yes</span>
-      <span class="lang ko">Yes</span>
-      <span class="lang ru">Yes</span>
-      <span class="lang zh-cn">??</span>
-      <span class="lang zh-tw">??</span>
-    </a>
-    <a href="#" class="button" onclick="$('#langMessage').hide();return false;">
-      <span class="lang en">No</span>
-      <span class="lang es">No</span>
-      <span class="lang ja">No</span>
-      <span class="lang ko">No</span>
-      <span class="lang ru">No</span>
-      <span class="lang zh-cn">??</span>
-      <span class="lang zh-tw">??</span>
-    </a>
-  </div>
-</div> -->
-
 <?cs if:!devsite ?><?cs # leave out the global header for devsite; it is in devsite template ?>
   <!-- Header -->
   <div id="header-wrapper">
@@ -171,7 +125,6 @@
                 <li><a href="<?cs var:toroot ?>distribute/users/index.html">Get Users</a></li>
                 <li><a href="<?cs var:toroot ?>distribute/engage/index.html">Engage &amp; Retain</a></li>
                 <li><a href="<?cs var:toroot ?>distribute/monetize/index.html">Monetize</a></li>
-                <li><a href="<?cs var:toroot ?>distribute/analyze/index.html">Analyze</a></li>
                 <li><a href="<?cs var:toroot ?>distribute/tools/index.html">Tools &amp; Reference</a></li>
                 <li><a href="<?cs var:toroot ?>distribute/stories/index.html">Developer Stories</a></li>
               </ul>
@@ -207,20 +160,31 @@
         <div class="wrap" style="position:relative;z-index:1">
 
         <?cs if:reference ?>
-        <?cs # HIDE HELPOUTS RECRUIT BANNER
             <a id="helpoutsLink" class="resource resource-card resource-card-6x2x3 resource-card-6x2 helpouts-card" 
-              href="http://helpouts.google.com/partner/landing/provider/googledevelopers?utm_source=dac&utm_medium=banner&utm_campaign=android_provider_banner3" target="_blank">
+              href="http://helpouts.google.com/partner/landing/provider/googledevelopers" target="_blank">
               <div class="card-bg helpouts-card-bg"></div>
               <div class="card-info">
                 <div class="helpouts-description">
-                  <div class="text">Help Android Wear and TV developers<br/>
-                    <span id="helpoutsLinkText" class="link-color" 
-                    style="display:block;padding-top:5px;text-align:right">Learn more</span>
+                  <div class="text">Help developers solve problems<br/>
+                    <span id="helpoutsLinkText" class="link-color" style="display:block;padding-top:5px;text-align:right">Learn more</span>
                   </div>
                 </div>
               </div>
             </a>
-        # END HIDE HELPOUTS ?>
+            <script>
+              var textA = "LEARN MORE";
+              var linkA = "http://helpouts.google.com/partner/landing/provider/googledevelopers?utm_source=android_banner1&utm_medium=banner&utm_campaign=android_provider_banner1";
+              var textB = "SIGN UP NOW";
+              var linkB = "http://helpouts.google.com/partner/landing/provider/googledevelopers?utm_source=android_banner2&utm_medium=banner&utm_campaign=android_provider_banner2";
+
+              if (Math.floor(1/Math.random()) > 1) {
+                $("a#helpoutsLink").attr('href', linkA);
+                $("span#helpoutsLinkText").text(textA);
+              } else {
+                $("a#helpoutsLink").attr('href', linkB);
+                $("span#helpoutsLinkText").text(textB);
+              }
+            </script>
         <?cs /if ?>
 
             <ul class="nav-x col-9 develop" style="width:100%">
@@ -269,7 +233,7 @@
     </div>
     <!-- /Sendondary x-nav DEVELOP -->
 
-  <?cs elif:distribute || googleplay || essentials || users || engage || monetize || analyze ||  disttools || stories ?>
+  <?cs elif:distribute || googleplay || essentials || users || engage || monetize || disttools || stories ?>
     <!-- Secondary distribute x-nav -->
     <div id="nav-x">
         <div class="wrap">
@@ -285,9 +249,6 @@
                 <li class="monetize"><a href="<?cs var:toroot ?>distribute/monetize/index.html"
                   >Monetize</a>
                 </li>
-                <li class="analyze"><a href="<?cs var:toroot ?>distribute/analyze/index.html"
-                  >Analyze</a>
-                </li>
                 <li class="disttools"><a href="<?cs var:toroot ?>distribute/tools/index.html"
                   >Tools</a>
                 </li>
@@ -385,4 +346,4 @@
 
 ?>    
 
-<?cs /def ?>
+<?cs /def ?>
\ No newline at end of file
diff --git a/tools/droiddoc/templates-sdk/customizations.cs b/tools/droiddoc/templates-sdk/customizations.cs
index c8c88cc..e0e3ca1 100644
--- a/tools/droiddoc/templates-sdk/customizations.cs
+++ b/tools/droiddoc/templates-sdk/customizations.cs
@@ -112,20 +112,6 @@
     </script>
 <?cs /def ?><?cs
 
-def:analyze_nav() ?>
-  <div class="wrap clearfix" id="body-content">
-    <div class="col-3" id="side-nav" itemscope itemtype="http://schema.org/SiteNavigationElement">
-      <div id="devdoc-nav" class="scroll-pane">
-<?cs include:"../../../../frameworks/base/docs/html/distribute/analyze/analyze_toc.cs" ?>
-      </div>
-    </div> <!-- end side-nav -->
-    <script>
-      $(document).ready(function() {
-        scrollIntoView("devdoc-nav");
-        });
-    </script>
-<?cs /def ?><?cs
-
 def:monetize_nav() ?>
   <div class="wrap clearfix" id="body-content">
     <div class="col-3" id="side-nav" itemscope itemtype="http://schema.org/SiteNavigationElement">
@@ -463,8 +449,8 @@
                 <option value="ja">???</option>
                 <option value="ko">???</option>
                 <option value="ru"></option>
-                <option value="zh-cn">??(??)</option>
-                <option value="zh-tw">??(??)</option>
+                <option value="zh-cn">?? (??)</option>
+                <option value="zh-tw">?? (??)</option>
             </select>
           </div>
         <script type="text/javascript">
@@ -558,8 +544,6 @@
       call:engage_nav() ?><?cs
     elif:monetize ?><?cs
       call:monetize_nav() ?><?cs
-    elif:analyze ?><?cs
-      call:analyze_nav() ?><?cs
     elif:disttools ?><?cs
       call:disttools_nav() ?><?cs
     elif:stories ?><?cs
diff --git a/tools/droiddoc/templates-sdk/docpage.cs b/tools/droiddoc/templates-sdk/docpage.cs
index d064222..7cd9d43 100644
--- a/tools/droiddoc/templates-sdk/docpage.cs
+++ b/tools/droiddoc/templates-sdk/docpage.cs
@@ -194,10 +194,10 @@
 
 <?cs include:"trailer.cs" ?>
   <script src="https://developer.android.com/ytblogger_lists_unified.js" type="text/javascript"></script>
-  <script src="<?cs var:toroot ?>jd_lists_unified.js?v=8" type="text/javascript"></script>
-  <script src="<?cs var:toroot ?>jd_extras.js?v=9" type="text/javascript"></script>
-  <script src="<?cs var:toroot ?>jd_collections.js?v=9" type="text/javascript"></script>
-  <script src="<?cs var:toroot ?>jd_tag_helpers.js?v=5" type="text/javascript"></script>
+  <script src="<?cs var:toroot ?>jd_lists_unified.js?v=3" type="text/javascript"></script>
+  <script src="<?cs var:toroot ?>jd_extras.js?v=4" type="text/javascript"></script>
+  <script src="<?cs var:toroot ?>jd_collections.js?v=4" type="text/javascript"></script>
+  <script src="<?cs var:toroot ?>jd_tag_helpers.js?v=3" type="text/javascript"></script>
 
 </body>
 </html>
diff --git a/tools/droiddoc/templates-sdk/head_tag.cs b/tools/droiddoc/templates-sdk/head_tag.cs
index 9f79f54..7ecb7f9 100644
--- a/tools/droiddoc/templates-sdk/head_tag.cs
+++ b/tools/droiddoc/templates-sdk/head_tag.cs
@@ -24,8 +24,8 @@
 <meta name="Description" content="<?cs var:page.metaDescription ?>"><?cs
   /if ?>
 <link rel="shortcut icon" type="image/x-icon" href="<?cs var:toroot ?>favicon.ico" />
-<title><?cs
-  if:page.title ?><?cs
+<title><?cs 
+  if:page.title ?><?cs 
     var:page.title ?> | <?cs
   /if ?>Android Developers</title>
 
@@ -38,7 +38,7 @@
 if:android.whichdoc != 'online' ?>http:<?cs
 /if ?>//fonts.googleapis.com/css?family=Roboto:light,regular,medium,thin,italic,mediumitalic,bold"
   title="roboto">
-<link href="<?cs var:toroot ?>assets/css/default.css?v=5" rel="stylesheet" type="text/css">
+<link href="<?cs var:toroot ?>assets/css/default.css?v=2" rel="stylesheet" type="text/css">
 
 <?cs if:reference && !(reference.gms || reference.gcm || preview) ?>
 <!-- FULLSCREEN STYLESHEET -->
@@ -62,14 +62,7 @@
   var metaTags = [<?cs var:meta.tags ?>];
   var devsite = <?cs if:devsite ?>true<?cs else ?>false<?cs /if ?>;
 </script>
-<script src="<?cs var:toroot ?>assets/js/docs.js?v=3" type="text/javascript"></script>
-
-<?cs if:helpoutsWidget ?>
-<script type="text/javascript" src="https://helpouts.google.com/ps/res/embed.js" defer async
-    data-helpouts-embed data-helpouts-vertical="programming"
-    data-helpouts-tags="<?cs var:page.tags ?>" data-helpouts-prefix="android"
-    data-helpouts-standalone="true"></script>
-<?cs /if ?>
+<script src="<?cs var:toroot ?>assets/js/docs.js?v=2" type="text/javascript"></script>
 
 <script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
diff --git a/tools/droiddoc/templates-sdk/macros_override.cs b/tools/droiddoc/templates-sdk/macros_override.cs
index 0a94598..1525be5 100644
--- a/tools/droiddoc/templates-sdk/macros_override.cs
+++ b/tools/droiddoc/templates-sdk/macros_override.cs
@@ -6,7 +6,7 @@
     <?cs each:anno = obj.showAnnotations ?>
       <?cs if:first(anno) ?>
         <span class='annotation-message'>
-          Included in documentation by the annotations:
+          Included in documention by the annotations:
       <?cs /if ?>
       @<?cs var:anno.type.label ?>
       <?cs if:last(anno) == 0 ?>
diff --git a/tools/droiddoc/templates-sdk/sdkpage.cs b/tools/droiddoc/templates-sdk/sdkpage.cs
index 817ac47..bbe6e97 100644
--- a/tools/droiddoc/templates-sdk/sdkpage.cs
+++ b/tools/droiddoc/templates-sdk/sdkpage.cs
@@ -264,137 +264,119 @@ <p>Redirecting to
 
 
 
-<div class="pax col-13 online" style="margin:0;">
+<h4><a href='' class="expandable"
+  onclick="toggleExpandable(this,'.pax');hideExpandable('.myide,.reqs');return false;"
+  >VIEW ALL DOWNLOADS AND SIZES</a></h4>
 
 
-<h3>SDK Tools Only</h3>
+<div class="pax col-13 online" style="display:none;margin:0;">
 
-<p>If you prefer to use a different IDE or run the tools from the
-command line or with build scripts, you can instead download the stand-alone Android SDK Tools.
-These packages provide the basic SDK tools for app development, without an IDE.
-Also see the <a href="<?cs var:toroot ?>tools/sdk/tools-notes.html">SDK tools release notes</a>.</p>
 
+<p class="table-caption"><strong>ADT Bundle</strong></p>
   <table class="download">
     <tr>
       <th>Platform</th>
       <th>Package</th>
       <th>Size</th>
-      <th>SHA-1 Checksum</th>
+      <th>MD5 Checksum</th>
   </tr>
   <tr>
-    <td rowspan="2">Windows</td>
+    <td>Windows 32-bit</td>
     <td>
-  <a onclick="return onDownload(this)" id="win-tools" href="http://dl.google.com/android/<?cs
-var:sdk.win_installer
-?>"><?cs var:sdk.win_installer ?></a> (Recommended)
+  <a onClick="return onDownload(this)" id="win-bundle32"
+     href="https://dl.google.com/android/adt/<?cs var:sdk.win32_bundle_download ?>"><?cs var:sdk.win32_bundle_download ?></a>
     </td>
-    <td><?cs var:sdk.win_installer_bytes ?> bytes</td>
-    <td><?cs var:sdk.win_installer_checksum ?></td>
+    <td><?cs var:sdk.win32_bundle_bytes ?> bytes</td>
+    <td><?cs var:sdk.win32_bundle_checksum ?></td>
   </tr>
   <tr>
-    <!-- blank TD from Windows rowspan -->
+    <td>Windows 64-bit</td>
     <td>
-  <a onclick="return onDownload(this)" href="http://dl.google.com/android/<?cs var:sdk.win_download
-?>"><?cs var:sdk.win_download ?></a>
+  <a onClick="return onDownload(this)" id="win-bundle64"
+     href="https://dl.google.com/android/adt/<?cs var:sdk.win64_bundle_download ?>"><?cs var:sdk.win64_bundle_download ?></a>
     </td>
-    <td><?cs var:sdk.win_bytes ?> bytes</td>
-    <td><?cs var:sdk.win_checksum ?></td>
+    <td><?cs var:sdk.win64_bundle_bytes ?> bytes</td>
+    <td><?cs var:sdk.win64_bundle_checksum ?></td>
   </tr>
   <tr>
-    <td><nobr>Mac OS X</nobr></td>
+    <td><nobr>Mac OS X 64-bit</nobr></td>
     <td>
-  <a onclick="return onDownload(this)" id="mac-tools" href="http://dl.google.com/android/<?cs
-var:sdk.mac_download
-?>"><?cs var:sdk.mac_download ?></a>
+  <a onClick="return onDownload(this)" id="mac-bundle64"
+     href="https://dl.google.com/android/adt/<?cs var:sdk.mac64_bundle_download ?>"><?cs var:sdk.mac64_bundle_download ?></a>
     </td>
-    <td><?cs var:sdk.mac_bytes ?> bytes</td>
-    <td><?cs var:sdk.mac_checksum ?></td>
+    <td><?cs var:sdk.mac64_bundle_bytes ?> bytes</td>
+    <td><?cs var:sdk.mac64_bundle_checksum ?></td>
   </tr>
   <tr>
-    <td>Linux</td>
+    <td>Linux 32-bit</td>
     <td>
-  <a onclick="return onDownload(this)" id="linux-tools" href="http://dl.google.com/android/<?cs
-var:sdk.linux_download
-?>"><?cs var:sdk.linux_download ?></a>
+  <a onClick="return onDownload(this)" id="linux-bundle32"
+     href="https://dl.google.com/android/adt/<?cs var:sdk.linux32_bundle_download ?>"><?cs var:sdk.linux32_bundle_download ?></a>
     </td>
-    <td><?cs var:sdk.linux_bytes ?> bytes</td>
-    <td><?cs var:sdk.linux_checksum ?></td>
+    <td><?cs var:sdk.linux32_bundle_bytes ?> bytes</td>
+    <td><?cs var:sdk.linux32_bundle_checksum ?></td>
+  </tr>
+  <tr>
+    <td>Linux 64-bit</td>
+    <td>
+  <a onClick="return onDownload(this)" id="linux-bundle64"
+     href="https://dl.google.com/android/adt/<?cs var:sdk.linux64_bundle_download ?>"><?cs var:sdk.linux64_bundle_download ?></a>
+    </td>
+    <td><?cs var:sdk.linux64_bundle_bytes ?> bytes</td>
+    <td><?cs var:sdk.linux64_bundle_checksum ?></td>
   </tr>
   </table>
 
 
-
-<h3>All Android Studio Packages</h3>
-
-<p>Select a specific Android Studio package for your platform. Also see the
-<a href="<?cs var:toroot ?>tools/revisions/studio.html">Android Studio release notes</a>.</p>
-
+<p class="table-caption"><strong>SDK Tools Only</strong></p>
   <table class="download">
     <tr>
       <th>Platform</th>
       <th>Package</th>
       <th>Size</th>
-      <th>SHA-1 Checksum</th>
-  </tr>
-
-  <tr>
-    <td rowspan="3">Windows</td>
-    <td>
-  <a onclick="return onDownload(this)" id="win-bundle"
-    href="https://dl.google.com/dl/android/studio/install/<?cs var:studio.version ?>/<?cs var:studio.win_bundle_exe_download ?>"
-    ><?cs var:studio.win_bundle_exe_download ?></a><br>(Recommended)
-    </td>
-    <td><?cs var:studio.win_bundle_exe_bytes ?> bytes</td>
-    <td><?cs var:studio.win_bundle_exe_checksum ?></td>
+      <th>MD5 Checksum</th>
   </tr>
-
   <tr>
-    <!-- blank TD from Windows rowspan -->
+    <td rowspan="2">Windows<br>32 &amp; 64-bit</td>
     <td>
-  <a onclick="return onDownload(this)"
-    href="https://dl.google.com/dl/android/studio/install/<?cs var:studio.version ?>/<?cs var:studio.win_notools_exe_download ?>"
-    ><?cs var:studio.win_notools_exe_download ?></a><br>(No SDK tools included)
+  <a onclick="return onDownload(this)" href="http://dl.google.com/android/<?cs var:sdk.win_download
+?>"><?cs var:sdk.win_download ?></a>
     </td>
-    <td><?cs var:studio.win_notools_exe_bytes ?> bytes</td>
-    <td><?cs var:studio.win_notools_exe_checksum ?></td>
+    <td><?cs var:sdk.win_bytes ?> bytes</td>
+    <td><?cs var:sdk.win_checksum ?></td>
   </tr>
-
   <tr>
     <!-- blank TD from Windows rowspan -->
     <td>
-  <a onclick="return onDownload(this)"
-    href="https://dl.google.com/dl/android/studio/ide-zips/<?cs var:studio.version ?>/<?cs var:studio.win_bundle_download ?>"
-    ><?cs var:studio.win_bundle_download ?></a>
+  <a onclick="return onDownload(this)" id="win-tools" href="http://dl.google.com/android/<?cs
+var:sdk.win_installer
+?>"><?cs var:sdk.win_installer ?></a> (Recommended)
     </td>
-    <td><?cs var:studio.win_bundle_bytes ?> bytes</td>
-    <td><?cs var:studio.win_bundle_checksum ?></td>
+    <td><?cs var:sdk.win_installer_bytes ?> bytes</td>
+    <td><?cs var:sdk.win_installer_checksum ?></td>
   </tr>
-
   <tr>
-    <td><nobr>Mac OS X</nobr></td>
+    <td><nobr>Mac OS X</nobr><br>32 &amp; 64-bit</td>
     <td>
-  <a onclick="return onDownload(this)" id="mac-bundle"
-    href="https://dl.google.com/dl/android/studio/install/<?cs var:studio.version ?>/<?cs var:studio.mac_bundle_download ?>"
-    ><?cs var:studio.mac_bundle_download ?></a>
+  <a onclick="return onDownload(this)" id="mac-tools" href="http://dl.google.com/android/<?cs
+var:sdk.mac_download
+?>"><?cs var:sdk.mac_download ?></a>
     </td>
-    <td><?cs var:studio.mac_bundle_bytes ?> bytes</td>
-    <td><?cs var:studio.mac_bundle_checksum ?></td>
+    <td><?cs var:sdk.mac_bytes ?> bytes</td>
+    <td><?cs var:sdk.mac_checksum ?></td>
   </tr>
-
   <tr>
-    <td>Linux</td>
+    <td>Linux<br>32 &amp; 64-bit</td>
     <td>
-  <a onclick="return onDownload(this)" id="linux-bundle"
-    href="https://dl.google.com/dl/android/studio/ide-zips/<?cs var:studio.version ?>/<?cs var:studio.linux_bundle_download ?>"
-    ><?cs var:studio.linux_bundle_download ?></a>
+  <a onclick="return onDownload(this)" id="linux-tools" href="http://dl.google.com/android/<?cs
+var:sdk.linux_download
+?>"><?cs var:sdk.linux_download ?></a>
     </td>
-    <td><?cs var:studio.linux_bundle_bytes ?> bytes</td>
-    <td><?cs var:studio.linux_bundle_checksum ?></td>
+    <td><?cs var:sdk.linux_bytes ?> bytes</td>
+    <td><?cs var:sdk.linux_checksum ?></td>
   </tr>
   </table>
 
-
-
 </div><!-- end pax -->
 
 
@@ -415,9 +397,7 @@ <p>Redirecting to
   var bundlename;
   var $toolslink;
 
-  if (navigator.appVersion.indexOf("Mobile")!=-1) {
-    // Do nothing for any "mobile" user agent
-  } else if (navigator.appVersion.indexOf("Win")!=-1) {
+  if (navigator.appVersion.indexOf("Win")!=-1) {
     os = "Windows";
     bundlename = '#win-bundle';
     $toolslink = $('#win-tools');
@@ -425,18 +405,26 @@ <p>Redirecting to
     os = "Mac";
     bundlename = '#mac-bundle';
     $toolslink = $('#mac-tools');
-  } else if (navigator.appVersion.indexOf("Linux")!=-1 && navigator.appVersion.indexOf("Android")==-1) {
+  } else if (navigator.appVersion.indexOf("Linux")!=-1) {
     os = "Linux";
     bundlename = '#linux-bundle';
     $toolslink = $('#linux-tools');
   }
 
-  if (os != undefined) {
+  if (os) {
     $('#not-supported').hide();
 
-    /* set up primary Android Studio download button */
-    $('.download-bundle-button').append(" <br/><span class='small'>for " + os + "</span>");
-    $('.download-bundle-button').click(function() {return onDownload(this,true,true);}).attr('href', bundlename);
+    /* set up primary adt download button */
+    $('#download-bundle-button').show();
+    $('#download-bundle-button').append("Download Eclipse ADT <br/><span class='small'>with the Android SDK for " + os + "</span>");
+    $('#download-bundle-button').click(function() {return onDownload(this,true,true);}).attr('href', bundlename);
+
+    /* set up sdk tools only button */
+    $('#download-tools-button').show();
+    $('#download-tools-button').append("Download the stand-alone Android SDK Tools for " + os);
+    $('#download-tools-button').click(function() {return onDownload(this,true);}).attr('href', $toolslink.attr('href'));
+  } else {
+    $('.pax').show();
   }
 
 
@@ -449,29 +437,44 @@ <p>Redirecting to
       $("#downloadForRealz").html("Download " + $(link).text());
     }
 
-    $("#downloadForRealz").attr('bundle', bundle);
-    $("a#downloadForRealz").attr("name", $(link).attr('href'));
+    /* if it's a bundle, show the 32/64-bit picker */
+    if (bundle) {
+      $("#downloadForRealz").attr('bundle','true');
+      if ($("#downloadForRealz").text().indexOf("Mac") == -1) {
+        $("p#bitpicker").show();
+      } else {
+        /* mac is always 64 bit, so set it checked */
+        $("p#bitpicker input[value=64]").attr('checked', true);
+      }
+      /* save link name until the bit version is chosen */
+      $("#downloadForRealz").attr('name',$(link).attr('href'));
+    } else {
+      /* if not using bundle, set download button to ignore bitpicker and set url */
+      $("#downloadForRealz").attr('bundle','false');
+      $("#downloadForRealz").attr('href',$(link).attr('href'));
+      /* set picker checked as a fake default */
+      $("p#bitpicker input[value=64]").attr('checked', true);
+      $("a#next-link").html("Setting Up an Existing IDE").attr('href',toRoot + 'sdk/installing/index.html');
+    }
 
-    $("#tos").show();
-    $("#landing").hide();
+    $("#tos").fadeIn('fast');
+    $("#landing").fadeOut('fast');
 
-    location.hash = "top";
+    location.hash = "download";
     return false;
   }
 
 
   function onAgreeChecked() {
-    /* verify that the TOS is agreed */
-    if ($("input#agree").is(":checked")) {
+    /* verify that the TOS is agreed and a bit version is chosen */
+    if ($("input#agree").is(":checked") && $("#bitpicker input:checked").length) {
 
       /* if downloading the bundle */
       if ($("#downloadForRealz").attr('bundle')) {
-        /* construct the name of the link we want */
-        linkId = $("a#downloadForRealz").attr("name");
+        /* construct the name of the link we want based on the bit version */
+        linkId = $("a#downloadForRealz").attr("name") + $("#bitpicker input:checked").val();
         /* set the real url for download */
         $("a#downloadForRealz").attr("href", $(linkId).attr("href"));
-      } else {
-        $("a#downloadForRealz").attr("href", $("a#downloadForRealz").attr("name"));
       }
 
       /* reveal the download button */
@@ -482,28 +485,25 @@ <p>Redirecting to
   }
 
   function onDownloadForRealz(link) {
-    if ($("input#agree").is(':checked')) {
-      location.hash = "";
-      location.hash = "top";
+    if ($("input#agree").is(':checked') && $("#bitpicker input:checked").length) {
       $("div.sdk-terms").slideUp();
-      $("h1#tos-header").text('Now downloading...');
-      $(".sdk-terms-intro").text('You\'ll be redirected to the install instructions in a moment.');
-      $("#sdk-terms-form").fadeOut('slow', function() {
+      $("h1#tos-header").text('Now redirecting to the install instructions...');
+      $("#sdk-terms-form,.sdk-terms-intro").fadeOut('slow', function() {
         setTimeout(function() {
           if ($("#downloadForRealz").attr('bundle') == 'true') {
-            // User downloaded the studio Bundle
-            window.location = "/sdk/installing/index.html?pkg=studio";
+            // User downloaded the ADT Bundle
+            window.location = "/sdk/installing/index.html?pkg=adt";
           } else {
             // User downloaded the SDK Tools
             window.location = "/sdk/installing/index.html?pkg=tools";
           }
-        }, 3000);
+        }, 500);
       });
       ga('send', 'event', 'SDK', 'IDE and Tools', $("#downloadForRealz").html());
       return true;
     } else {
-      $("label#agreeLabel").parent().stop().animate({color: "#258AAF"}, 200,
-        function() {$("label#agreeLabel").parent().stop().animate({color: "#222"}, 200)}
+      $("label#agreeLabel,#bitpicker input").parent().stop().animate({color: "#258AAF"}, 200,
+        function() {$("label#agreeLabel,#bitpicker input").parent().stop().animate({color: "#222"}, 200)}
       );
       return false;
     }
diff --git a/tools/post_process_props.py b/tools/post_process_props.py
index 82c92a8..a20ded9 100644
--- a/tools/post_process_props.py
+++ b/tools/post_process_props.py
@@ -40,9 +40,9 @@ def mangle_build_prop(prop, overrides):
 # Put the modifications that you need to make into the /default.prop into this
 # function. The prop object has get(name) and put(name,value) methods.
 def mangle_default_prop(prop):
-  # If ro.adb.secure is not 1, then enable adb on USB by default
-  # (this is for eng builds)
-  if prop.get("ro.adb.secure") != "1":
+  # If ro.debuggable is 1, then enable adb on USB by default
+  # (this is for userdebug builds)
+  if prop.get("ro.debuggable") == "1":
     val = prop.get("persist.sys.usb.config")
     if val == "":
       val = "adb"
diff --git a/tools/releasetools/add_img_to_target_files.py b/tools/releasetools/add_img_to_target_files.py
index db9895d..bf217e0 100644
--- a/tools/releasetools/add_img_to_target_files.py
+++ b/tools/releasetools/add_img_to_target_files.py
@@ -45,28 +45,10 @@
 
 OPTIONS = common.OPTIONS
 
-OPTIONS.add_missing = False
-OPTIONS.rebuild_recovery = False
 
-def AddSystem(output_zip, prefix="IMAGES/", recovery_img=None, boot_img=None):
+def AddSystem(output_zip, prefix="IMAGES/"):
   """Turn the contents of SYSTEM into a system image and store it in
   output_zip."""
-
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "system.img")
-  if os.path.exists(prebuilt_path):
-    print "system.img already exists in %s, no need to rebuild..." % (prefix,)
-    return
-
-  def output_sink(fn, data):
-     ofile = open(os.path.join(OPTIONS.input_tmp,"SYSTEM",fn), "w")
-     ofile.write(data)
-     ofile.close()
-
-  if OPTIONS.rebuild_recovery:
-    print("Building new recovery patch")
-    common.MakeRecoveryPatch(OPTIONS.input_tmp, output_sink, recovery_img, boot_img,
-                             info_dict=OPTIONS.info_dict)
-
   block_list = common.MakeTempFile(prefix="system-blocklist-", suffix=".map")
   imgname = BuildSystem(OPTIONS.input_tmp, OPTIONS.info_dict,
                         block_list=block_list)
@@ -85,12 +67,6 @@ def BuildSystem(input_dir, info_dict, block_list=None):
 def AddVendor(output_zip, prefix="IMAGES/"):
   """Turn the contents of VENDOR into a vendor image and store in it
   output_zip."""
-
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "vendor.img")
-  if os.path.exists(prebuilt_path):
-    print "vendor.img already exists in %s, no need to rebuild..." % (prefix,)
-    return
-
   block_list = common.MakeTempFile(prefix="vendor-blocklist-", suffix=".map")
   imgname = BuildVendor(OPTIONS.input_tmp, OPTIONS.info_dict,
                      block_list=block_list)
@@ -105,29 +81,6 @@ def BuildVendor(input_dir, info_dict, block_list=None):
   file containing it."""
   return CreateImage(input_dir, info_dict, "vendor", block_list=block_list)
 
-def AddOem(output_zip, prefix="IMAGES/"):
-  """Turn the contents of OEM into a oem image and store in it
-  output_zip."""
-
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "oem.img")
-  if os.path.exists(prebuilt_path):
-    print "oem.img already exists in %s, no need to rebuild..." % (prefix,)
-    return
-
-  block_list = common.MakeTempFile(prefix="oem-blocklist-", suffix=".map")
-  imgname = BuildOem(OPTIONS.input_tmp, OPTIONS.info_dict,
-                     block_list=block_list)
-  with open(imgname, "rb") as f:
-    common.ZipWriteStr(output_zip, prefix + "oem.img", f.read())
-  with open(block_list, "rb") as f:
-    common.ZipWriteStr(output_zip, prefix + "oem.map", f.read())
-
-
-def BuildOem(input_dir, info_dict, block_list=None):
-  """Build the (sparse) oem image and return the name of a temp
-  file containing it."""
-  return CreateImage(input_dir, info_dict, "oem", block_list=block_list)
-
 
 def CreateImage(input_dir, info_dict, what, block_list=None):
   print "creating " + what + ".img..."
@@ -178,11 +131,6 @@ def CreateImage(input_dir, info_dict, what, block_list=None):
 def AddUserdata(output_zip, prefix="IMAGES/"):
   """Create an empty userdata image and store it in output_zip."""
 
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "userdata.img")
-  if os.path.exists(prebuilt_path):
-    print "userdata.img already exists in %s, no need to rebuild..." % (prefix,)
-    return
-
   image_props = build_image.ImagePropFromGlobalDict(OPTIONS.info_dict,
                                                     "data")
   # We only allow yaffs to have a 0/missing partition_size.
@@ -214,61 +162,9 @@ def AddUserdata(output_zip, prefix="IMAGES/"):
   os.rmdir(temp_dir)
 
 
-def AddUserdataExtra(output_zip, prefix="IMAGES/"):
-  """Create extra userdata image and store it in output_zip."""
-
-  image_props = build_image.ImagePropFromGlobalDict(OPTIONS.info_dict,
-                                                  "data_extra")
-
-  # The build system has to explicitly request extra userdata.
-  if "fs_type" not in image_props:
-    return
-
-  extra_name = image_props.get("partition_name", "extra")
-
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "userdata_%s.img" % extra_name)
-  if os.path.exists(prebuilt_path):
-    print "userdata_%s.img already exists in %s, no need to rebuild..." % (extra_name, prefix,)
-    return
-
-  # We only allow yaffs to have a 0/missing partition_size.
-  # Extfs, f2fs must have a size. Skip userdata_extra.img if no size.
-  if (not image_props.get("fs_type", "").startswith("yaffs") and
-      not image_props.get("partition_size")):
-    return
-
-  print "creating userdata_%s.img..." % extra_name
-
-  # The name of the directory it is making an image out of matters to
-  # mkyaffs2image.  So we create a temp dir, and within it we create an
-  # empty dir named "data", and build the image from that.
-  temp_dir = tempfile.mkdtemp()
-  user_dir = os.path.join(temp_dir, "data")
-  os.mkdir(user_dir)
-  img = tempfile.NamedTemporaryFile()
-
-  fstab = OPTIONS.info_dict["fstab"]
-  if fstab:
-    image_props["fs_type" ] = fstab["/data"].fs_type
-  succ = build_image.BuildImage(user_dir, image_props, img.name)
-  assert succ, "build userdata_%s.img image failed" % extra_name
-
-  # Disable size check since this fetches original data partition size
-  #common.CheckSize(img.name, "userdata_extra.img", OPTIONS.info_dict)
-  output_zip.write(img.name, prefix + "userdata_%s.img" % extra_name)
-  img.close()
-  os.rmdir(user_dir)
-  os.rmdir(temp_dir)
-
-
 def AddCache(output_zip, prefix="IMAGES/"):
   """Create an empty cache image and store it in output_zip."""
 
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, prefix, "cache.img")
-  if os.path.exists(prebuilt_path):
-    print "cache.img already exists in %s, no need to rebuild..." % (prefix,)
-    return
-
   image_props = build_image.ImagePropFromGlobalDict(OPTIONS.info_dict,
                                                     "cache")
   # The build system has to explicitly request for cache.img.
@@ -301,11 +197,10 @@ def AddCache(output_zip, prefix="IMAGES/"):
 def AddImagesToTargetFiles(filename):
   OPTIONS.input_tmp, input_zip = common.UnzipTemp(filename)
 
-  if not OPTIONS.add_missing:
-    for n in input_zip.namelist():
-      if n.startswith("IMAGES/"):
-        print "target_files appears to already contain images."
-        sys.exit(1)
+  for n in input_zip.namelist():
+    if n.startswith("IMAGES/"):
+      print "target_files appears to already contain images."
+      sys.exit(1)
 
   try:
     input_zip.getinfo("VENDOR/")
@@ -313,12 +208,6 @@ def AddImagesToTargetFiles(filename):
   except KeyError:
     has_vendor = False
 
-  try:
-    input_zip.getinfo("OEM/")
-    has_oem = True
-  except KeyError:
-    has_oem = False
-
   OPTIONS.info_dict = common.LoadInfoDict(input_zip)
   if "selinux_fc" in OPTIONS.info_dict:
     OPTIONS.info_dict["selinux_fc"] = os.path.join(
@@ -332,69 +221,32 @@ def banner(s):
     print "\n\n++++ " + s + " ++++\n\n"
 
   banner("boot")
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, "IMAGES", "boot.img")
-  boot_image = None
-  if os.path.exists(prebuilt_path):
-    print "boot.img already exists in IMAGES/, no need to rebuild..."
-    if OPTIONS.rebuild_recovery:
-      boot_image = common.GetBootableImage(
-          "IMAGES/boot.img", "boot.img", OPTIONS.input_tmp, "BOOT")
-  else:
-    boot_image = common.GetBootableImage(
-        "IMAGES/boot.img", "boot.img", OPTIONS.input_tmp, "BOOT")
-    if boot_image:
-      boot_image.AddToZip(output_zip)
+  boot_image = common.GetBootableImage(
+      "IMAGES/boot.img", "boot.img", OPTIONS.input_tmp, "BOOT")
+  if boot_image:
+    boot_image.AddToZip(output_zip)
 
   banner("recovery")
-  recovery_image = None
-  prebuilt_path = os.path.join(OPTIONS.input_tmp, "IMAGES", "recovery.img")
-  if os.path.exists(prebuilt_path):
-    print "recovery.img already exists in IMAGES/, no need to rebuild..."
-    if OPTIONS.rebuild_recovery:
-      recovery_image = common.GetBootableImage(
-          "IMAGES/recovery.img", "recovery.img", OPTIONS.input_tmp, "RECOVERY")
-  else:
-    recovery_image = common.GetBootableImage(
-        "IMAGES/recovery.img", "recovery.img", OPTIONS.input_tmp, "RECOVERY")
-    if recovery_image:
-      recovery_image.AddToZip(output_zip)
+  recovery_image = common.GetBootableImage(
+      "IMAGES/recovery.img", "recovery.img", OPTIONS.input_tmp, "RECOVERY")
+  if recovery_image:
+    recovery_image.AddToZip(output_zip)
 
   banner("system")
-  AddSystem(output_zip, recovery_img=recovery_image, boot_img=boot_image)
+  AddSystem(output_zip)
   if has_vendor:
     banner("vendor")
     AddVendor(output_zip)
   banner("userdata")
   AddUserdata(output_zip)
-  banner("extrauserdata")
-  AddUserdataExtra(output_zip)
   banner("cache")
   AddCache(output_zip)
-  if has_oem:
-    banner("oem")
-    AddOem(output_zip)
-
 
   output_zip.close()
 
-def main(argv):
-
-  def option_handler(o, a):
-    if o in ("-a", "--add_missing"):
-      OPTIONS.add_missing = True
-    elif o in ("-r", "--rebuild_recovery",):
-      OPTIONS.rebuild_recovery = True
-    else:
-      return False
-    return True
-
-  args = common.ParseOptions(argv, __doc__,
-                             extra_opts="ar",
-                             extra_long_opts=["add_missing",
-                                              "rebuild_recovery",
-                                              ],
-                             extra_option_handler=option_handler)
 
+def main(argv):
+  args = common.ParseOptions(argv, __doc__)
 
   if len(args) != 1:
     common.Usage(__doc__)
diff --git a/tools/releasetools/blockimgdiff.py b/tools/releasetools/blockimgdiff.py
index 079523a..216486c 100644
--- a/tools/releasetools/blockimgdiff.py
+++ b/tools/releasetools/blockimgdiff.py
@@ -16,8 +16,6 @@
 
 from collections import deque, OrderedDict
 from hashlib import sha1
-import common
-import heapq
 import itertools
 import multiprocessing
 import os
@@ -28,12 +26,6 @@
 import threading
 import tempfile
 
-try:
-  from backports import lzma
-except ImportError:
-  lzma = None
-  pass
-
 from rangelib import *
 
 __all__ = ["EmptyImage", "DataImage", "BlockImageDiff"]
@@ -150,22 +142,9 @@ def __init__(self, tgt_name, src_name, tgt_ranges, src_ranges, style, by_id):
     self.goes_before = {}
     self.goes_after = {}
 
-    self.stash_before = []
-    self.use_stash = []
-
     self.id = len(by_id)
     by_id.append(self)
 
-  def NetStashChange(self):
-    return (sum(sr.size() for (_, sr) in self.stash_before) -
-            sum(sr.size() for (_, sr) in self.use_stash))
-
-  def ConvertToNew(self):
-    assert self.style != "new"
-    self.use_stash = []
-    self.style = "new"
-    self.src_ranges = RangeSet()
-
   def __str__(self):
     return (str(self.id) + ": <" + str(self.src_ranges) + " " + self.style +
             " to " + str(self.tgt_ranges) + ">")
@@ -203,15 +182,11 @@ def __str__(self):
 # original image.
 
 class BlockImageDiff(object):
-  def __init__(self, tgt, src=None, threads=None, version=3, use_lzma=False):
+  def __init__(self, tgt, src=None, threads=None):
     if threads is None:
       threads = multiprocessing.cpu_count() // 2
       if threads == 0: threads = 1
     self.threads = threads
-    self.version = version
-    self.use_lzma = use_lzma
-
-    assert version in (1, 2, 3)
 
     self.tgt = tgt
     if src is None:
@@ -246,145 +221,28 @@ def Compute(self, prefix):
     self.FindVertexSequence()
     # Fix up the ordering dependencies that the sequence didn't
     # satisfy.
-    if self.version == 1:
-      self.RemoveBackwardEdges()
-    else:
-      self.ReverseBackwardEdges()
-      self.ImproveVertexSequence()
-
-    # Ensure the runtime stash size is under the limit.
-    if self.version >= 2 and common.OPTIONS.cache_size is not None:
-      self.ReviseStashSize()
-
+    self.RemoveBackwardEdges()
     # Double-check our work.
     self.AssertSequenceGood()
 
     self.ComputePatches(prefix)
     self.WriteTransfers(prefix)
 
-  def HashBlocks(self, source, ranges):
-    data = source.ReadRangeSet(ranges)
-    ctx = sha1()
-
-    for p in data:
-      ctx.update(p)
-
-    return ctx.hexdigest()
-
   def WriteTransfers(self, prefix):
     out = []
 
+    out.append("1\n")   # format version number
     total = 0
     performs_read = False
 
-    stashes = {}
-    stashed_blocks = 0
-    max_stashed_blocks = 0
-
-    free_stash_ids = []
-    next_stash_id = 0
-
     for xf in self.transfers:
 
-      if self.version < 2:
-        assert not xf.stash_before
-        assert not xf.use_stash
-
-      for s, sr in xf.stash_before:
-        assert s not in stashes
-        if free_stash_ids:
-          sid = heapq.heappop(free_stash_ids)
-        else:
-          sid = next_stash_id
-          next_stash_id += 1
-        stashes[s] = sid
-        stashed_blocks += sr.size()
-        if self.version == 2:
-          out.append("stash %d %s\n" % (sid, sr.to_string_raw()))
-        else:
-          sh = self.HashBlocks(self.src, sr)
-          if sh in stashes:
-            stashes[sh] += 1
-          else:
-            stashes[sh] = 1
-            out.append("stash %s %s\n" % (sh, sr.to_string_raw()))
-
-      if stashed_blocks > max_stashed_blocks:
-        max_stashed_blocks = stashed_blocks
-
-      free_string = []
-
-      if self.version == 1:
-        src_string = xf.src_ranges.to_string_raw()
-      elif self.version >= 2:
-
-        #   <# blocks> <src ranges>
-        #     OR
-        #   <# blocks> <src ranges> <src locs> <stash refs...>
-        #     OR
-        #   <# blocks> - <stash refs...>
-
-        size = xf.src_ranges.size()
-        src_string = [str(size)]
-
-        unstashed_src_ranges = xf.src_ranges
-        mapped_stashes = []
-        for s, sr in xf.use_stash:
-          sid = stashes.pop(s)
-          stashed_blocks -= sr.size()
-          unstashed_src_ranges = unstashed_src_ranges.subtract(sr)
-          sh = self.HashBlocks(self.src, sr)
-          sr = xf.src_ranges.map_within(sr)
-          mapped_stashes.append(sr)
-          if self.version == 2:
-            src_string.append("%d:%s" % (sid, sr.to_string_raw()))
-            # A stash will be used only once. We need to free the stash
-            # immediately after the use, instead of waiting for the automatic
-            # clean-up at the end. Because otherwise it may take up extra space
-            # and lead to OTA failures.
-            # Bug: 23119955
-            free_string.append("free %d\n" % (sid,))
-          else:
-            assert sh in stashes
-            src_string.append("%s:%s" % (sh, sr.to_string_raw()))
-            stashes[sh] -= 1
-            if stashes[sh] == 0:
-              free_string.append("free %s\n" % (sh))
-              stashes.pop(sh)
-          heapq.heappush(free_stash_ids, sid)
-
-        if unstashed_src_ranges:
-          src_string.insert(1, unstashed_src_ranges.to_string_raw())
-          if xf.use_stash:
-            mapped_unstashed = xf.src_ranges.map_within(unstashed_src_ranges)
-            src_string.insert(2, mapped_unstashed.to_string_raw())
-            mapped_stashes.append(mapped_unstashed)
-            self.AssertPartition(RangeSet(data=(0, size)), mapped_stashes)
-        else:
-          src_string.insert(1, "-")
-          self.AssertPartition(RangeSet(data=(0, size)), mapped_stashes)
-
-        src_string = " ".join(src_string)
-
-      # all versions:
-      #   zero <rangeset>
-      #   new <rangeset>
-      #   erase <rangeset>
-      #
-      # version 1:
-      #   bsdiff patchstart patchlen <src rangeset> <tgt rangeset>
-      #   imgdiff patchstart patchlen <src rangeset> <tgt rangeset>
-      #   move <src rangeset> <tgt rangeset>
-      #
-      # version 2:
-      #   bsdiff patchstart patchlen <tgt rangeset> <src_string>
-      #   imgdiff patchstart patchlen <tgt rangeset> <src_string>
-      #   move <tgt rangeset> <src_string>
-      #
-      # version 3:
-      #   bsdiff patchstart patchlen srchash tgthash <tgt rangeset> <src_string>
-      #   imgdiff patchstart patchlen srchash tgthash <tgt rangeset> <src_string>
-      #   move hash <tgt rangeset> <src_string>
+      # zero [rangeset]
+      # new [rangeset]
+      # bsdiff patchstart patchlen [src rangeset] [tgt rangeset]
+      # imgdiff patchstart patchlen [src rangeset] [tgt rangeset]
+      # move [src rangeset] [tgt rangeset]
+      # erase [rangeset]
 
       tgt_size = xf.tgt_ranges.size()
 
@@ -397,51 +255,17 @@ def WriteTransfers(self, prefix):
         assert xf.tgt_ranges
         assert xf.src_ranges.size() == tgt_size
         if xf.src_ranges != xf.tgt_ranges:
-          if self.version == 1:
-            out.append("%s %s %s\n" % (
-                xf.style,
-                xf.src_ranges.to_string_raw(), xf.tgt_ranges.to_string_raw()))
-          elif self.version == 2:
-            out.append("%s %s %s\n" % (
-                xf.style,
-                xf.tgt_ranges.to_string_raw(), src_string))
-          elif self.version >= 3:
-            # take into account automatic stashing of overlapping blocks
-            if xf.src_ranges.overlaps(xf.tgt_ranges):
-              temp_stash_usage = stashed_blocks + xf.src_ranges.size();
-              if temp_stash_usage > max_stashed_blocks:
-                max_stashed_blocks = temp_stash_usage
-
-            out.append("%s %s %s %s\n" % (
-                xf.style,
-                self.HashBlocks(self.tgt, xf.tgt_ranges),
-                xf.tgt_ranges.to_string_raw(), src_string))
+          out.append("%s %s %s\n" % (
+              xf.style,
+              xf.src_ranges.to_string_raw(), xf.tgt_ranges.to_string_raw()))
           total += tgt_size
       elif xf.style in ("bsdiff", "imgdiff"):
         performs_read = True
         assert xf.tgt_ranges
         assert xf.src_ranges
-        if self.version == 1:
-          out.append("%s %d %d %s %s\n" % (
-              xf.style, xf.patch_start, xf.patch_len,
-              xf.src_ranges.to_string_raw(), xf.tgt_ranges.to_string_raw()))
-        elif self.version == 2:
-          out.append("%s %d %d %s %s\n" % (
-              xf.style, xf.patch_start, xf.patch_len,
-              xf.tgt_ranges.to_string_raw(), src_string))
-        elif self.version >= 3:
-          # take into account automatic stashing of overlapping blocks
-          if xf.src_ranges.overlaps(xf.tgt_ranges):
-            temp_stash_usage = stashed_blocks + xf.src_ranges.size();
-            if temp_stash_usage > max_stashed_blocks:
-              max_stashed_blocks = temp_stash_usage
-
-          out.append("%s %d %d %s %s %s %s\n" % (
-              xf.style,
-              xf.patch_start, xf.patch_len,
-              self.HashBlocks(self.src, xf.src_ranges),
-              self.HashBlocks(self.tgt, xf.tgt_ranges),
-              xf.tgt_ranges.to_string_raw(), src_string))
+        out.append("%s %d %d %s %s\n" % (
+            xf.style, xf.patch_start, xf.patch_len,
+            xf.src_ranges.to_string_raw(), xf.tgt_ranges.to_string_raw()))
         total += tgt_size
       elif xf.style == "zero":
         assert xf.tgt_ranges
@@ -452,23 +276,7 @@ def WriteTransfers(self, prefix):
       else:
         raise ValueError, "unknown transfer style '%s'\n" % (xf.style,)
 
-      if free_string:
-        out.append("".join(free_string))
-
-      if self.version >= 2 and common.OPTIONS.cache_size is not None:
-        # Sanity check: abort if we're going to need more stash space than
-        # the allowed size (cache_size * threshold). There are two purposes
-        # of having a threshold here. a) Part of the cache may have been
-        # occupied by some recovery logs. b) It will buy us some time to deal
-        # with the oversize issue.
-        cache_size = common.OPTIONS.cache_size
-        stash_threshold = common.OPTIONS.stash_threshold
-        max_allowed = cache_size * stash_threshold
-        assert max_stashed_blocks * self.tgt.blocksize < max_allowed, \
-               'Stash size %d (%d * %d) exceeds the limit %d (%d * %.2f)' % (
-                   max_stashed_blocks * self.tgt.blocksize, max_stashed_blocks,
-                   self.tgt.blocksize, max_allowed, cache_size,
-                   stash_threshold)
+    out.insert(1, str(total) + "\n")
 
     all_tgt = RangeSet(data=(0, self.tgt.total_blocks))
     if performs_read:
@@ -481,113 +289,17 @@ def WriteTransfers(self, prefix):
     else:
       # if nothing is read (ie, this is a full OTA), then we can start
       # by erasing the entire partition.
-      out.insert(0, "erase %s\n" % (all_tgt.to_string_raw(),))
-
-    out.insert(0, "%d\n" % (self.version,))   # format version number
-    out.insert(1, str(total) + "\n")
-    if self.version >= 2:
-      # version 2 only: after the total block count, we give the number
-      # of stash slots needed, and the maximum size needed (in blocks)
-      out.insert(2, str(next_stash_id) + "\n")
-      out.insert(3, str(max_stashed_blocks) + "\n")
+      out.insert(2, "erase %s\n" % (all_tgt.to_string_raw(),))
 
     with open(prefix + ".transfer.list", "wb") as f:
       for i in out:
         f.write(i)
 
-    if self.version >= 2:
-      max_stashed_size = max_stashed_blocks * self.tgt.blocksize
-      OPTIONS = common.OPTIONS
-      if OPTIONS.cache_size is not None:
-        max_allowed = OPTIONS.cache_size * OPTIONS.stash_threshold
-        print("max stashed blocks: %d  (%d bytes), "
-              "limit: %d bytes (%.2f%%)\n" % (
-              max_stashed_blocks, max_stashed_size, max_allowed,
-              max_stashed_size * 100.0 / max_allowed))
-      else:
-        print("max stashed blocks: %d  (%d bytes), limit: <unknown>\n" % (
-              max_stashed_blocks, max_stashed_size))
-
-  def ReviseStashSize(self):
-    print("Revising stash size...")
-    stashes = {}
-
-    # Create the map between a stash and its def/use points. For example, for a
-    # given stash of (idx, sr), stashes[idx] = (sr, def_cmd, use_cmd).
-    for xf in self.transfers:
-      # Command xf defines (stores) all the stashes in stash_before.
-      for idx, sr in xf.stash_before:
-        stashes[idx] = (sr, xf)
-
-      # Record all the stashes command xf uses.
-      for idx, _ in xf.use_stash:
-        stashes[idx] += (xf,)
-
-    # Compute the maximum blocks available for stash based on /cache size and
-    # the threshold.
-    cache_size = common.OPTIONS.cache_size
-    stash_threshold = common.OPTIONS.stash_threshold
-    max_allowed = cache_size * stash_threshold / self.tgt.blocksize
-
-    stashed_blocks = 0
-
-    # Now go through all the commands. Compute the required stash size on the
-    # fly. If a command requires excess stash than available, it deletes the
-    # stash by replacing the command that uses the stash with a "new" command
-    # instead.
-    for xf in self.transfers:
-      replaced_cmds = []
-
-      # xf.stash_before generates explicit stash commands.
-      for idx, sr in xf.stash_before:
-        if stashed_blocks + sr.size() > max_allowed:
-          # We cannot stash this one for a later command. Find out the command
-          # that will use this stash and replace the command with "new".
-          use_cmd = stashes[idx][2]
-          replaced_cmds.append(use_cmd)
-          print("  %s replaced due to an explicit stash of %d blocks." % (
-              use_cmd, sr.size()))
-        else:
-          stashed_blocks += sr.size()
-
-      # xf.use_stash generates free commands.
-      for _, sr in xf.use_stash:
-        stashed_blocks -= sr.size()
-
-      # "move" and "diff" may introduce implicit stashes in BBOTA v3. Prior to
-      # ComputePatches(), they both have the style of "diff".
-      if xf.style == "diff" and self.version >= 3:
-        assert xf.tgt_ranges and xf.src_ranges
-        if xf.src_ranges.overlaps(xf.tgt_ranges):
-          if stashed_blocks + xf.src_ranges.size() > max_allowed:
-            replaced_cmds.append(xf)
-            print("  %s replaced due to an implicit stash of %d blocks." % (
-                xf, xf.src_ranges.size()))
-
-      # Replace the commands in replaced_cmds with "new"s.
-      for cmd in replaced_cmds:
-        # It no longer uses any commands in "use_stash". Remove the def points
-        # for all those stashes.
-        for idx, sr in cmd.use_stash:
-          def_cmd = stashes[idx][1]
-          assert (idx, sr) in def_cmd.stash_before
-          def_cmd.stash_before.remove((idx, sr))
-
-        cmd.ConvertToNew()
-
   def ComputePatches(self, prefix):
     print("Reticulating splines...")
     diff_q = []
     patch_num = 0
-
-    if lzma and self.use_lzma:
-        open_patch = lzma.open
-        new_file = ".new.dat.xz"
-    else:
-        open_patch = open
-        new_file = ".new.dat"
-
-    with open_patch(prefix + new_file, "wb") as new_f:
+    with open(prefix + ".new.dat", "wb") as new_f:
       for xf in self.transfers:
         if xf.style == "zero":
           pass
@@ -697,13 +409,7 @@ def AssertSequenceGood(self):
     # Imagine processing the transfers in order.
     for xf in self.transfers:
       # Check that the input blocks for this transfer haven't yet been touched.
-
-      x = xf.src_ranges
-      if self.version >= 2:
-        for _, sr in xf.use_stash:
-          x = x.subtract(sr)
-
-      assert not touched.overlaps(x)
+      assert not touched.overlaps(xf.src_ranges)
       # Check that the output blocks for this transfer haven't yet been touched.
       assert not touched.overlaps(xf.tgt_ranges)
       # Touch all the blocks written by this transfer.
@@ -712,47 +418,6 @@ def AssertSequenceGood(self):
     # Check that we've written every target block.
     assert touched == self.tgt.care_map
 
-  def ImproveVertexSequence(self):
-    print("Improving vertex order...")
-
-    # At this point our digraph is acyclic; we reversed any edges that
-    # were backwards in the heuristically-generated sequence.  The
-    # previously-generated order is still acceptable, but we hope to
-    # find a better order that needs less memory for stashed data.
-    # Now we do a topological sort to generate a new vertex order,
-    # using a greedy algorithm to choose which vertex goes next
-    # whenever we have a choice.
-
-    # Make a copy of the edge set; this copy will get destroyed by the
-    # algorithm.
-    for xf in self.transfers:
-      xf.incoming = xf.goes_after.copy()
-      xf.outgoing = xf.goes_before.copy()
-
-    L = []   # the new vertex order
-
-    # S is the set of sources in the remaining graph; we always choose
-    # the one that leaves the least amount of stashed data after it's
-    # executed.
-    S = [(u.NetStashChange(), u.order, u) for u in self.transfers
-         if not u.incoming]
-    heapq.heapify(S)
-
-    while S:
-      _, _, xf = heapq.heappop(S)
-      L.append(xf)
-      for u in xf.outgoing:
-        del u.incoming[xf]
-        if not u.incoming:
-          heapq.heappush(S, (u.NetStashChange(), u.order, u))
-
-    # if this fails then our graph had a cycle.
-    assert len(L) == len(self.transfers)
-
-    self.transfers = L
-    for i, xf in enumerate(L):
-      xf.order = i
-
   def RemoveBackwardEdges(self):
     print("Removing backward edges...")
     in_order = 0
@@ -760,17 +425,19 @@ def RemoveBackwardEdges(self):
     lost_source = 0
 
     for xf in self.transfers:
+      io = 0
+      ooo = 0
       lost = 0
       size = xf.src_ranges.size()
       for u in xf.goes_before:
         # xf should go before u
         if xf.order < u.order:
           # it does, hurray!
-          in_order += 1
+          io += 1
         else:
           # it doesn't, boo.  trim the blocks that u writes from xf's
           # source, so that xf can go after u.
-          out_of_order += 1
+          ooo += 1
           assert xf.src_ranges.overlaps(u.tgt_ranges)
           xf.src_ranges = xf.src_ranges.subtract(u.tgt_ranges)
           xf.intact = False
@@ -781,6 +448,8 @@ def RemoveBackwardEdges(self):
 
       lost = size - xf.src_ranges.size()
       lost_source += lost
+      in_order += io
+      out_of_order += ooo
 
     print(("  %d/%d dependencies (%.2f%%) were violated; "
            "%d source blocks removed.") %
@@ -789,48 +458,6 @@ def RemoveBackwardEdges(self):
            if (in_order + out_of_order) else 0.0,
            lost_source))
 
-  def ReverseBackwardEdges(self):
-    print("Reversing backward edges...")
-    in_order = 0
-    out_of_order = 0
-    stashes = 0
-    stash_size = 0
-
-    for xf in self.transfers:
-      lost = 0
-      size = xf.src_ranges.size()
-      for u in xf.goes_before.copy():
-        # xf should go before u
-        if xf.order < u.order:
-          # it does, hurray!
-          in_order += 1
-        else:
-          # it doesn't, boo.  modify u to stash the blocks that it
-          # writes that xf wants to read, and then require u to go
-          # before xf.
-          out_of_order += 1
-
-          overlap = xf.src_ranges.intersect(u.tgt_ranges)
-          assert overlap
-
-          u.stash_before.append((stashes, overlap))
-          xf.use_stash.append((stashes, overlap))
-          stashes += 1
-          stash_size += overlap.size()
-
-          # reverse the edge direction; now xf must go after u
-          del xf.goes_before[u]
-          del u.goes_after[xf]
-          xf.goes_after[u] = None    # value doesn't matter
-          u.goes_before[xf] = None
-
-    print(("  %d/%d dependencies (%.2f%%) were violated; "
-           "%d source blocks stashed.") %
-          (out_of_order, in_order + out_of_order,
-           (out_of_order * 100.0 / (in_order + out_of_order))
-           if (in_order + out_of_order) else 0.0,
-           stash_size))
-
   def FindVertexSequence(self):
     print("Finding vertex sequence...")
 
diff --git a/tools/releasetools/build_image.py b/tools/releasetools/build_image.py
index 9a57b7a..4c1efe1 100644
--- a/tools/releasetools/build_image.py
+++ b/tools/releasetools/build_image.py
@@ -160,7 +160,7 @@ def MakeVerityEnabledImage(out_file, prop_dict):
   # get properties
   image_size = prop_dict["partition_size"]
   block_dev = prop_dict["verity_block_device"]
-  signer_key = prop_dict["verity_key"] + ".pk8"
+  signer_key = prop_dict["verity_key"]
   signer_path = prop_dict["verity_signer_cmd"]
 
   # make a tempdir
@@ -237,23 +237,15 @@ def BuildImage(in_dir, prop_dict, out_file,
     if "extfs_sparse_flag" in prop_dict:
       build_command.append(prop_dict["extfs_sparse_flag"])
       #run_fsck = True
-    if "is_userdataextra" in prop_dict:
-      build_command.extend([in_dir, out_file, fs_type,
-                           "data"])
-    else:
-      build_command.extend([in_dir, out_file, fs_type,
-                            prop_dict["mount_point"]])
+    build_command.extend([in_dir, out_file, fs_type,
+                          prop_dict["mount_point"]])
     build_command.append(prop_dict["partition_size"])
-    if "journal_size" in prop_dict:
-      build_command.extend(["-j", prop_dict["journal_size"]])
     if "timestamp" in prop_dict:
       build_command.extend(["-T", str(prop_dict["timestamp"])])
     if fs_config is not None:
       build_command.extend(["-C", fs_config])
     if block_list is not None:
       build_command.extend(["-B", block_list])
-    if "transparent_compression_method" in prop_dict:
-      build_command.extend(["-M", prop_dict["transparent_compression_method"]])
     if fc_config is not None:
       build_command.append(fc_config)
     elif "selinux_fc" in prop_dict:
@@ -318,8 +310,7 @@ def copy_prop(src_p, dest_p):
       "skip_fsck",
       "verity",
       "verity_key",
-      "verity_signer_cmd",
-      "transparent_compression_method"
+      "verity_signer_cmd"
       )
   for p in common_props:
     copy_prop(p, p)
@@ -327,32 +318,23 @@ def copy_prop(src_p, dest_p):
   d["mount_point"] = mount_point
   if mount_point == "system":
     copy_prop("fs_type", "fs_type")
-    copy_prop("system_fs_type", "fs_type")
     copy_prop("system_size", "partition_size")
-    copy_prop("system_journal_size", "journal_size")
     copy_prop("system_verity_block_device", "verity_block_device")
   elif mount_point == "data":
     # Copy the generic fs type first, override with specific one if available.
     copy_prop("fs_type", "fs_type")
     copy_prop("userdata_fs_type", "fs_type")
     copy_prop("userdata_size", "partition_size")
-  elif mount_point == "data_extra":
-    copy_prop("fs_type", "fs_type")
-    copy_prop("userdataextra_size", "partition_size")
-    copy_prop("userdataextra_name", "partition_name")
-    d["is_userdataextra"] = True
   elif mount_point == "cache":
     copy_prop("cache_fs_type", "fs_type")
     copy_prop("cache_size", "partition_size")
   elif mount_point == "vendor":
     copy_prop("vendor_fs_type", "fs_type")
     copy_prop("vendor_size", "partition_size")
-    copy_prop("vendor_journal_size", "journal_size")
     copy_prop("vendor_verity_block_device", "verity_block_device")
   elif mount_point == "oem":
     copy_prop("fs_type", "fs_type")
     copy_prop("oem_size", "partition_size")
-    copy_prop("oem_journal_size", "journal_size")
 
   return d
 
diff --git a/tools/releasetools/common.py b/tools/releasetools/common.py
index 499e8c2..7908b70 100644
--- a/tools/releasetools/common.py
+++ b/tools/releasetools/common.py
@@ -29,11 +29,6 @@
 import time
 import zipfile
 
-try:
-  from backports import lzma;
-except ImportError:
-  lzma = None
-
 import blockimgdiff
 from rangelib import *
 
@@ -67,10 +62,6 @@ class Options(object): pass
 OPTIONS.extras = {}
 OPTIONS.info_dict = None
 
-# Stash size cannot exceed cache_size * threshold.
-OPTIONS.cache_size = None
-OPTIONS.stash_threshold = 0.8
-
 
 # Values for "certificate" in apkcerts that mean special things.
 SPECIAL_CERT_STRINGS = ("PRESIGNED", "EXTERNAL")
@@ -305,7 +296,6 @@ def BuildBootableImage(sourcedir, fs_config_file, info_dict=None):
 
   ramdisk_img = tempfile.NamedTemporaryFile()
   img = tempfile.NamedTemporaryFile()
-  bootimg_key = os.getenv("PRODUCT_PRIVATE_KEY", None)
 
   if os.access(fs_config_file, os.F_OK):
     cmd = ["mkbootfs", "-f", fs_config_file, os.path.join(sourcedir, "RAMDISK")]
@@ -365,16 +355,15 @@ def BuildBootableImage(sourcedir, fs_config_file, info_dict=None):
       cmd.append("--ramdisk_offset")
       cmd.append(open(fn).read().rstrip("\n"))
 
-    fn = os.path.join(sourcedir, "dt")
+    fn = os.path.join(sourcedir, "dt_args")
     if os.access(fn, os.F_OK):
       cmd.append("--dt")
-      cmd.append(fn)
+      cmd.append(open(fn).read().rstrip("\n"))
 
     fn = os.path.join(sourcedir, "pagesize")
     if os.access(fn, os.F_OK):
-      kernel_pagesize=open(fn).read().rstrip("\n")
       cmd.append("--pagesize")
-      cmd.append(kernel_pagesize)
+      cmd.append(open(fn).read().rstrip("\n"))
 
     args = info_dict.get("mkbootimg_args", None)
     if args and args.strip():
@@ -387,45 +376,9 @@ def BuildBootableImage(sourcedir, fs_config_file, info_dict=None):
   assert p.returncode == 0, "mkbootimg of %s image failed" % (
       os.path.basename(sourcedir),)
 
-  if bootimg_key and os.path.exists(bootimg_key) and kernel_pagesize > 0:
-    print "Signing bootable image..."
-    bootimg_key_passwords = {}
-    bootimg_key_passwords.update(PasswordManager().GetPasswords(bootimg_key.split()))
-    bootimg_key_password = bootimg_key_passwords[bootimg_key]
-    if bootimg_key_password is not None:
-        bootimg_key_password += "\n"
-    img_sha256 = tempfile.NamedTemporaryFile()
-    img_sig = tempfile.NamedTemporaryFile()
-    img_sig_padded = tempfile.NamedTemporaryFile()
-    img_secure = tempfile.NamedTemporaryFile()
-    p = Run(["openssl", "dgst", "-sha256", "-binary", "-out", img_sha256.name, img.name],
-        stdout=subprocess.PIPE)
-    p.communicate()
-    assert p.returncode == 0, "signing of bootable image failed"
-    p = Run(["openssl", "rsautl", "-sign", "-in", img_sha256.name, "-inkey", bootimg_key, "-out",
-        img_sig.name, "-passin", "stdin"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
-    p.communicate(bootimg_key_password)
-    assert p.returncode == 0, "signing of bootable image failed"
-    p = Run(["dd", "if=/dev/zero", "of=%s" % img_sig_padded.name, "bs=%s" % kernel_pagesize,
-        "count=1"], stdout=subprocess.PIPE)
-    p.communicate()
-    assert p.returncode == 0, "signing of bootable image failed"
-    p = Run(["dd", "if=%s" % img_sig.name, "of=%s" % img_sig_padded.name, "conv=notrunc"],
-        stdout=subprocess.PIPE)
-    p.communicate()
-    assert p.returncode == 0, "signing of bootable image failed"
-    p = Run(["cat", img.name, img_sig_padded.name], stdout=img_secure.file.fileno())
-    p.communicate()
-    assert p.returncode == 0, "signing of bootable image failed"
-    shutil.copyfile(img_secure.name, img.name)
-    img_sha256.close()
-    img_sig.close()
-    img_sig_padded.close()
-    img_secure.close()
-
   if info_dict.get("verity_key", None):
     path = "/" + os.path.basename(sourcedir).lower()
-    cmd = ["boot_signer", path, img.name, info_dict["verity_key"] + ".pk8", info_dict["verity_key"] + ".x509.pem", img.name]
+    cmd = ["boot_signer", path, img.name, info_dict["verity_key"], img.name]
     p = Run(cmd, stdout=subprocess.PIPE)
     p.communicate()
     assert p.returncode == 0, "boot_signer of %s image failed" % path
@@ -481,7 +434,7 @@ def UnzipTemp(filename, pattern=None):
   OPTIONS.tempfiles.append(tmp)
 
   def unzip_to_dir(filename, dirname):
-    subprocess.call(["rm", "-rf", dirname + filename, "targetfiles-*"])
+    cmd = ["rm", "-rf", dirname + filename, "targetfiles-*"]
     cmd = ["unzip", "-o", "-q", filename, "-d", dirname]
     if pattern is not None:
       cmd.append(pattern)
@@ -609,7 +562,6 @@ def CheckSize(data, target, info_dict):
   fs_type = None
   limit = None
   if info_dict["fstab"]:
-    if mount_point == "/userdata_extra": mount_point = "/data"
     if mount_point == "/userdata": mount_point = "/data"
     p = info_dict["fstab"][mount_point]
     fs_type = p.fs_type
@@ -1109,23 +1061,13 @@ def worker():
 
 
 class BlockDifference:
-  def __init__(self, partition, tgt, src=None, check_first_block=False, version=None, use_lzma=False):
+  def __init__(self, partition, tgt, src=None, check_first_block=False):
     self.tgt = tgt
     self.src = src
     self.partition = partition
     self.check_first_block = check_first_block
-    self.use_lzma = use_lzma
-
-    if version is None:
-      version = 1
-      if OPTIONS.info_dict:
-        version = max(
-            int(i) for i in
-            OPTIONS.info_dict.get("blockimgdiff_versions", "1").split(","))
-    self.version = version
-
-    b = blockimgdiff.BlockImageDiff(tgt, src, threads=OPTIONS.worker_threads,
-                                    version=self.version, use_lzma=use_lzma)
+
+    b = blockimgdiff.BlockImageDiff(tgt, src, threads=OPTIONS.worker_threads)
     tmpdir = tempfile.mkdtemp()
     OPTIONS.tempfiles.append(tmpdir)
     self.path = os.path.join(tmpdir, partition)
@@ -1136,92 +1078,54 @@ def __init__(self, partition, tgt, src=None, check_first_block=False, version=No
   def WriteScript(self, script, output_zip, progress=None):
     if not self.src:
       # write the output unconditionally
-      script.Print("Patching %s image unconditionally..." % (self.partition,))
-    else:
-      script.Print("Patching %s image after verification." % (self.partition,))
-
-    if progress: script.ShowProgress(progress, 0)
-    self._WriteUpdate(script, output_zip)
+      if progress: script.ShowProgress(progress, 0)
+      self._WriteUpdate(script, output_zip)
 
-  def WriteVerifyScript(self, script):
-    partition = self.partition
-    if not self.src:
-      script.Print("Image %s will be patched unconditionally." % (partition,))
     else:
-      if self.version >= 3:
-        script.AppendExtra(('if (range_sha1("%s", "%s") == "%s" || '
-                            'block_image_verify("%s", '
-                            'package_extract_file("%s.transfer.list"), '
-                            '"%s.new.dat", "%s.patch.dat")) then') % (
-                            self.device, self.src.care_map.to_string_raw(),
-                            self.src.TotalSha1(),
-                            self.device, partition, partition, partition))
-      else:
-        script.AppendExtra('if range_sha1("%s", "%s") == "%s" then' %
-                            (self.device, self.src.care_map.to_string_raw(),
-                            self.src.TotalSha1()))
-      script.Print('Verified %s image...' % (partition,))
-      script.AppendExtra('else');
-
-      # When generating incrementals for the system and vendor partitions,
-      # explicitly check the first block (which contains the superblock) of
-      # the partition to see if it's what we expect. If this check fails,
-      # give an explicit log message about the partition having been
-      # remounted R/W (the most likely explanation) and the need to flash to
-      # get OTAs working again.
       if self.check_first_block:
         self._CheckFirstBlock(script)
 
-      # Abort the OTA update. Note that the incremental OTA cannot be applied
-      # even if it may match the checksum of the target partition.
-      # a) If version < 3, operations like move and erase will make changes
-      #    unconditionally and damage the partition.
-      # b) If version >= 3, it won't even reach here.
-      script.AppendExtra(('abort("%s partition has unexpected contents");\n'
-                          'endif;') % (partition,))
+      script.AppendExtra('if range_sha1("%s", "%s") == "%s" then' %
+                         (self.device, self.src.care_map.to_string_raw(),
+                          self.src.TotalSha1()))
+      script.Print("Patching %s image..." % (self.partition,))
+      if progress: script.ShowProgress(progress, 0)
+      self._WriteUpdate(script, output_zip)
+      script.AppendExtra(('else\n'
+                          '  (range_sha1("%s", "%s") == "%s") ||\n'
+                          '  abort("%s partition has unexpected contents");\n'
+                          'endif;') %
+                         (self.device, self.tgt.care_map.to_string_raw(),
+                          self.tgt.TotalSha1(), self.partition))
 
   def _WriteUpdate(self, script, output_zip):
     partition = self.partition
-    suffix = ".new.dat"
-
     with open(self.path + ".transfer.list", "rb") as f:
       ZipWriteStr(output_zip, partition + ".transfer.list", f.read())
-    if lzma and self.use_lzma:
-      suffix += ".xz"
-      with open(self.path + suffix, "rb") as f:
-        ZipWriteStr(output_zip, partition + suffix, f.read(),
-                           compression=zipfile.ZIP_STORED)
-    else:
-      with open(self.path + suffix, "rb") as f:
-        ZipWriteStr(output_zip, partition + suffix, f.read())
+    with open(self.path + ".new.dat", "rb") as f:
+      ZipWriteStr(output_zip, partition + ".new.dat", f.read())
     with open(self.path + ".patch.dat", "rb") as f:
       ZipWriteStr(output_zip, partition + ".patch.dat", f.read(),
                          compression=zipfile.ZIP_STORED)
 
     call = (('block_image_update("%s", '
              'package_extract_file("%s.transfer.list"), '
-             '"%s%s", "%s.patch.dat");\n') %
-            (self.device, partition, partition, suffix, partition))
+             '"%s.new.dat", "%s.patch.dat");\n') %
+            (self.device, partition, partition, partition))
     script.AppendExtra(script._WordWrap(call))
 
-  def _HashBlocks(self, source, ranges):
-    data = source.ReadRangeSet(ranges)
-    ctx = sha1()
-
-    for p in data:
-      ctx.update(p)
-
-    return ctx.hexdigest()
-
   def _CheckFirstBlock(self, script):
     r = RangeSet((0, 1))
-    srchash = self._HashBlocks(self.src, r);
+    h = sha1()
+    for data in self.src.ReadRangeSet(r):
+      h.update(data)
+    h = h.hexdigest()
 
     script.AppendExtra(('(range_sha1("%s", "%s") == "%s") || '
                         'abort("%s has been remounted R/W; '
                         'reflash device to reenable OTA updates");')
-                       % (self.device, r.to_string_raw(), srchash,
-                          self.device))
+                       % (self.device, r.to_string_raw(), h, self.device))
+
 
 DataImage = blockimgdiff.DataImage
 
diff --git a/tools/releasetools/edify_generator.py b/tools/releasetools/edify_generator.py
index 80cf7ce..1e262fe 100644
--- a/tools/releasetools/edify_generator.py
+++ b/tools/releasetools/edify_generator.py
@@ -147,12 +147,28 @@ def AssertSomeBaseband(self, *basebands):
     self.script.append(self._WordWrap(cmd))
 
   def RunBackup(self, command):
-    self.script.append(('run_program("/tmp/install/bin/backuptool.sh", "%s");' % command))
+    self.script.append('package_extract_file("system/bin/backuptool.sh", "/tmp/backuptool.sh");')
+    self.script.append('package_extract_file("system/bin/backuptool.functions", "/tmp/backuptool.functions");')
+    if not self.info.get("use_set_metadata", False):
+      self.script.append('set_perm(0, 0, 0755, "/tmp/backuptool.sh");')
+      self.script.append('set_perm(0, 0, 0644, "/tmp/backuptool.functions");')
+    else:
+      self.script.append('set_metadata("/tmp/backuptool.sh", "uid", 0, "gid", 0, "mode", 0755);')
+      self.script.append('set_metadata("/tmp/backuptool.functions", "uid", 0, "gid", 0, "mode", 0644);')
+    self.script.append(('run_program("/tmp/backuptool.sh", "%s");' % command))
+    if command == "restore":
+        self.script.append('delete("/system/bin/backuptool.sh");')
+        self.script.append('delete("/system/bin/backuptool.functions");')
 
   def ValidateSignatures(self, command):
-    self.script.append('package_extract_file("META-INF/org/cyanogenmod/releasekey", "/tmp/releasekey");')
-    # Exit code 124 == abort. run_program returns raw, so left-shift 8bit
-    self.script.append('run_program("/tmp/install/bin/otasigcheck.sh") != "31744" || abort("Can\'t install this package on top of incompatible data. Please try another package or run a factory reset");')
+    if command == "cleanup":
+        self.script.append('delete("/system/bin/otasigcheck.sh");')
+    else:
+        self.script.append('package_extract_file("system/bin/otasigcheck.sh", "/tmp/otasigcheck.sh");')
+        self.script.append('package_extract_file("META-INF/org/cyanogenmod/releasekey", "/tmp/releasekey");')
+        self.script.append('set_metadata("/tmp/otasigcheck.sh", "uid", 0, "gid", 0, "mode", 0755);')
+        # Exit code 124 == abort. run_program returns raw, so left-shift 8bit
+        self.script.append('run_program("/tmp/otasigcheck.sh") != "31744" || abort("Can\'t install this package on top of incompatible data. Please try another package or run a factory reset");')
 
   def ShowProgress(self, frac, dur):
     """Update the progress bar, advancing it over 'frac' over the next
@@ -231,17 +247,6 @@ def Print(self, message):
     """Log a message to the screen (if the logs are visible)."""
     self.script.append('ui_print("%s");' % (message,))
 
-  def TunePartition(self, partition, *options):
-    fstab = self.info.get("fstab", None)
-    if fstab:
-      p = fstab[partition]
-      if (p.fs_type not in ( "ext2", "ext3", "ext4")):
-        raise ValueError("Partition %s cannot be tuned\n" % (partition,))
-    self.script.append('tune2fs(' +
-                       "".join(['"%s", ' % (i,) for i in options]) +
-                       '"%s") || abort("Failed to tune partition %s");'
-                       % ( p.device,partition));
-
   def FormatPartition(self, partition):
     """Format the given partition, specified by its mount point (eg,
     "/system")."""
@@ -269,15 +274,6 @@ def DeleteFiles(self, file_list):
     cmd = "delete(" + ",\0".join(['"%s"' % (i,) for i in file_list]) + ");"
     self.script.append(self._WordWrap(cmd))
 
-  def DeleteFilesIfNotMatching(self, file_list):
-    """Delete the file in file_list if not matching the checksum."""
-    if not file_list:
-      return
-    for name, sha1 in file_list:
-      cmd = ('sha1_check(read_file("{name}"), "{sha1}") || '
-             'delete("{name}");'.format(name=name, sha1=sha1))
-      self.script.append(self._WordWrap(cmd))
-
   def RenameFile(self, srcfile, tgtfile):
     """Moves a file from one location to another."""
     if self.info.get("update_rename_support", False):
@@ -289,7 +285,7 @@ def SkipNextActionIfTargetExists(self, tgtfile, tgtsha1):
     """Prepend an action with an apply_patch_check in order to
        skip the action if the file exists.  Used when a patch
        is later renamed."""
-    cmd = ('sha1_check(read_file("%s"), %s) ||' % (tgtfile, tgtsha1))
+    cmd = ('sha1_check(read_file("%s"), %s) || ' % (tgtfile, tgtsha1))
     self.script.append(self._WordWrap(cmd))
 
   def ApplyPatch(self, srcfile, tgtfile, tgtsize, tgtsha1, *patchpairs):
@@ -335,10 +331,9 @@ def SetPermissions(self, fn, uid, gid, mode, selabel, capabilities):
     if not self.info.get("use_set_metadata", False):
       self.script.append('set_perm(%d, %d, 0%o, "%s");' % (uid, gid, mode, fn))
     else:
-      cmd = 'set_metadata("%s", "uid", %d, "gid", %d, "mode", 0%o' \
-          % (fn, uid, gid, mode)
-      if capabilities is not None:
-        cmd += ', "capabilities", %s' % ( capabilities )
+      if capabilities is None: capabilities = "0x0"
+      cmd = 'set_metadata("%s", "uid", %d, "gid", %d, "mode", 0%o, ' \
+          '"capabilities", %s' % (fn, uid, gid, mode, capabilities)
       if selabel is not None:
         cmd += ', "selabel", "%s"' % ( selabel )
       cmd += ');'
@@ -350,11 +345,10 @@ def SetPermissionsRecursive(self, fn, uid, gid, dmode, fmode, selabel, capabilit
       self.script.append('set_perm_recursive(%d, %d, 0%o, 0%o, "%s");'
                          % (uid, gid, dmode, fmode, fn))
     else:
+      if capabilities is None: capabilities = "0x0"
       cmd = 'set_metadata_recursive("%s", "uid", %d, "gid", %d, ' \
-          '"dmode", 0%o, "fmode", 0%o' \
-          % (fn, uid, gid, dmode, fmode)
-      if capabilities is not None:
-        cmd += ', "capabilities", "%s"' % ( capabilities )
+          '"dmode", 0%o, "fmode", 0%o, "capabilities", %s' \
+          % (fn, uid, gid, dmode, fmode, capabilities)
       if selabel is not None:
         cmd += ', "selabel", "%s"' % ( selabel )
       cmd += ');'
diff --git a/tools/releasetools/img_from_target_files.py b/tools/releasetools/img_from_target_files.py
index eef4c97..4b88e73 100644
--- a/tools/releasetools/img_from_target_files.py
+++ b/tools/releasetools/img_from_target_files.py
@@ -54,31 +54,6 @@ def CopyInfo(output_zip):
   output_zip.write(os.path.join(OPTIONS.input_tmp, "OTA", "android-info.txt"),
                    "android-info.txt")
 
-def AddRadio(output_zip):
-  """If they exist, add RADIO files to the output."""
-  if os.path.isdir(os.path.join(OPTIONS.input_tmp, "RADIO")):
-    for radio_root, radio_dirs, radio_files in os.walk(os.path.join(OPTIONS.input_tmp, "RADIO")):
-      for radio_file in radio_files:
-        output_zip.write(os.path.join(radio_root, radio_file), radio_file)
-
-    # If a filesmap file exists, create a script to flash the radio images based on it
-    filesmap = os.path.join(OPTIONS.input_tmp, "RADIO/filesmap")
-    if os.path.isfile(filesmap):
-      print "creating flash-radio.sh..."
-      filesmap_data = open(filesmap, "r")
-      filesmap_regex = re.compile(r'^(\S+)\s\S+\/by-name\/(\S+).*')
-      tmp_flash_radio = tempfile.NamedTemporaryFile()
-      tmp_flash_radio.write("#!/bin/sh\n\n")
-      for filesmap_line in filesmap_data:
-        filesmap_entry = filesmap_regex.search(filesmap_line)
-        if filesmap_entry:
-          tmp_flash_radio.write("fastboot flash %s %s\n" % (filesmap_entry.group(2), filesmap_entry.group(1)))
-      tmp_flash_radio.flush()
-      if os.path.getsize(tmp_flash_radio.name) > 0:
-        output_zip.write(tmp_flash_radio.name, "flash-radio.sh")
-      else:
-        print "flash-radio.sh is empty, skipping..."
-      tmp_flash_radio.close()
 
 def main(argv):
   bootable_only = [False]
@@ -104,7 +79,6 @@ def option_handler(o, a):
   OPTIONS.input_tmp, input_zip = common.UnzipTemp(args[0])
   output_zip = zipfile.ZipFile(args[1], "w", compression=zipfile.ZIP_DEFLATED)
   CopyInfo(output_zip)
-  AddRadio(output_zip)
 
   try:
     done = False
@@ -112,13 +86,11 @@ def option_handler(o, a):
     if os.path.exists(images_path):
       # If this is a new target-files, it already contains the images,
       # and all we have to do is copy them to the output zip.
-      # Skip oem.img files since they are not needed in fastboot images.
       images = os.listdir(images_path)
       if images:
         for i in images:
           if bootable_only and i not in ("boot.img", "recovery.img"): continue
           if not i.endswith(".img"): continue
-          if i == "oem.img": continue
           with open(os.path.join(images_path, i), "r") as f:
             common.ZipWriteStr(output_zip, i, f.read())
         done = True
@@ -162,8 +134,6 @@ def banner(s):
           pass   # no vendor partition for this device
         banner("AddUserdata")
         add_img_to_target_files.AddUserdata(output_zip, prefix="")
-        banner("AddUserdataExtra")
-        add_img_to_target_files.AddUserdataExtra(output_zip, prefix="")
         banner("AddCache")
         add_img_to_target_files.AddCache(output_zip, prefix="")
 
diff --git a/tools/releasetools/ota_from_target_files b/tools/releasetools/ota_from_target_files
index 011fe53..4e17483 100644
--- a/tools/releasetools/ota_from_target_files
+++ b/tools/releasetools/ota_from_target_files
@@ -70,10 +70,6 @@ Usage:  ota_from_target_files [flags] input_target_files output_ota_package
       file-based OTA if the target_files is older and doesn't support
       block-based OTAs.
 
-  -z  Compress the block-based image using LZMA. Results in substantial
-      space reduction at the cost of longer compress/decompress time.
-      Requires the "backports.lzma" module to be installed.
-
   -b  (--binary)  <file>
       Use the given binary as the update-binary in the output package,
       instead of the binary in the build's target_files.  Use for
@@ -83,10 +79,6 @@ Usage:  ota_from_target_files [flags] input_target_files output_ota_package
       Specifies the number of worker-threads that will be used when
       generating patches for incremental updates (defaults to 3).
 
-  --stash_threshold <float>
-      Specifies the threshold that will be used to compute the maximum
-      allowed stash size (defaults to 0.8).
-
   --backup <boolean>
       Enable or disable the execution of backuptool.sh.
       Disabled by default.
@@ -147,7 +139,6 @@ OPTIONS.fallback_to_full = True
 OPTIONS.backuptool = False
 OPTIONS.override_device = 'auto'
 OPTIONS.override_prop = False
-OPTIONS.use_lzma = False
 
 def MostPopularKey(d, default):
   """Given a dict, return the key corresponding to the largest
@@ -485,15 +476,6 @@ def GetImage(which, tmpdir, info_dict):
   return sparse_img.SparseImage(path, mappath)
 
 
-def CopyInstallTools(output_zip):
-  install_path = os.path.join(OPTIONS.input_tmp, "INSTALL")
-  for root, subdirs, files in os.walk(install_path):
-    for f in files:
-      install_source = os.path.join(root, f)
-      install_target = os.path.join("install", os.path.relpath(root, install_path), f)
-      output_zip.write(install_source, install_target)
-
-
 def WriteFullOTAPackage(input_zip, output_zip):
   # TODO: how to determine this?  We don't know what version it will
   # be installed on top of.  For now, we expect the API just won't
@@ -585,12 +567,12 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
   script.AppendExtra("ifelse(is_mounted(\"/system\"), unmount(\"/system\"));")
   device_specific.FullOTA_InstallBegin()
 
-  CopyInstallTools(output_zip)
-  script.UnpackPackageDir("install", "/tmp/install")
-  script.SetPermissionsRecursive("/tmp/install", 0, 0, 0755, 0644, None, None)
-  script.SetPermissionsRecursive("/tmp/install/bin", 0, 0, 0755, 0755, None, None)
-
   if OPTIONS.backuptool:
+    if block_based:
+      common.ZipWriteStr(output_zip, "system/bin/backuptool.sh",
+                     ""+input_zip.read("SYSTEM/bin/backuptool.sh"))
+      common.ZipWriteStr(output_zip, "system/bin/backuptool.functions",
+                     ""+input_zip.read("SYSTEM/bin/backuptool.functions"))
     script.Mount("/system")
     script.RunBackup("backup")
     script.Unmount("/system")
@@ -602,14 +584,17 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
   if HasVendorPartition(input_zip):
     system_progress -= 0.1
 
-  if not OPTIONS.wipe_user_data:
-    script.AppendExtra("if is_mounted(\"/data\") then")
-    script.ValidateSignatures("data")
-    script.AppendExtra("else")
-    script.Mount("/data")
-    script.ValidateSignatures("data")
-    script.Unmount("/data")
-    script.AppendExtra("endif;")
+  if block_based:
+    common.ZipWriteStr(output_zip, "system/bin/otasigcheck.sh",
+                   ""+input_zip.read("SYSTEM/bin/otasigcheck.sh"))
+
+  script.AppendExtra("if is_mounted(\"/data\") then")
+  script.ValidateSignatures("data")
+  script.AppendExtra("else")
+  script.Mount("/data")
+  script.ValidateSignatures("data")
+  script.Unmount("/data")
+  script.AppendExtra("endif;")
 
   if "selinux_fc" in OPTIONS.info_dict:
     WritePolicyConfig(OPTIONS.info_dict["selinux_fc"], output_zip)
@@ -618,7 +603,6 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
 
   system_items = ItemSet("system", "META/filesystem_config.txt")
   script.ShowProgress(system_progress, 0)
-
   if block_based:
     # Full OTA is done as an "incremental" against an empty source
     # image.  This has the effect of writing new data from the package
@@ -626,7 +610,7 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
     # writes incrementals to do it.
     system_tgt = GetImage("system", OPTIONS.input_tmp, OPTIONS.info_dict)
     system_tgt.ResetFileMap()
-    system_diff = common.BlockDifference("system", system_tgt, src=None, use_lzma=OPTIONS.use_lzma)
+    system_diff = common.BlockDifference("system", system_tgt, src=None)
     system_diff.WriteScript(script, output_zip)
   else:
     script.FormatPartition("/system")
@@ -658,7 +642,7 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
     if block_based:
       vendor_tgt = GetImage("vendor", OPTIONS.input_tmp, OPTIONS.info_dict)
       vendor_tgt.ResetFileMap()
-      vendor_diff = common.BlockDifference("vendor", vendor_tgt, use_lzma=OPTIONS.use_lzma)
+      vendor_diff = common.BlockDifference("vendor", vendor_tgt)
       vendor_diff.WriteScript(script, output_zip)
     else:
       script.FormatPartition("/vendor")
@@ -674,6 +658,12 @@ else if get_stage("%(bcb_dev)s") == "3/3" then
   common.CheckSize(boot_img.data, "boot.img", OPTIONS.info_dict)
   common.ZipWriteStr(output_zip, "boot.img", boot_img.data)
 
+  if block_based:
+    script.Mount("/system")
+  script.ValidateSignatures("cleanup")
+  if block_based:
+    script.Unmount("/system")
+
   device_specific.FullOTA_PostValidate()
 
   if OPTIONS.backuptool:
@@ -714,9 +704,6 @@ endif;
   script.AddToZip(input_zip, output_zip, input_path=OPTIONS.updater_binary)
   WriteMetadata(metadata, output_zip)
 
-  common.ZipWriteStr(output_zip, "system/build.prop",
-                     ""+input_zip.read("SYSTEM/build.prop"))
-
   common.ZipWriteStr(output_zip, "META-INF/org/cyanogenmod/releasekey",
                      ""+input_zip.read("META/releasekey.txt"))
 
@@ -791,9 +778,7 @@ def WriteBlockIncrementalOTAPackage(target_zip, source_zip, output_zip):
       source_zip=source_zip,
       source_version=source_version,
       target_zip=target_zip,
-      input_zip=target_zip,
       target_version=target_version,
-      input_version=target_version,
       output_zip=output_zip,
       script=script,
       metadata=metadata,
@@ -821,17 +806,8 @@ def WriteBlockIncrementalOTAPackage(target_zip, source_zip, output_zip):
 
   system_src = GetImage("system", OPTIONS.source_tmp, OPTIONS.source_info_dict)
   system_tgt = GetImage("system", OPTIONS.target_tmp, OPTIONS.target_info_dict)
-
-  blockimgdiff_version = 1
-  if OPTIONS.info_dict:
-    blockimgdiff_version = max(
-        int(i) for i in
-        OPTIONS.info_dict.get("blockimgdiff_versions", "1").split(","))
-
   system_diff = common.BlockDifference("system", system_tgt, system_src,
-                                       check_first_block=True,
-                                       version=blockimgdiff_version,
-                                       use_lzma=OPTIONS.use_lzma)
+                                       check_first_block=True)
 
   if HasVendorPartition(target_zip):
     if not HasVendorPartition(source_zip):
@@ -839,9 +815,7 @@ def WriteBlockIncrementalOTAPackage(target_zip, source_zip, output_zip):
     vendor_src = GetImage("vendor", OPTIONS.source_tmp, OPTIONS.source_info_dict)
     vendor_tgt = GetImage("vendor", OPTIONS.target_tmp, OPTIONS.target_info_dict)
     vendor_diff = common.BlockDifference("vendor", vendor_tgt, vendor_src,
-                                         check_first_block=True,
-                                         version=blockimgdiff_version,
-                                         use_lzma=OPTIONS.use_lzma)
+                                         check_first_block=True)
   else:
     vendor_diff = None
 
@@ -902,22 +876,11 @@ else if get_stage("%(bcb_dev)s") != "3/3" then
   device_specific.IncrementalOTA_VerifyBegin()
 
   if oem_props is None:
-    # When blockimgdiff version is less than 3 (non-resumable block-based OTA),
-    # patching on a device that's already on the target build will damage the
-    # system. Because operations like move don't check the block state, they
-    # always apply the changes unconditionally.
-    if blockimgdiff_version <= 2:
-      script.AssertSomeFingerprint(source_fp)
-    else:
-      script.AssertSomeFingerprint(source_fp, target_fp)
+    script.AssertSomeFingerprint(source_fp, target_fp)
   else:
-    if blockimgdiff_version <= 2:
-      script.AssertSomeThumbprint(
-          GetBuildProp("ro.build.thumbprint", OPTIONS.source_info_dict))
-    else:
-      script.AssertSomeThumbprint(
-          GetBuildProp("ro.build.thumbprint", OPTIONS.target_info_dict),
-          GetBuildProp("ro.build.thumbprint", OPTIONS.source_info_dict))
+    script.AssertSomeThumbprint(
+        GetBuildProp("ro.build.thumbprint", OPTIONS.target_info_dict),
+        GetBuildProp("ro.build.thumbprint", OPTIONS.source_info_dict))
 
   if updating_boot:
     boot_type, boot_device = common.GetTypeAndDevice("/boot", OPTIONS.info_dict)
@@ -949,11 +912,6 @@ reboot_now("%(bcb_dev)s", "");
 else
 """ % bcb_dev)
 
-  # Verify the existing partitions.
-  system_diff.WriteVerifyScript(script)
-  if vendor_diff:
-    vendor_diff.WriteVerifyScript(script)
-
   script.Comment("---- start making changes here ----")
 
   device_specific.IncrementalOTA_InstallBegin()
@@ -1089,7 +1047,7 @@ class FileDifference:
     so_far = 0
     for tf, sf, size, patch_sha in self.patch_list:
       if tf.name != sf.name:
-        script.SkipNextActionIfTargetExists("/"+tf.name, tf.sha1)
+        script.SkipNextActionIfTargetExists(tf.name, tf.sha1)
       script.PatchCheck("/"+sf.name, tf.sha1, sf.sha1)
       so_far += sf.size
     return so_far
@@ -1102,13 +1060,11 @@ class FileDifference:
       script.FileCheck(tf.name, tf.sha1)
 
   def RemoveUnneededFiles(self, script, extras=()):
-    file_list = ["/" + i[0] for i in self.verbatim_targets]
-    file_list += ["/" + i for i in self.source_data
-                  if i not in self.target_data and i not in self.renames]
-    file_list += list(extras)
-    # Sort the list in descending order, which removes all the files first
-    # before attempting to remove the folder. (Bug: 22960996)
-    script.DeleteFiles(sorted(file_list, reverse=True))
+    script.DeleteFiles(["/"+i[0] for i in self.verbatim_targets] +
+                       ["/"+i for i in sorted(self.source_data)
+                              if i not in self.target_data and
+                              i not in self.renames] +
+                       list(extras))
 
   def TotalPatchSize(self):
     return sum(i[1].size for i in self.patch_list)
@@ -1121,7 +1077,7 @@ class FileDifference:
         deferred_patch_list.append(item)
         continue
       if (sf.name != tf.name):
-        script.SkipNextActionIfTargetExists("/"+tf.name, tf.sha1)
+        script.SkipNextActionIfTargetExists(tf.name, tf.sha1)
       script.ApplyPatch("/"+sf.name, "-", tf.size, tf.sha1, sf.sha1, "patch/"+sf.name+".p")
       so_far += tf.size
       script.SetProgress(so_far / total_patch_size)
@@ -1185,9 +1141,7 @@ def WriteIncrementalOTAPackage(target_zip, source_zip, output_zip):
       source_zip=source_zip,
       source_version=source_version,
       target_zip=target_zip,
-      input_zip=target_zip,
       target_version=target_version,
-      input_version=target_version,
       output_zip=output_zip,
       script=script,
       metadata=metadata,
@@ -1289,24 +1243,19 @@ else if get_stage("%(bcb_dev)s") != "3/3" then
     so_far += vendor_diff.EmitVerification(script)
 
   if updating_boot:
-    boot_type, boot_device = common.GetTypeAndDevice("/boot", OPTIONS.info_dict)
     d = common.Difference(target_boot, source_boot)
     _, _, d = d.ComputePatch()
-    if d is None:
-      include_full_boot = True
-      common.ZipWriteStr(output_zip, "boot.img", target_boot.data)
-    else:
-      include_full_boot = False
-
-      print "boot      target: %d  source: %d  diff: %d" % (
+    print "boot      target: %d  source: %d  diff: %d" % (
         target_boot.size, source_boot.size, len(d))
 
-      common.ZipWriteStr(output_zip, "patch/boot.img.p", d)
+    common.ZipWriteStr(output_zip, "patch/boot.img.p", d)
 
-      script.PatchCheck("%s:%s:%d:%s:%d:%s" %
-                        (boot_type, boot_device,
-                         source_boot.size, source_boot.sha1,
-                         target_boot.size, target_boot.sha1))
+    boot_type, boot_device = common.GetTypeAndDevice("/boot", OPTIONS.info_dict)
+
+    script.PatchCheck("%s:%s:%d:%s:%d:%s" %
+                      (boot_type, boot_device,
+                       source_boot.size, source_boot.sha1,
+                       target_boot.size, target_boot.sha1))
     so_far += source_boot.size
 
   size = []
@@ -1355,23 +1304,20 @@ else
 
   if not OPTIONS.two_step:
     if updating_boot:
-      if include_full_boot:
-        print "boot image changed; including full."
-        script.Print("Installing boot image...")
-        script.WriteRawImage("/boot", "boot.img")
-      else:
-        # Produce the boot image by applying a patch to the current
-        # contents of the boot partition, and write it back to the
-        # partition.
-        print "boot image changed; including patch."
-        script.Print("Patching boot image...")
-        script.ApplyPatch("%s:%s:%d:%s:%d:%s"
-                          % (boot_type, boot_device,
-                             source_boot.size, source_boot.sha1,
-                             target_boot.size, target_boot.sha1),
-                          "-",
-                          target_boot.size, target_boot.sha1,
-                          source_boot.sha1, "patch/boot.img.p")
+      # Produce the boot image by applying a patch to the current
+      # contents of the boot partition, and write it back to the
+      # partition.
+      script.Print("Patching boot image...")
+      script.ApplyPatch("%s:%s:%d:%s:%d:%s"
+                        % (boot_type, boot_device,
+                           source_boot.size, source_boot.sha1,
+                           target_boot.size, target_boot.sha1),
+                        "-",
+                        target_boot.size, target_boot.sha1,
+                        source_boot.sha1, "patch/boot.img.p")
+      so_far += target_boot.size
+      script.SetProgress(so_far / total_patch_size)
+      print "boot image changed; including."
     else:
       print "boot image unchanged; skipping."
 
@@ -1427,36 +1373,11 @@ else
   # Delete all the symlinks in source that aren't in target.  This
   # needs to happen before verbatim files are unpacked, in case a
   # symlink in the source is replaced by a real file in the target.
-
-  # If a symlink in the source will be replaced by a regular file, we cannot
-  # delete the symlink/file in case the package gets applied again. For such
-  # a symlink, we prepend a sha1_check() to detect if it has been updated.
-  # (Bug: 23646151)
-  replaced_symlinks = dict()
-  if system_diff:
-    for i in system_diff.verbatim_targets:
-      replaced_symlinks["/%s" % (i[0],)] = i[2]
-  if vendor_diff:
-    for i in vendor_diff.verbatim_targets:
-      replaced_symlinks["/%s" % (i[0],)] = i[2]
-
-  if system_diff:
-    for tf in system_diff.renames.values():
-      replaced_symlinks["/%s" % (tf.name,)] = tf.sha1
-  if vendor_diff:
-    for tf in vendor_diff.renames.values():
-      replaced_symlinks["/%s" % (tf.name,)] = tf.sha1
-
-  always_delete = []
-  may_delete = []
+  to_delete = []
   for dest, link in source_symlinks:
     if link not in target_symlinks_d:
-      if link in replaced_symlinks:
-        may_delete.append((link, replaced_symlinks[link]))
-      else:
-        always_delete.append(link)
-  script.DeleteFiles(always_delete)
-  script.DeleteFilesIfNotMatching(may_delete)
+      to_delete.append(link)
+  script.DeleteFiles(to_delete)
 
   if system_diff.verbatim_targets:
     script.Print("Unpacking new system files...")
@@ -1570,28 +1491,18 @@ def main(argv):
       OPTIONS.updater_binary = a
     elif o in ("--no_fallback_to_full",):
       OPTIONS.fallback_to_full = False
-    elif o == "--stash_threshold":
-      try:
-        OPTIONS.stash_threshold = float(a)
-      except ValueError:
-        raise ValueError("Cannot parse value %r for option %r - expecting "
-                         "a float" % (a, o))
     elif o in ("--backup"):
       OPTIONS.backuptool = bool(a.lower() == 'true')
     elif o in ("--override_device"):
       OPTIONS.override_device = a
     elif o in ("--override_prop"):
       OPTIONS.override_prop = bool(a.lower() == 'true')
-    elif o in ("-z", "--use_lzma"):
-      OPTIONS.use_lzma = True
-      # Import now, and bomb out if backports.lzma isn't installed
-      from backports import lzma
     else:
       return False
     return True
 
   args = common.ParseOptions(argv, __doc__,
-                             extra_opts="b:k:i:d:wne:t:a:2o:z",
+                             extra_opts="b:k:i:d:wne:t:a:2o:",
                              extra_long_opts=["board_config=",
                                               "package_key=",
                                               "incremental_from=",
@@ -1607,11 +1518,9 @@ def main(argv):
                                               "oem_settings=",
                                               "verify",
                                               "no_fallback_to_full",
-                                              "stash_threshold=",
                                               "backup=",
                                               "override_device=",
-                                              "override_prop=",
-                                              "use_lzma"],
+                                              "override_prop="],
                              extra_option_handler=option_handler)
 
   if len(args) != 2:
@@ -1667,11 +1576,6 @@ def main(argv):
       output_zip = zipfile.ZipFile(temp_zip_file, "w",
                                    compression=zipfile.ZIP_DEFLATED)
 
-    cache_size = OPTIONS.info_dict.get("cache_size", None)
-    if cache_size is None:
-      print "--- can't determine the cache partition size ---"
-    OPTIONS.cache_size = cache_size
-
     if OPTIONS.incremental_source is None:
       WriteFullOTAPackage(input_zip, output_zip)
       if OPTIONS.package_key is None:
diff --git a/tools/releasetools/rangelib.py b/tools/releasetools/rangelib.py
index 7279c60..8a85d2d 100644
--- a/tools/releasetools/rangelib.py
+++ b/tools/releasetools/rangelib.py
@@ -24,9 +24,7 @@ class RangeSet(object):
   lots of runs."""
 
   def __init__(self, data=None):
-    if isinstance(data, str):
-      self._parse_internal(data)
-    elif data:
+    if data:
       self.data = tuple(self._remove_pairs(data))
     else:
       self.data = ()
@@ -48,9 +46,6 @@ def __str__(self):
     else:
       return self.to_string()
 
-  def __repr__(self):
-    return '<RangeSet("' + self.to_string() + '")>'
-
   @classmethod
   def parse(cls, text):
     """Parse a text string consisting of a space-separated list of
@@ -64,9 +59,7 @@ def parse(cls, text):
     "15-20 30 10-14" is not, even though they represent the same set
     of blocks (and the two RangeSets will compare equal with ==).
     """
-    return cls(text)
 
-  def _parse_internal(self, text):
     data = []
     last = -1
     monotonic = True
@@ -88,8 +81,9 @@ def _parse_internal(self, text):
         else:
           monotonic = True
     data.sort()
-    self.data = tuple(self._remove_pairs(data))
-    self.monotonic = monotonic
+    r = RangeSet(cls._remove_pairs(data))
+    r.monotonic = monotonic
+    return r
 
   @staticmethod
   def _remove_pairs(source):
@@ -119,13 +113,7 @@ def to_string_raw(self):
 
   def union(self, other):
     """Return a new RangeSet representing the union of this RangeSet
-    with the argument.
-
-    >>> RangeSet("10-19 30-34").union(RangeSet("18-29"))
-    <RangeSet("10-34")>
-    >>> RangeSet("10-19 30-34").union(RangeSet("22 32"))
-    <RangeSet("10-19 22 30-34")>
-    """
+    with the argument."""
     out = []
     z = 0
     for p, d in heapq.merge(zip(self.data, itertools.cycle((+1, -1))),
@@ -137,13 +125,7 @@ def union(self, other):
 
   def intersect(self, other):
     """Return a new RangeSet representing the intersection of this
-    RangeSet with the argument.
-
-    >>> RangeSet("10-19 30-34").intersect(RangeSet("18-32"))
-    <RangeSet("18-19 30-32")>
-    >>> RangeSet("10-19 30-34").intersect(RangeSet("22-28"))
-    <RangeSet("")>
-    """
+    RangeSet with the argument."""
     out = []
     z = 0
     for p, d in heapq.merge(zip(self.data, itertools.cycle((+1, -1))),
@@ -155,13 +137,7 @@ def intersect(self, other):
 
   def subtract(self, other):
     """Return a new RangeSet representing subtracting the argument
-    from this RangeSet.
-
-    >>> RangeSet("10-19 30-34").subtract(RangeSet("18-32"))
-    <RangeSet("10-17 33-34")>
-    >>> RangeSet("10-19 30-34").subtract(RangeSet("22-28"))
-    <RangeSet("10-19 30-34")>
-    """
+    from this RangeSet."""
 
     out = []
     z = 0
@@ -174,13 +150,7 @@ def subtract(self, other):
 
   def overlaps(self, other):
     """Returns true if the argument has a nonempty overlap with this
-    RangeSet.
-
-    >>> RangeSet("10-19 30-34").overlaps(RangeSet("18-32"))
-    True
-    >>> RangeSet("10-19 30-34").overlaps(RangeSet("22-28"))
-    False
-    """
+    RangeSet."""
 
     # This is like intersect, but we can stop as soon as we discover the
     # output is going to be nonempty.
@@ -194,11 +164,7 @@ def overlaps(self, other):
 
   def size(self):
     """Returns the total size of the RangeSet (ie, how many integers
-    are in the set).
-
-    >>> RangeSet("10-19 30-34").size()
-    15
-    """
+    are in the set)."""
 
     total = 0
     for i, p in enumerate(self.data):
@@ -207,37 +173,3 @@ def size(self):
       else:
         total -= p
     return total
-
-  def map_within(self, other):
-    """'other' should be a subset of 'self'.  Returns a RangeSet
-    representing what 'other' would get translated to if the integers
-    of 'self' were translated down to be contiguous starting at zero.
-
-    >>> RangeSet("0-9").map_within(RangeSet("3-4"))
-    <RangeSet("3-4")>
-    >>> RangeSet("10-19").map_within(RangeSet("13-14"))
-    <RangeSet("3-4")>
-    >>> RangeSet("10-19 30-39").map_within(RangeSet("17-19 30-32"))
-    <RangeSet("7-12")>
-    >>> RangeSet("10-19 30-39").map_within(RangeSet("12-13 17-19 30-32"))
-    <RangeSet("2-3 7-12")>
-    """
-
-    out = []
-    offset = 0
-    start = None
-    for p, d in heapq.merge(zip(self.data, itertools.cycle((-5, +5))),
-                            zip(other.data, itertools.cycle((-1, +1)))):
-      if d == -5:
-        start = p
-      elif d == +5:
-        offset += p-start
-        start = None
-      else:
-        out.append(offset + p - start)
-    return RangeSet(data=out)
-
-
-if __name__ == "__main__":
-  import doctest
-  doctest.testmod()
diff --git a/tools/releasetools/sign_target_files_apks b/tools/releasetools/sign_target_files_apks
index 9c86686..b17130b 100644
--- a/tools/releasetools/sign_target_files_apks
+++ b/tools/releasetools/sign_target_files_apks
@@ -198,7 +198,6 @@ def ProcessTargetFiles(input_tf_zip, output_tf_zip, misc_info,
         print "NOT signing: %s" % (name,)
         output_tf_zip.writestr(out_info, data)
     elif info.filename in ("SYSTEM/build.prop",
-                           "VENDOR/build.prop",
                            "RECOVERY/RAMDISK/default.prop"):
       print "rewriting %s:" % (info.filename,)
       new_data = RewriteProps(data, misc_info)
@@ -296,12 +295,12 @@ def RewriteProps(data, misc_info):
     original_line = line
     if line and line[0] != '#' and "=" in line:
       key, value = line.split("=", 1)
-      if (key in ("ro.build.fingerprint", "ro.vendor.build.fingerprint")
+      if (key == "ro.build.fingerprint"
           and misc_info.get("oem_fingerprint_properties") is None):
         pieces = value.split("/")
         pieces[-1] = EditTags(pieces[-1])
         value = "/".join(pieces)
-      elif (key in ("ro.build.thumbprint", "ro.vendor.build.thumbprint")
+      elif (key == "ro.build.thumbprint"
           and misc_info.get("oem_fingerprint_properties") is not None):
         pieces = value.split("/")
         pieces[-1] = EditTags(pieces[-1])
diff --git a/tools/repopick.py b/tools/repopick.py
index ac8875a..3dbe751 100644
--- a/tools/repopick.py
+++ b/tools/repopick.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 #
-# Copyright (C) 2013-15 The CyanogenMod Project
+# Copyright (C) 2013-14 The CyanogenMod Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -28,328 +28,370 @@
 import re
 import argparse
 import textwrap
-from xml.etree import ElementTree
 
 try:
-    # For python3
-    import urllib.error
-    import urllib.request
+  # For python3
+  import urllib.request
 except ImportError:
-    # For python2
-    import imp
-    import urllib2
-    urllib = imp.new_module('urllib')
-    urllib.error = urllib2
-    urllib.request = urllib2
-
-
-# Verifies whether pathA is a subdirectory (or the same) as pathB
-def is_subdir(a, b):
-    a = os.path.realpath(a) + '/'
-    b = os.path.realpath(b) + '/'
-    return b == a[:len(b)]
-
-
-def fetch_query_via_ssh(remote_url, query):
-    """Given a remote_url and a query, return the list of changes that fit it
-       This function is slightly messy - the ssh api does not return data in the same structure as the HTTP REST API
-       We have to get the data, then transform it to match what we're expecting from the HTTP RESET API"""
-    if remote_url.count(':') == 2:
-        (uri, userhost, port) = remote_url.split(':')
-        userhost = userhost[2:]
-    elif remote_url.count(':') == 1:
-        (uri, userhost) = remote_url.split(':')
-        userhost = userhost[2:]
-        port = 29418
+  # For python2
+  import imp
+  import urllib2
+  urllib = imp.new_module('urllib')
+  urllib.request = urllib2
+
+# Parse the command line
+parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent('''\
+    repopick.py is a utility to simplify the process of cherry picking
+    patches from CyanogenMod's Gerrit instance.
+
+    Given a list of change numbers, repopick will cd into the project path
+    and cherry pick the latest patch available.
+
+    With the --start-branch argument, the user can specify that a branch
+    should be created before cherry picking. This is useful for
+    cherry-picking many patches into a common branch which can be easily
+    abandoned later (good for testing other's changes.)
+
+    The --abandon-first argument, when used in conjuction with the
+    --start-branch option, will cause repopick to abandon the specified
+    branch in all repos first before performing any cherry picks.'''))
+parser.add_argument('change_number', nargs='*', help='change number to cherry pick.  Use {change number}/{patchset number} to get a specific revision.')
+parser.add_argument('-i', '--ignore-missing', action='store_true', help='do not error out if a patch applies to a missing directory')
+parser.add_argument('-s', '--start-branch', nargs=1, help='start the specified branch before cherry picking')
+parser.add_argument('-a', '--abandon-first', action='store_true', help='before cherry picking, abandon the branch specified in --start-branch')
+parser.add_argument('-b', '--auto-branch', action='store_true', help='shortcut to "--start-branch auto --abandon-first --ignore-missing"')
+parser.add_argument('-q', '--quiet', action='store_true', help='print as little as possible')
+parser.add_argument('-v', '--verbose', action='store_true', help='print extra information to aid in debug')
+parser.add_argument('-f', '--force', action='store_true', help='force cherry pick even if commit has been merged')
+parser.add_argument('-p', '--pull', action='store_true', help='execute pull instead of cherry-pick')
+parser.add_argument('-t', '--topic', help='pick all commits from a specified topic')
+args = parser.parse_args()
+if args.start_branch == None and args.abandon_first:
+    parser.error('if --abandon-first is set, you must also give the branch name with --start-branch')
+if args.auto_branch:
+    args.abandon_first = True
+    args.ignore_missing = True
+    if not args.start_branch:
+        args.start_branch = ['auto']
+if args.quiet and args.verbose:
+    parser.error('--quiet and --verbose cannot be specified together')
+if len(args.change_number) > 0 and args.topic:
+    parser.error('cannot specify a topic and change number(s) together')
+if len(args.change_number) == 0 and not args.topic:
+    parser.error('must specify at least one commit id or a topic')
+
+# Helper function to determine whether a path is an executable file
+def is_exe(fpath):
+    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)
+
+# Implementation of Unix 'which' in Python
+#
+# From: http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python
+def which(program):
+    fpath, fname = os.path.split(program)
+    if fpath:
+        if is_exe(program):
+            return program
     else:
-        raise Exception('Malformed URI: Expecting ssh://[user@]host[:port]')
+        for path in os.environ["PATH"].split(os.pathsep):
+            path = path.strip('"')
+            exe_file = os.path.join(path, program)
+            if is_exe(exe_file):
+                return exe_file
+
+    return None
+
+# Simple wrapper for os.system() that:
+#   - exits on error
+#   - prints out the command if --verbose
+#   - suppresses all output if --quiet
+def execute_cmd(cmd):
+    if args.verbose:
+        print('Executing: %s' % cmd)
+    if args.quiet:
+        cmd = cmd.replace(' && ', ' &> /dev/null && ')
+        cmd = cmd + " &> /dev/null"
+    if os.system(cmd):
+        if not args.verbose:
+            print('\nCommand that failed:\n%s' % cmd)
+        sys.exit(1)
 
+# Verifies whether pathA is a subdirectory (or the same) as pathB
+def is_pathA_subdir_of_pathB(pathA, pathB):
+    pathA = os.path.realpath(pathA) + '/'
+    pathB = os.path.realpath(pathB) + '/'
+    return(pathB == pathA[:len(pathB)])
+
+# Find the necessary bins - repo
+repo_bin = which('repo')
+if repo_bin == None:
+    repo_bin = os.path.join(os.environ["HOME"], 'repo')
+    if not is_exe(repo_bin):
+        sys.stderr.write('ERROR: Could not find the repo program in either $PATH or $HOME/bin\n')
+        sys.exit(1)
 
-    out = subprocess.check_output(['ssh', '-x', '-p{0}'.format(port), userhost, 'gerrit', 'query', '--format=JSON --patch-sets --current-patch-set', query])
+# Find the necessary bins - git
+git_bin = which('git')
+if not is_exe(git_bin):
+    sys.stderr.write('ERROR: Could not find the git program in $PATH\n')
+    sys.exit(1)
+
+# Change current directory to the top of the tree
+if 'ANDROID_BUILD_TOP' in os.environ:
+    top = os.environ['ANDROID_BUILD_TOP']
+    if not is_pathA_subdir_of_pathB(os.getcwd(), top):
+        sys.stderr.write('ERROR: You must run this tool from within $ANDROID_BUILD_TOP!\n')
+        sys.exit(1)
+    os.chdir(os.environ['ANDROID_BUILD_TOP'])
+
+# Sanity check that we are being run from the top level of the tree
+if not os.path.isdir('.repo'):
+    sys.stderr.write('ERROR: No .repo directory found. Please run this from the top of your tree.\n')
+    sys.exit(1)
+
+# If --abandon-first is given, abandon the branch before starting
+if args.abandon_first:
+    # Determine if the branch already exists; skip the abandon if it does not
+    plist = subprocess.Popen([repo_bin,"info"], stdout=subprocess.PIPE)
+    needs_abandon = False
+    while(True):
+        pline = plist.stdout.readline().rstrip()
+        if not pline:
+            break
+        matchObj = re.match(r'Local Branches.*\[(.*)\]', pline.decode())
+        if matchObj:
+            local_branches = re.split('\s*,\s*', matchObj.group(1))
+            if any(args.start_branch[0] in s for s in local_branches):
+                needs_abandon = True
+
+    if needs_abandon:
+        # Perform the abandon only if the branch already exists
+        if not args.quiet:
+            print('Abandoning branch: %s' % args.start_branch[0])
+        cmd = '%s abandon %s' % (repo_bin, args.start_branch[0])
+        execute_cmd(cmd)
+        if not args.quiet:
+            print('')
 
-    reviews = []
-    for line in out.split('\n'):
-        try:
-            data = json.loads(line)
-            # make our data look like the http rest api data
-            review = {
-                'branch': data['branch'],
-                'change_id': data['id'],
-                'current_revision': data['currentPatchSet']['revision'],
-                'number': int(data['number']),
-                'revisions': {patch_set['revision']: {
-                    'number': int(patch_set['number']),
-                    'fetch': {
-                        'ssh': {
-                            'ref': patch_set['ref'],
-                            'url': u'ssh://{0}:{1}/{2}'.format(userhost, port, data['project'])
-                        }
-                    }
-                } for patch_set in data['patchSets']},
-                'subject': data['subject'],
-                'project': data['project'],
-                'status': data['status']
-            }
-            reviews.append(review)
-        except:
-            pass
-    args.quiet or print('Found {0} reviews'.format(len(reviews)))
-    return reviews
+# Get the list of projects that repo knows about
+#   - convert the project name to a project path
+project_name_to_path = {}
+plist = subprocess.Popen([repo_bin,"list"], stdout=subprocess.PIPE)
+project_path = None
+while(True):
+    pline = plist.stdout.readline().rstrip()
+    if not pline:
+        break
+    ppaths = re.split('\s*:\s*', pline.decode())
+    project_name_to_path[ppaths[1]] = ppaths[0]
+
+# Get all commits for a specified topic
+if args.topic:
+    url = 'http://review.cyanogenmod.org/changes/?q=topic:%s' % args.topic
+    if args.verbose:
+        print('Fetching all commits from topic: %s\n' % args.topic)
+    f = urllib.request.urlopen(url)
+    d = f.read().decode("utf-8")
+    if args.verbose:
+        print('Result from request:\n' + d)
+
+    # Clean up the result
+    d = d.split(')]}\'\n')[1]
+    matchObj = re.match(r'\[\s*\]', d)
+    if matchObj:
+        sys.stderr.write('ERROR: Topic %s was not found on the server\n' % args.topic)
+        sys.exit(1)
+    d = re.sub(r'\[(.*)\]', r'\1', d)
+    if args.verbose:
+        print('Result from request:\n' + d)
+
+    data = json.loads(d)
+    changelist = []
+    for c in xrange(0, len(data)):
+        changelist.append(data[c]['_number'])
+
+    # Reverse the array as we want to pick the lowest one first
+    args.change_number = reversed(changelist)
+
+# Check for range of commits and rebuild array
+changelist = []
+for change in args.change_number:
+    c=str(change)
+    if '-' in c:
+        templist = c.split('-')
+        for i in range(int(templist[0]), int(templist[1]) + 1):
+            changelist.append(str(i))
+    else:
+        changelist.append(c)
 
+args.change_number = changelist
 
-def fetch_query_via_http(remote_url, query):
+# Iterate through the requested change numbers
+for changeps in args.change_number:
 
-    """Given a query, fetch the change numbers via http"""
-    url = '{0}/changes/?q={1}&o=CURRENT_REVISION&o=ALL_REVISIONS'.format(remote_url, query)
-    data = urllib.request.urlopen(url).read().decode('utf-8')
-    reviews = json.loads(data[5:])
+    if '/' in changeps:
+        change = changeps.split('/')[0]
+        patchset = changeps.split('/')[1]
+    else:
+        change = changeps
+        patchset = ''
 
-    for review in reviews:
-        review[u'number'] = review.pop('_number')
+    if not args.quiet:
+        if len(patchset) == 0:
+            print('Applying change number %s ...' % change)
+        else:
+            print('Applying change number {change}/{patchset} ...'.format(change=change, patchset=patchset))
 
-    return reviews
+    if len(patchset) == 0:
+        query_revision = 'CURRENT_REVISION'
+    else:
+        query_revision = 'ALL_REVISIONS'
+
+    # Fetch information about the change from Gerrit's REST API
+    #
+    # gerrit returns two lines, a magic string and then valid JSON:
+    #   )]}'
+    #   [ ... valid JSON ... ]
+    url = 'http://review.cyanogenmod.org/changes/?q={change}&o={query_revision}&o=CURRENT_COMMIT&pp=0'.format(change=change, query_revision=query_revision)
+    if args.verbose:
+        print('Fetching from: %s\n' % url)
+    f = urllib.request.urlopen(url)
+    d = f.read().decode("utf-8")
+    if args.verbose:
+        print('Result from request:\n' + d)
+
+    # Clean up the result
+    d = d.split('\n')[1]
+    matchObj = re.match(r'\[\s*\]', d)
+    if matchObj:
+        sys.stderr.write('ERROR: Change number %s was not found on the server\n' % change)
+        sys.exit(1)
+    d = re.sub(r'\[(.*)\]', r'\1', d)
+
+    # Parse the JSON
+    try:
+        data = json.loads(d)
+    except ValueError:
+        sys.stderr.write('ERROR: The response from the server could not be parsed properly\n')
+        if not args.verbose:
+            sys.stderr.write('The malformed response was: %s\n' % d)
+        sys.exit(1)
 
+    # Extract information from the JSON response
+    date_fluff       = '.000000000'
+    project_name     = data['project']
+    project_branch   = data['branch']
+    change_number    = data['_number']
+    status           = data['status']
+    patchsetfound    = False
 
-def fetch_query(remote_url, query):
-    """Wrapper for fetch_query_via_proto functions"""
-    if remote_url[0:3] == 'ssh':
-        return fetch_query_via_ssh(remote_url, query)
-    elif remote_url[0:4] == 'http':
-        return fetch_query_via_http(remote_url, query.replace(' ', '+'))
+    if len(patchset) > 0:
+        try:
+            for revision in data['revisions']:
+                if (int(data['revisions'][revision]['_number']) == int(patchset)) and not patchsetfound:
+                    target_revision = data['revisions'][revision]
+                    if args.verbose:
+                       print('Using found patch set {patchset} ...'.format(patchset=patchset))
+                    patchsetfound = True
+                    break
+            if not patchsetfound:
+                print('ERROR: The patch set could not be found, using CURRENT_REVISION instead.')
+        except:
+            print('ERROR: The patch set could not be found, using CURRENT_REVISION instead.')
+            patchsetfound = False
+
+    if not patchsetfound:
+        target_revision = data['revisions'][data['current_revision']]
+
+    current_revision = data['revisions'][data['current_revision']]
+
+    patch_number     = target_revision['_number']
+    fetch_url        = target_revision['fetch']['anonymous http']['url']
+    fetch_ref        = target_revision['fetch']['anonymous http']['ref']
+    author_name      = current_revision['commit']['author']['name']
+    author_email     = current_revision['commit']['author']['email']
+    author_date      = current_revision['commit']['author']['date'].replace(date_fluff, '')
+    committer_name   = current_revision['commit']['committer']['name']
+    committer_email  = current_revision['commit']['committer']['email']
+    committer_date   = current_revision['commit']['committer']['date'].replace(date_fluff, '')
+    subject          = current_revision['commit']['subject']
+
+    # Check if commit has already been merged and exit if it has, unless -f is specified
+    if status == "MERGED":
+        if args.force:
+            print("!! Force-picking a merged commit !!\n")
+        else:
+            print("Commit already merged. Skipping the cherry pick.\nUse -f to force this pick.")
+            continue;
+
+    # Convert the project name to a project path
+    #   - check that the project path exists
+    if project_name in project_name_to_path:
+        project_path = project_name_to_path[project_name];
+
+        if project_path.startswith('hardware/qcom/'):
+            split_path = project_path.split('/')
+            # split_path[2] might be display or it might be display-caf, trim the -caf
+            split_path[2] = split_path[2].split('-')[0]
+
+            # Need to treat hardware/qcom/{audio,display,media} specially
+            if split_path[2] == 'audio' or split_path[2] == 'display' or split_path[2] == 'media':
+                split_branch = project_branch.split('-')
+
+                # display is extra special
+                if split_path[2] == 'display' and len(split_path) == 3:
+                    project_path = '/'.join(split_path)
+                else:
+                    project_path = '/'.join(split_path[:-1])
+
+                if len(split_branch) == 4 and split_branch[0] == 'cm' and split_branch[2] == 'caf':
+                    project_path += '-caf/msm' + split_branch[3]
+                # audio and media are different from display
+                elif split_path[2] == 'audio' or split_path[2] == 'media':
+                    project_path += '/default'
+    elif args.ignore_missing:
+        print('WARNING: Skipping %d since there is no project directory for: %s\n' % (change_number, project_name))
+        continue;
     else:
-        raise Exception('Gerrit URL should be in the form http[s]://hostname/ or ssh://[user@]host[:port]')
-
-if __name__ == '__main__':
-    # Default to CyanogenMod Gerrit
-    default_gerrit = 'http://review.cyanogenmod.org'
-
-    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent('''\
-        repopick.py is a utility to simplify the process of cherry picking
-        patches from CyanogenMod's Gerrit instance (or any gerrit instance of your choosing)
-
-        Given a list of change numbers, repopick will cd into the project path
-        and cherry pick the latest patch available.
-
-        With the --start-branch argument, the user can specify that a branch
-        should be created before cherry picking. This is useful for
-        cherry-picking many patches into a common branch which can be easily
-        abandoned later (good for testing other's changes.)
-
-        The --abandon-first argument, when used in conjunction with the
-        --start-branch option, will cause repopick to abandon the specified
-        branch in all repos first before performing any cherry picks.'''))
-    parser.add_argument('change_number', nargs='*', help='change number to cherry pick.  Use {change number}/{patchset number} to get a specific revision.')
-    parser.add_argument('-i', '--ignore-missing', action='store_true', help='do not error out if a patch applies to a missing directory')
-    parser.add_argument('-s', '--start-branch', nargs=1, help='start the specified branch before cherry picking')
-    parser.add_argument('-a', '--abandon-first', action='store_true', help='before cherry picking, abandon the branch specified in --start-branch')
-    parser.add_argument('-b', '--auto-branch', action='store_true', help='shortcut to "--start-branch auto --abandon-first --ignore-missing"')
-    parser.add_argument('-q', '--quiet', action='store_true', help='print as little as possible')
-    parser.add_argument('-v', '--verbose', action='store_true', help='print extra information to aid in debug')
-    parser.add_argument('-f', '--force', action='store_true', help='force cherry pick even if change is closed')
-    parser.add_argument('-p', '--pull', action='store_true', help='execute pull instead of cherry-pick')
-    parser.add_argument('-P', '--path', help='use the specified path for the change')
-    parser.add_argument('-t', '--topic', help='pick all commits from a specified topic')
-    parser.add_argument('-Q', '--query', help='pick all commits using the specified query')
-    parser.add_argument('-g', '--gerrit', default=default_gerrit, help='Gerrit Instance to use. Form proto://[user@]host[:port]')
-    args = parser.parse_args()
-    if not args.start_branch and args.abandon_first:
-        parser.error('if --abandon-first is set, you must also give the branch name with --start-branch')
-    if args.auto_branch:
-        args.abandon_first = True
-        args.ignore_missing = True
-        if not args.start_branch:
-            args.start_branch = ['auto']
-    if args.quiet and args.verbose:
-        parser.error('--quiet and --verbose cannot be specified together')
-
-    if (1 << bool(args.change_number) << bool(args.topic) << bool(args.query)) != 2:
-        parser.error('One (and only one) of change_number, topic, and query are allowed')
-
-    # Change current directory to the top of the tree
-    if 'ANDROID_BUILD_TOP' in os.environ:
-        top = os.environ['ANDROID_BUILD_TOP']
-
-        if not is_subdir(os.getcwd(), top):
-            sys.stderr.write('ERROR: You must run this tool from within $ANDROID_BUILD_TOP!\n')
-            sys.exit(1)
-        os.chdir(os.environ['ANDROID_BUILD_TOP'])
-
-    # Sanity check that we are being run from the top level of the tree
-    if not os.path.isdir('.repo'):
-        sys.stderr.write('ERROR: No .repo directory found. Please run this from the top of your tree.\n')
+        sys.stderr.write('ERROR: For %d, could not determine the project path for project %s\n' % (change_number, project_name))
         sys.exit(1)
 
-    # If --abandon-first is given, abandon the branch before starting
-    if args.abandon_first:
-        # Determine if the branch already exists; skip the abandon if it does not
-        plist = subprocess.check_output(['repo', 'info'])
-        needs_abandon = False
-        for pline in plist:
-            matchObj = re.match(r'Local Branches.*\[(.*)\]', pline)
-            if matchObj:
-                local_branches = re.split('\s*,\s*', matchObj.group(1))
-                if any(args.start_branch[0] in s for s in local_branches):
-                    needs_abandon = True
-
-        if needs_abandon:
-            # Perform the abandon only if the branch already exists
-            if not args.quiet:
-                print('Abandoning branch: %s' % args.start_branch[0])
-            subprocess.check_output(['repo', 'abandon', args.start_branch[0]])
-            if not args.quiet:
-                print('')
-
-    # Get the master manifest from repo
-    #   - convert project name and revision to a path
-    project_name_to_data = {}
-    manifest = subprocess.check_output(['repo', 'manifest'])
-    xml_root = ElementTree.fromstring(manifest)
-    projects = xml_root.findall('project')
-    default_revision = xml_root.findall('default')[0].get('revision').split('/')[-1]
-
-    #dump project data into the a list of dicts with the following data:
-    #{project: {path, revision}}
-
-    for project in projects:
-        name = project.get('name')
-        path = project.get('path')
-        revision = project.get('revision')
-        if revision is None:
-            revision = default_revision
-
-        if not name in project_name_to_data:
-            project_name_to_data[name] = {}
-        project_name_to_data[name][revision] = path
-
-    # get data on requested changes
-    reviews = []
-    change_numbers = []
-    if args.topic:
-        reviews = fetch_query(args.gerrit, 'topic:{0}'.format(args.topic))
-        change_numbers = sorted([str(r['number']) for r in reviews])
-    if args.query:
-        reviews = fetch_query(args.gerrit, args.query)
-        change_numbers = sorted([str(r['number']) for r in reviews])
-    if args.change_number:
-        reviews = fetch_query(args.gerrit, ' OR '.join('change:{0}'.format(x.split('/')[0]) for x in args.change_number))
-        change_numbers = args.change_number
-
-    # make list of things to actually merge
-    mergables = []
-
-    for change in change_numbers:
-        patchset = None
-        if '/' in change:
-            (change, patchset) = change.split('/')
-        change = int(change)
-
-        review = [x for x in reviews if x['number'] == change][0]
-        mergables.append({
-            'subject': review['subject'],
-            'project': review['project'],
-            'branch': review['branch'],
-            'change_number': review['number'],
-            'status': review['status'],
-            'fetch': None
-        })
-        mergables[-1]['fetch'] = review['revisions'][review['current_revision']]['fetch']
-        mergables[-1]['id'] = change
-        if patchset:
-            try:
-                mergables[-1]['fetch'] = [x['fetch'] for x in review['revisions'] if x['_number'] == patchset][0]
-                mergables[-1]['id'] = '{0}/{1}'.format(change, patchset)
-            except (IndexError, ValueError):
-                args.quiet or print('ERROR: The patch set {0}/{1} could not be found, using CURRENT_REVISION instead.'.format(change, patchset))
-
-    for item in mergables:
-        args.quiet or print('Applying change number {0}...'.format(item['id']))
-        # Check if change is open and exit if it's not, unless -f is specified
-        if (item['status'] != 'OPEN' and item['status'] != 'NEW') and not args.query:
-            if args.force:
-                print('!! Force-picking a closed change !!\n')
-            else:
-                print('Change status is ' + item['status'] + '. Skipping the cherry pick.\nUse -f to force this pick.')
-                continue
-
-        # Convert the project name to a project path
-        #   - check that the project path exists
-        project_path = None
-
-        if item['project'] in project_name_to_data and item['branch'] in project_name_to_data[item['project']]:
-            project_path = project_name_to_data[item['project']][item['branch']]
-        elif args.path:
-            project_path = args.path
-        elif args.ignore_missing:
-            print('WARNING: Skipping {0} since there is no project directory for: {1}\n'.format(item['id'], item['project']))
-            continue
+    # If --start-branch is given, create the branch (more than once per path is okay; repo ignores gracefully)
+    if args.start_branch:
+        cmd = '%s start %s %s' % (repo_bin, args.start_branch[0], project_path)
+        execute_cmd(cmd)
+
+    # Print out some useful info
+    if not args.quiet:
+        print('--> Subject:       "%s"' % subject)
+        print('--> Project path:  %s' % project_path)
+        print('--> Change number: %d (Patch Set %d)' % (change_number, patch_number))
+        print('--> Author:        %s <%s> %s' % (author_name, author_email, author_date))
+        print('--> Committer:     %s <%s> %s' % (committer_name, committer_email, committer_date))
+
+    # Try fetching from GitHub first
+    if args.verbose:
+       print('Trying to fetch the change from GitHub')
+    if args.pull:
+      cmd = 'cd %s && git pull --no-edit github %s' % (project_path, fetch_ref)
+    else:
+      cmd = 'cd %s && git fetch github %s' % (project_path, fetch_ref)
+    execute_cmd(cmd)
+    # Check if it worked
+    FETCH_HEAD = '%s/.git/FETCH_HEAD' % project_path
+    if os.stat(FETCH_HEAD).st_size == 0:
+        # That didn't work, fetch from Gerrit instead
+        if args.verbose:
+          print('Fetching from GitHub didn\'t work, trying to fetch the change from Gerrit')
+        if args.pull:
+          cmd = 'cd %s && git pull --no-edit %s %s' % (project_path, fetch_url, fetch_ref)
         else:
-            sys.stderr.write('ERROR: For {0}, could not determine the project path for project {1}\n'.format(item['id'], item['project']))
-            sys.exit(1)
-
-        # If --start-branch is given, create the branch (more than once per path is okay; repo ignores gracefully)
-        if args.start_branch:
-            subprocess.check_output(['repo', 'start', args.start_branch[0], project_path])
-
-        # Print out some useful info
-        if not args.quiet:
-            print('--> Subject:       "{0}"'.format(item['subject']))
-            print('--> Project path:  {0}'.format(project_path))
-            print('--> Change number: {0} (Patch Set {0})'.format(item['id']))
+          cmd = 'cd %s && git fetch %s %s' % (project_path, fetch_url, fetch_ref)
+        execute_cmd(cmd)
+    # Perform the cherry-pick
+    cmd = 'cd %s && git cherry-pick FETCH_HEAD' % (project_path)
+    if not args.pull:
+      execute_cmd(cmd)
+    if not args.quiet:
+        print('')
 
-        if 'anonymous http' in item['fetch']:
-            method = 'anonymous http'
-        else:
-            method = 'ssh'
-
-        # Try fetching from GitHub first if using default gerrit
-        if args.gerrit == default_gerrit:
-            if args.verbose:
-                print('Trying to fetch the change from GitHub')
-
-            if args.pull:
-                cmd = ['git pull --no-edit github', item['fetch'][method]['ref']]
-            else:
-                cmd = ['git fetch github', item['fetch'][method]['ref']]
-            if args.quiet:
-                cmd.append('--quiet')
-            else:
-                print(cmd)
-            result = subprocess.call([' '.join(cmd)], cwd=project_path, shell=True)
-            if result != 0:
-                print('ERROR: git command failed')
-                sys.exit(result)
-            FETCH_HEAD = '{0}/.git/FETCH_HEAD'.format(project_path)
-        # Check if it worked
-        if args.gerrit != default_gerrit or os.stat(FETCH_HEAD).st_size == 0:
-            # If not using the default gerrit or github failed, fetch from gerrit.
-            if args.verbose:
-                if args.gerrit == default_gerrit:
-                    print('Fetching from GitHub didn\'t work, trying to fetch the change from Gerrit')
-                else:
-                    print('Fetching from {0}'.format(args.gerrit))
-
-            if args.pull:
-                cmd = ['git pull --no-edit', item['fetch'][method]['url'], item['fetch'][method]['ref']]
-            else:
-                cmd = ['git fetch', item['fetch'][method]['url'], item['fetch'][method]['ref']]
-            if args.quiet:
-                cmd.append('--quiet')
-            else:
-                print(cmd)
-            result = subprocess.call([' '.join(cmd)], cwd=project_path, shell=True)
-            if result != 0:
-                print('ERROR: git command failed')
-                sys.exit(result)
-        # Perform the cherry-pick
-        if not args.pull:
-            cmd = ['git cherry-pick FETCH_HEAD']
-            if args.quiet:
-                cmd_out = open(os.devnull, 'wb')
-            else:
-                cmd_out = None
-            result = subprocess.call(cmd, cwd=project_path, shell=True, stdout=cmd_out, stderr=cmd_out)
-            if result != 0:
-                print('ERROR: git command failed')
-                sys.exit(result)
-        if not args.quiet:
-            print('')
diff --git a/tools/roomservice.py b/tools/roomservice.py
index e8c9112..3d065ef 100644
--- a/tools/roomservice.py
+++ b/tools/roomservice.py
@@ -1,6 +1,5 @@
 #!/usr/bin/env python
 # Copyright (C) 2012-2013, The CyanogenMod Project
-#           (C) 2017 The LineageOS Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -52,7 +51,7 @@
     device = product
 
 if not depsonly:
-    print("Device %s not found. Attempting to retrieve device repository from LineageOS Github (http://github.com/LineageOS)." % device)
+    print("Device %s not found. Attempting to retrieve device repository from CyanogenMod Github (http://github.com/CyanogenMod)." % device)
 
 repositories = []
 
@@ -71,17 +70,18 @@ def add_auth(githubreq):
         githubreq.add_header("Authorization","Basic %s" % githubauth)
 
 if not depsonly:
-    githubreq = urllib.request.Request("https://api.github.com/search/repositories?q=%s+user:LineageOS+in:name+fork:true" % device)
+    githubreq = urllib.request.Request("https://api.github.com/search/repositories?q=%s+user:CyanogenMod+in:name+fork:true" % device)
     add_auth(githubreq)
+    result = json.loads(urllib.request.urlopen(githubreq).read().decode())
     try:
-        result = json.loads(urllib.request.urlopen(githubreq).read().decode())
-    except urllib.error.URLError:
-        print("Failed to search GitHub")
+        numresults = int(result['total_count'])
+    except:
+        print("Failed to search GitHub (offline?)")
         sys.exit()
-    except ValueError:
-        print("Failed to parse return data from GitHub")
+    if (numresults == 0):
+        print("Could not find device %s on github.com/CyanogenMod" % device)
         sys.exit()
-    for res in result.get('items', []):
+    for res in result['items']:
         repositories.append(res)
 
 local_manifests = r'.repo/local_manifests'
@@ -174,12 +174,12 @@ def add_to_manifest(repositories, fallback_branch = None):
         repo_name = repository['repository']
         repo_target = repository['target_path']
         if exists_in_tree(lm, repo_name):
-            print('LineageOS/%s already exists' % (repo_name))
+            print('CyanogenMod/%s already exists' % (repo_name))
             continue
 
-        print('Adding dependency: LineageOS/%s -> %s' % (repo_name, repo_target))
+        print('Adding dependency: CyanogenMod/%s -> %s' % (repo_name, repo_target))
         project = ElementTree.Element("project", attrib = { "path": repo_target,
-            "remote": "github", "name": "LineageOS/%s" % repo_name })
+            "remote": "github", "name": "CyanogenMod/%s" % repo_name })
 
         if 'branch' in repository:
             project.set('revision',repository['branch'])
@@ -210,7 +210,7 @@ def fetch_dependencies(repo_path, fallback_branch = None):
         fetch_list = []
 
         for dependency in dependencies:
-            if not is_in_manifest("LineageOS/%s" % dependency['repository']):
+            if not is_in_manifest("CyanogenMod/%s" % dependency['repository']):
                 fetch_list.append(dependency)
                 syncable_repos.append(dependency['target_path'])
 
@@ -224,7 +224,7 @@ def fetch_dependencies(repo_path, fallback_branch = None):
 
     if len(syncable_repos) > 0:
         print('Syncing dependencies')
-        os.system('repo sync --force-sync %s' % ' '.join(syncable_repos))
+        os.system('repo sync %s' % ' '.join(syncable_repos))
 
     for deprepo in syncable_repos:
         fetch_dependencies(deprepo)
@@ -286,11 +286,11 @@ def has_branch(branches, revision):
             add_to_manifest([adding], fallback_branch)
 
             print("Syncing repository to retrieve project.")
-            os.system('repo sync --force-sync %s' % repo_path)
+            os.system('repo sync %s' % repo_path)
             print("Repository synced!")
 
             fetch_dependencies(repo_path, fallback_branch)
             print("Done")
             sys.exit()
 
-print("Repository for %s not found in the LineageOS Github repository list. If this is in error, you may need to manually add it to your local_manifests/roomservice.xml." % device)
+print("Repository for %s not found in the CyanogenMod Github repository list. If this is in error, you may need to manually add it to your local_manifests/roomservice.xml." % device)